"""
Script to create additional visualization plots for model evaluation:
- Residuals plot
- Actual vs Predicted plot
- Feature importance plot (for tree-based models)
"""

import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

# Visualization
try:
    import matplotlib

    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    import seaborn as sns

    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("‚ö†Ô∏è Visualization libraries not available")

# LightGBM for feature importance
try:
    import lightgbm as lgb

    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False


def load_model_and_data():
    """Load best model and test data."""
    models_dir = Path("models")
    processed_dir = Path("data/processed")

    # Load model
    model_path = models_dir / "best_model.pkl"
    features_path = models_dir / "best_model_features.json"
    config_path = models_dir / "best_model_config.json"

    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")

    model = joblib.load(model_path)

    with open(features_path, "r") as f:
        feature_names = json.load(f)

    with open(config_path, "r") as f:
        config = json.load(f)

    model_name = config.get("best_model", "Unknown")

    # Load test data
    test_path = processed_dir / "test_encoded.csv"
    if not test_path.exists():
        raise FileNotFoundError(f"Test data not found: {test_path}")

    test_df = pd.read_csv(test_path)

    # Ensure we have the correct features (model might expect specific order)
    if "SalePrice" in test_df.columns:
        X_test = test_df[feature_names]
        y_test = test_df["SalePrice"]
    else:
        raise ValueError("SalePrice column not found in test data")

    return model, model_name, X_test, y_test, feature_names


def create_residuals_plot(model, X_test, y_test, model_name, save_path):
    """Create residuals plot and actual vs predicted plot."""
    if not VISUALIZATION_AVAILABLE:
        return

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate residuals
    residuals = y_test - y_pred

    # Create figure with 2 subplots
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Set style
    try:
        plt.style.use("seaborn-v0_8-darkgrid")
    except:
        try:
            plt.style.use("seaborn-darkgrid")
        except:
            plt.style.use("default")

    # 1. Residuals Plot
    ax1 = axes[0]
    ax1.scatter(y_pred, residuals, alpha=0.6, s=30, color="#3498db")
    ax1.axhline(
        y=0, color="red", linestyle="--", linewidth=2, label="Perfect Prediction"
    )
    ax1.set_xlabel("Predicted Values (log scale)", fontsize=11, fontweight="bold")
    ax1.set_ylabel("Residuals (Actual - Predicted)", fontsize=11, fontweight="bold")
    ax1.set_title(
        f"{model_name} Residuals Plot\n(Homogeneity Check)",
        fontsize=12,
        fontweight="bold",
    )
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # Add annotations
    rmse = np.sqrt(np.mean(residuals**2))
    ax1.text(
        0.05,
        0.95,
        f"RMSE: {rmse:.4f}",
        transform=ax1.transAxes,
        fontsize=10,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )

    # 2. Actual vs Predicted Plot
    ax2 = axes[1]
    ax2.scatter(y_test, y_pred, alpha=0.6, s=30, color="#2ecc71", zorder=2)

    # Perfect prediction line: y = x (diagonal line)
    # Calculate the range that includes both actual and predicted values
    all_values = np.concatenate(
        [
            y_test.values if hasattr(y_test, "values") else y_test,
            y_pred if isinstance(y_pred, np.ndarray) else np.array(y_pred),
        ]
    )
    min_val = np.min(all_values)
    max_val = np.max(all_values)

    # Add a small margin for better visualization
    margin = (max_val - min_val) * 0.05
    line_min = min_val - margin
    line_max = max_val + margin

    # Draw the perfect prediction line (y = x) - this should be a 45-degree diagonal
    ax2.plot(
        [line_min, line_max],
        [line_min, line_max],
        "r--",
        linewidth=2,
        label="Perfect Prediction (y=x)",
        zorder=1,
    )

    # Set equal aspect ratio and same limits for both axes to ensure 45-degree line
    ax2.set_aspect("equal", adjustable="box")

    ax2.set_xlabel("Actual Values (log scale)", fontsize=11, fontweight="bold")
    ax2.set_ylabel("Predicted Values (log scale)", fontsize=11, fontweight="bold")
    ax2.set_title(
        f"{model_name} Actual vs Predicted\n(Prediction Accuracy)",
        fontsize=12,
        fontweight="bold",
    )
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # Calculate R¬≤
    from sklearn.metrics import r2_score

    r2 = r2_score(y_test, y_pred)
    ax2.text(
        0.05,
        0.95,
        f"R¬≤: {r2:.4f}",
        transform=ax2.transAxes,
        fontsize=10,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()

    print(f"‚úÖ Residuals plot saved: {save_path}")


def create_feature_importance_plot(model, model_name, feature_names, save_path):
    """Create feature importance plot for tree-based models."""
    if not VISUALIZATION_AVAILABLE:
        return

    # Check if model supports feature_importances_
    if not hasattr(model, "feature_importances_"):
        print(f"‚ö†Ô∏è Model {model_name} does not support feature_importances_")
        return

    # Get feature importance
    importances = model.feature_importances_

    # Create DataFrame
    importance_df = (
        pd.DataFrame({"feature": feature_names, "importance": importances})
        .sort_values("importance", ascending=False)
        .head(20)
    )

    # Create plot
    fig, ax = plt.subplots(figsize=(12, 8))

    try:
        plt.style.use("seaborn-v0_8-darkgrid")
    except:
        try:
            plt.style.use("seaborn-darkgrid")
        except:
            plt.style.use("default")

    colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))
    bars = ax.barh(range(len(importance_df)), importance_df["importance"], color=colors)

    ax.set_yticks(range(len(importance_df)))
    ax.set_yticklabels(importance_df["feature"], fontsize=9)
    ax.set_xlabel("Feature Importance", fontsize=11, fontweight="bold")
    ax.set_title(
        f"{model_name} - Top 20 Feature Importance\n(Which features matter most?)",
        fontsize=12,
        fontweight="bold",
    )
    ax.invert_yaxis()
    ax.grid(axis="x", alpha=0.3)

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, importance_df["importance"])):
        ax.text(
            val + val * 0.01,
            i,
            f"{val:.2f}",
            va="center",
            fontsize=8,
            fontweight="bold",
        )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()

    print(f"‚úÖ Feature importance plot saved: {save_path}")


def main():
    """Create all additional plots."""
    print("\n" + "=" * 60)
    print("CREATING ADDITIONAL VISUALIZATION PLOTS")
    print("=" * 60)

    if not VISUALIZATION_AVAILABLE:
        print("‚ùå Visualization libraries not available")
        return

    try:
        # Load model and data
        print("\nüìä Loading model and test data...")
        model, model_name, X_test, y_test, feature_names = load_model_and_data()

        print(f"   Model: {model_name}")
        print(f"   Test samples: {len(X_test)}")
        print(f"   Features: {len(feature_names)}")

        models_dir = Path("models")

        # 1. Residuals and Actual vs Predicted plot
        print("\nüìà Creating residuals plot...")
        residuals_path = models_dir / "model_residuals.png"
        create_residuals_plot(model, X_test, y_test, model_name, residuals_path)

        # 2. Feature importance plot (if supported)
        print("\nüìä Creating feature importance plot...")
        importance_path = models_dir / "model_feature_importance.png"
        create_feature_importance_plot(
            model, model_name, feature_names, importance_path
        )

        print("\n‚úÖ All additional plots created successfully!")

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
