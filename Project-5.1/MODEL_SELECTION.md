# ğŸ¯ Model Selection: TÃ¬m Kiáº¿m Model Tá»‘t Nháº¥t Cho Dá»± ÄoÃ¡n GiÃ¡ NhÃ 

## ğŸ“– Má»¥c Lá»¥c

1. [Giá»›i thiá»‡u: Táº¡i sao cáº§n Model Selection?](#giá»›i-thiá»‡u)
2. [CÃ¡c KhÃ¡i Niá»‡m CÆ¡ Báº£n](#cÃ¡c-khÃ¡i-niá»‡m-cÆ¡-báº£n)
3. [Táº¡i Sao Chá»n 6 Models NÃ y?](#táº¡i-sao-chá»n-6-models-nÃ y)
4. [Quy TrÃ¬nh Model Selection](#quy-trÃ¬nh-model-selection)
5. [Hyperparameter Tuning: TÃ¬m Tham Sá»‘ Tá»‘i Æ¯u](#hyperparameter-tuning)
6. [Káº¿t Quáº£ So SÃ¡nh Models](#káº¿t-quáº£-so-sÃ¡nh-models)
7. [PhÃ¢n TÃ­ch Chi Tiáº¿t Tá»«ng Model](#phÃ¢n-tÃ­ch-chi-tiáº¿t-tá»«ng-model)
8. [Káº¿t Luáº­n: Chá»n LightGBM](#káº¿t-luáº­n)

---

## ğŸŒŸ Giá»›i thiá»‡u: Táº¡i sao cáº§n Model Selection?

Khi lÃ m machine learning, báº¡n sáº½ luÃ´n tá»± há»i: **"Model nÃ o lÃ  tá»‘t nháº¥t cho bÃ i toÃ¡n cá»§a tÃ´i?"**

Thá»±c táº¿ lÃ  **KHÃ”NG CÃ“ MODEL NÃ€O LÃ€ HOÃ€N Háº¢O CHO Má»ŒI BÃ€I TOÃN**. Má»—i model cÃ³ Ä‘iá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u riÃªng:

- **Linear Regression**: ÄÆ¡n giáº£n, dá»… hiá»ƒu nhÆ°ng khÃ´ng báº¯t Ä‘Æ°á»£c pattern phá»©c táº¡p
- **Tree-based (LightGBM, XGBoost)**: Máº¡nh máº½, chÃ­nh xÃ¡c cao nhÆ°ng khÃ³ giáº£i thÃ­ch
- **Regularized Models (Ridge, Lasso)**: CÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng giáº£i thÃ­ch

ğŸ‘‰ **Model Selection** lÃ  quÃ¡ trÃ¬nh **so sÃ¡nh nhiá»u models khÃ¡c nhau** Ä‘á»ƒ tÃ¬m ra model phÃ¹ há»£p nháº¥t cho bÃ i toÃ¡n cá»§a báº¡n.

**Trong project nÃ y, chÃºng ta sáº½:**

1. âœ… Train 6 models khÃ¡c nhau
2. âœ… Tune hyperparameters cho tá»«ng model
3. âœ… So sÃ¡nh performance báº±ng metrics
4. âœ… Chá»n model tá»‘t nháº¥t Ä‘á»ƒ deploy

---

## ğŸ“š CÃ¡c KhÃ¡i Niá»‡m CÆ¡ Báº£n

TrÆ°á»›c khi Ä‘i sÃ¢u vÃ o code, hÃ£y hiá»ƒu má»™t sá»‘ khÃ¡i niá»‡m quan trá»ng:

### 1. **Metrics: LÃ m sao Ä‘Ã¡nh giÃ¡ model tá»‘t hay khÃ´ng?**

ChÃºng ta dÃ¹ng 3 metrics chÃ­nh:

#### **RMSE (Root Mean Squared Error)**

```
RMSE = âˆš[Î£(y_thá»±c - y_dá»±_Ä‘oÃ¡n)Â² / n]
```

- **Ã nghÄ©a**: Sai sá»‘ trung bÃ¬nh (cÃ ng tháº¥p cÃ ng tá»‘t)
- **VÃ­ dá»¥**: RMSE = 0.125 nghÄ©a lÃ  sai sá»‘ trung bÃ¬nh khoáº£ng 0.125 (trong scale log)
- **Æ¯u Ä‘iá»ƒm**: Pháº¡t náº·ng cÃ¡c lá»—i lá»›n (outliers cÃ³ áº£nh hÆ°á»Ÿng nhiá»u)

#### **MAE (Mean Absolute Error)**

```
MAE = Î£|y_thá»±c - y_dá»±_Ä‘oÃ¡n| / n
```

- **Ã nghÄ©a**: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh (cÃ ng tháº¥p cÃ ng tá»‘t)
- **VÃ­ dá»¥**: MAE = 0.084 nghÄ©a lÃ  sai sá»‘ trung bÃ¬nh 0.084
- **Æ¯u Ä‘iá»ƒm**: KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng quÃ¡ nhiá»u bá»Ÿi outliers

#### **RÂ² Score (R-squared)**

```
RÂ² = 1 - (SS_res / SS_tot)
```

- **Ã nghÄ©a**: Tá»· lá»‡ variance Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi model (cÃ ng cao cÃ ng tá»‘t, tá»‘i Ä‘a = 1.0)
- **VÃ­ dá»¥**: RÂ² = 0.906 nghÄ©a lÃ  model giáº£i thÃ­ch Ä‘Æ°á»£c 90.6% sá»± biáº¿n thiÃªn cá»§a giÃ¡ nhÃ 
- **Æ¯u Ä‘iá»ƒm**: Dá»… hiá»ƒu, cÃ³ thá»ƒ so sÃ¡nh giá»¯a cÃ¡c models

### 2. **Cross-Validation: Kiá»ƒm tra model cÃ³ á»•n Ä‘á»‹nh khÃ´ng?**

**Váº¥n Ä‘á»**: Náº¿u chá»‰ train/test má»™t láº§n, káº¿t quáº£ cÃ³ thá»ƒ "may máº¯n" hoáº·c "khÃ´ng may"

**Giáº£i phÃ¡p**: **K-Fold Cross-Validation**

```
Dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh K pháº§n (vÃ­ dá»¥ K=5):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚  5  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Láº§n 1: Train trÃªn 2,3,4,5 â†’ Test trÃªn 1
Láº§n 2: Train trÃªn 1,3,4,5 â†’ Test trÃªn 2
Láº§n 3: Train trÃªn 1,2,4,5 â†’ Test trÃªn 3
Láº§n 4: Train trÃªn 1,2,3,5 â†’ Test trÃªn 4
Láº§n 5: Train trÃªn 1,2,3,4 â†’ Test trÃªn 5

â†’ TÃ­nh trung bÃ¬nh 5 káº¿t quáº£
```

**Táº¡i sao dÃ¹ng CV?**

- âœ… Kiá»ƒm tra model cÃ³ **overfitting** khÃ´ng (há»c quÃ¡ ká»¹ training data)
- âœ… ÄÃ¡nh giÃ¡ **stability** cá»§a model (performance cÃ³ thay Ä‘á»•i nhiá»u khÃ´ng)
- âœ… TÃ¬m hyperparameters tá»‘t nháº¥t má»™t cÃ¡ch **khÃ¡ch quan**

### 3. **Hyperparameter: CÃ¡c "nÃºt Ä‘iá»u chá»‰nh" cá»§a model**

Má»—i model cÃ³ cÃ¡c **hyperparameters** (tham sá»‘) mÃ  báº¡n cáº§n Ä‘iá»u chá»‰nh:

**VÃ­ dá»¥ vá»›i Ridge Regression:**

- `alpha`: Má»©c Ä‘á»™ regularization (0.001, 0.01, 0.1, 1, 10, 100)
  - `alpha` nhá» â†’ Model phá»©c táº¡p hÆ¡n, dá»… overfitting
  - `alpha` lá»›n â†’ Model Ä‘Æ¡n giáº£n hÆ¡n, dá»… underfitting

**VÃ­ dá»¥ vá»›i LightGBM:**

- `learning_rate`: Tá»‘c Ä‘á»™ há»c (0.01, 0.05, 0.1, 0.2)
- `max_depth`: Äá»™ sÃ¢u cá»§a cÃ¢y (3, 5, 7, 10)
- `num_leaves`: Sá»‘ lÃ¡ tá»‘i Ä‘a (31, 50, 100, 200)

ğŸ‘‰ **Hyperparameter Tuning** lÃ  tÃ¬m giÃ¡ trá»‹ tá»‘i Æ°u cho cÃ¡c tham sá»‘ nÃ y.

---

## ğŸ¯ Táº¡i Sao Chá»n 6 Models NÃ y?

### NhÃ³m 1: Linear Models vá»›i Regularization (4 models)

#### **1. Ridge Regression (L2 Regularization)**

```python
Ridge(alpha=100)
```

**Ã tÆ°á»Ÿng**: ThÃªm "penalty" vÃ o cÃ¡c há»‡ sá»‘ lá»›n â†’ Giá»¯ táº¥t cáº£ features nhÆ°ng giáº£m áº£nh hÆ°á»Ÿng

**Khi nÃ o dÃ¹ng?**

- âœ… CÃ³ nhiá»u features tÆ°Æ¡ng quan vá»›i nhau (multicollinearity)
- âœ… Muá»‘n giá»¯ táº¥t cáº£ features (khÃ´ng loáº¡i bá»)
- âœ… Cáº§n baseline Ä‘Æ¡n giáº£n

**Trade-off**: ÄÆ¡n giáº£n nhÆ°ng cÃ³ thá»ƒ khÃ´ng báº¯t Ä‘Æ°á»£c pattern phá»©c táº¡p

#### **2. Lasso Regression (L1 Regularization)**

```python
Lasso(alpha=0.01)
```

**Ã tÆ°á»Ÿng**: ÄÆ°a má»™t sá»‘ há»‡ sá»‘ vá» 0 â†’ **Tá»± Ä‘á»™ng chá»n features quan trá»ng**

**Khi nÃ o dÃ¹ng?**

- âœ… CÃ³ nhiá»u features, muá»‘n chá»n features quan trá»ng
- âœ… Cáº§n model **sparse** (Ã­t features)
- âœ… Muá»‘n interpretability (biáº¿t features nÃ o quan trá»ng)

**Trade-off**: CÃ³ thá»ƒ loáº¡i bá» nháº§m features quan trá»ng

#### **3. ElasticNet (L1 + L2)**

```python
ElasticNet(alpha=0.1, l1_ratio=0.1)
```

**Ã tÆ°á»Ÿng**: Káº¿t há»£p **Æ°u Ä‘iá»ƒm cá»§a cáº£ Ridge vÃ  Lasso**

**Khi nÃ o dÃ¹ng?**

- âœ… Muá»‘n vá»«a chá»n features vá»«a xá»­ lÃ½ multicollinearity
- âœ… Linh hoáº¡t hÆ¡n Ridge/Lasso

**Trade-off**: Phá»©c táº¡p hÆ¡n, cáº§n tune 2 tham sá»‘

#### **4. Huber Regressor**

```python
HuberRegressor(epsilon=1.2, alpha=0.1)
```

**Ã tÆ°á»Ÿng**: DÃ¹ng **robust loss function** â†’ Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers

**Khi nÃ o dÃ¹ng?**

- âœ… CÃ³ nhiá»u outliers
- âœ… Dá»¯ liá»‡u khÃ´ng clean

**Trade-off**: ThÆ°á»ng kÃ©m chÃ­nh xÃ¡c hÆ¡n cÃ¡c models khÃ¡c

### NhÃ³m 2: Tree-based Ensemble Models (2 models)

#### **5. LightGBM (Light Gradient Boosting Machine)**

```python
LGBMRegressor(
    learning_rate=0.1,
    max_depth=3,
    num_leaves=200,
    ...
)
```

**Ã tÆ°á»Ÿng**: Gradient Boosting vá»›i **cÃ¢y quyáº¿t Ä‘á»‹nh** â†’ Báº¯t Ä‘Æ°á»£c pattern phá»©c táº¡p

**Khi nÃ o dÃ¹ng?**

- âœ… Dá»¯ liá»‡u phá»©c táº¡p, cÃ³ nhiá»u feature interactions
- âœ… Cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao
- âœ… CÃ³ thá»ƒ cháº¥p nháº­n "black box"

**Trade-off**: KhÃ³ giáº£i thÃ­ch nhÆ°ng ráº¥t máº¡nh

#### **6. XGBoost (Extreme Gradient Boosting)**

```python
XGBRegressor(
    learning_rate=0.1,
    max_depth=3,
    ...
)
```

**Ã tÆ°á»Ÿng**: TÆ°Æ¡ng tá»± LightGBM nhÆ°ng **thuáº­t toÃ¡n khÃ¡c má»™t chÃºt**

**Khi nÃ o dÃ¹ng?**

- âœ… TÆ°Æ¡ng tá»± LightGBM
- âœ… Muá»‘n so sÃ¡nh 2 thuáº­t toÃ¡n boosting

**Trade-off**: ThÆ°á»ng cháº­m hÆ¡n LightGBM má»™t chÃºt

---

## ğŸ”„ Quy TrÃ¬nh Model Selection

### BÆ°á»›c 1: Chuáº©n bá»‹ Dá»¯ liá»‡u

Sau cÃ¡c bÆ°á»›c preprocessing, feature engineering, encoding, chÃºng ta cÃ³:

```
Train data: 1,239 samples Ã— 176 features
Test data:  219 samples Ã— 176 features
Target:      SalePrice (Ä‘Ã£ log-transform, skewness = 0.205)
```

**Code:**

```python
# Load encoded data
train_df = pd.read_csv('data/processed/train_encoded.csv')
test_df = pd.read_csv('data/processed/test_encoded.csv')

# Separate features and target
X_train = train_df.drop('SalePrice', axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop('SalePrice', axis=1)
y_test = test_df['SalePrice']

print(f"Train: {X_train.shape}")  # (1239, 176)
print(f"Test: {X_test.shape}")    # (219, 176)
```

### BÆ°á»›c 2: Khá»Ÿi táº¡o ModelTrainer

**Code:**

```python
from src.Modeling import ModelTrainer

trainer = ModelTrainer(models_dir='models', random_state=42)
```

**Chá»©c nÄƒng cá»§a ModelTrainer:**

- âœ… Train nhiá»u models
- âœ… Tune hyperparameters
- âœ… ÄÃ¡nh giÃ¡ báº±ng cross-validation
- âœ… So sÃ¡nh káº¿t quáº£
- âœ… LÆ°u model tá»‘t nháº¥t

### BÆ°á»›c 3: Train Táº¥t Cáº£ Models

**Code:**

```python
results_df = trainer.train_all_models(
    X_train, y_train,
    X_test, y_test
)
```

**Quy trÃ¬nh bÃªn trong:**

1. Train linear models (Ridge, Lasso, ElasticNet, Huber)
2. Train tree-based models (LightGBM, XGBoost)
3. Má»—i model Ä‘Æ°á»£c tune hyperparameters
4. ÄÃ¡nh giÃ¡ báº±ng 5-fold CV
5. Test trÃªn test set
6. So sÃ¡nh táº¥t cáº£ káº¿t quáº£

---

## ğŸ›ï¸ Hyperparameter Tuning: TÃ¬m Tham Sá»‘ Tá»‘i Æ¯u

### Chiáº¿n LÆ°á»£c 1: Grid Search (Cho Linear Models)

**Ã tÆ°á»Ÿng**: Thá»­ **Táº¤T Cáº¢** cÃ¡c combinations cá»§a hyperparameters

**VÃ­ dá»¥ vá»›i Ridge:**

```python
ridge_params = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100]
}
# 7 giÃ¡ trá»‹ alpha

# Grid Search sáº½ thá»­:
# alpha=0.001 â†’ 5-fold CV
# alpha=0.01 â†’ 5-fold CV
# alpha=0.1 â†’ 5-fold CV
# ...
# alpha=100 â†’ 5-fold CV

# Tá»•ng: 7 Ã— 5 = 35 láº§n train
```

**Code:**

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge

# Define model
ridge_model = Ridge(random_state=42)

# Define parameter grid
ridge_params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100]}

# Grid Search vá»›i 5-fold CV
search = GridSearchCV(
    ridge_model,
    ridge_params,
    cv=5,                           # 5-fold cross-validation
    scoring='neg_mean_squared_error', # Lower is better (negative)
    n_jobs=-1,                       # Use all CPUs
    verbose=0
)

# Fit vÃ  tÃ¬m best parameters
search.fit(X_train, y_train)

print(f"Best alpha: {search.best_params_['alpha']}")
# Output: Best alpha: 100
```

**VÃ­ dá»¥ vá»›i ElasticNet (2 tham sá»‘):**

```python
elasticnet_params = {
    'alpha': [0.001, 0.01, 0.1, 1],
    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
}
# 4 Ã— 5 = 20 combinations
# Ã— 5 CV folds = 100 láº§n train

# Best: alpha=0.1, l1_ratio=0.1
```

**Táº¡i sao dÃ¹ng Grid Search cho Linear Models?**

- âœ… KhÃ´ng gian tham sá»‘ nhá» (1-2 tham sá»‘)
- âœ… CÃ³ thá»ƒ thá»­ háº¿t â†’ TÃ¬m Ä‘Æ°á»£c **global optimum**
- âœ… KhÃ´ng tá»‘n quÃ¡ nhiá»u thá»i gian

### Chiáº¿n LÆ°á»£c 2: Randomized Search (Cho Tree Models)

**Ã tÆ°á»Ÿng**: Thá»­ **NGáºªU NHIÃŠN** má»™t sá»‘ combinations (khÃ´ng thá»­ háº¿t)

**VÃ­ dá»¥ vá»›i LightGBM:**

```python
lgb_params = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],      # 4 giÃ¡ trá»‹
    'num_leaves': [31, 50, 100, 200],             # 4 giÃ¡ trá»‹
    'max_depth': [3, 5, 7, 10],                   # 4 giÃ¡ trá»‹
    'min_child_samples': [20, 50, 100],           # 3 giÃ¡ trá»‹
    'subsample': [0.8, 0.9, 1.0],                 # 3 giÃ¡ trá»‹
    'colsample_bytree': [0.8, 0.9, 1.0]          # 3 giÃ¡ trá»‹
}
# Tá»•ng: 4Ã—4Ã—4Ã—3Ã—3Ã—3 = 4,320 combinations!

# Grid Search: 4,320 Ã— 5 CV = 21,600 láº§n train â†’ QUÃ NHIá»€U!

# Randomized Search: Chá»‰ thá»­ 30 combinations ngáº«u nhiÃªn
# 30 Ã— 5 CV = 150 láº§n train â†’ Há»£p lÃ½!
```

**Code:**

```python
from sklearn.model_selection import RandomizedSearchCV
import lightgbm as lgb

# Define model
lgb_model = lgb.LGBMRegressor(random_state=42, verbose=-1)

# Define parameter grid
lgb_params = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [31, 50, 100, 200],
    'max_depth': [3, 5, 7, 10],
    'min_child_samples': [20, 50, 100],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Randomized Search vá»›i 30 iterations
search = RandomizedSearchCV(
    lgb_model,
    lgb_params,
    n_iter=30,                     # Chá»‰ thá»­ 30 combinations
    cv=5,                          # 5-fold CV
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42,
    verbose=0
)

search.fit(X_train, y_train)

print(f"Best params: {search.best_params_}")
# Output: {
#     'subsample': 0.9,
#     'num_leaves': 200,
#     'min_child_samples': 20,
#     'max_depth': 3,
#     'learning_rate': 0.1,
#     'colsample_bytree': 0.8
# }
```

**Táº¡i sao dÃ¹ng Randomized Search cho Tree Models?**

- âœ… KhÃ´ng gian tham sá»‘ **ráº¥t lá»›n** (6 tham sá»‘)
- âœ… Grid Search sáº½ tá»‘n quÃ¡ nhiá»u thá»i gian
- âœ… Randomized Search thÆ°á»ng tÃ¬m Ä‘Æ°á»£c vÃ¹ng tá»‘t vá»›i Ã­t iterations hÆ¡n

### So SÃ¡nh Grid Search vs Randomized Search

| **TiÃªu chÃ­**           | **Grid Search**        | **Randomized Search** |
| ---------------------- | ---------------------- | --------------------- |
| **KhÃ´ng gian tham sá»‘** | Nhá» (1-2 tham sá»‘)      | Lá»›n (5+ tham sá»‘)      |
| **Sá»‘ láº§n thá»­**         | Táº¥t cáº£ combinations    | Chá»‰ má»™t pháº§n (n_iter) |
| **Káº¿t quáº£**            | Global optimum         | Good enough           |
| **Thá»i gian**          | Cháº­m vá»›i nhiá»u tham sá»‘ | Nhanh hÆ¡n             |
| **Khi nÃ o dÃ¹ng?**      | Linear models          | Tree-based models     |

---

## ğŸ“Š Káº¿t Quáº£ So SÃ¡nh Models

Sau khi train táº¥t cáº£ 6 models, Ä‘Ã¢y lÃ  káº¿t quáº£:

| **Rank** | **Model**      | **RMSE**   | **MAE**    | **RÂ²**     | **CV Score** |
| -------- | -------------- | ---------- | ---------- | ---------- | ------------ |
| ğŸ¥‡ **1** | **LightGBM**   | **0.1249** | **0.0839** | **0.9058** | **0.01768**  |
| ğŸ¥ˆ **2** | **Lasso**      | 0.1258     | 0.0859     | 0.9045     | 0.02043      |
| ğŸ¥‰ **3** | **ElasticNet** | 0.1276     | 0.0879     | 0.9017     | 0.02020      |
| 4ï¸âƒ£       | **XGBoost**    | 0.1288     | 0.0854     | 0.8998     | 0.01825      |
| 5ï¸âƒ£       | **Ridge**      | 0.1329     | 0.0883     | 0.8933     | 0.02222      |
| 6ï¸âƒ£       | **Huber**      | 0.1901     | 0.0897     | 0.7820     | 0.04617      |

### ğŸ“ˆ Visualization

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c biá»ƒu Ä‘á»“ so sÃ¡nh chi tiáº¿t giá»¯a cÃ¡c models:

#### Dashboard So SÃ¡nh Chi Tiáº¿t

Biá»ƒu Ä‘á»“ nÃ y hiá»ƒn thá»‹ 6 gÃ³c nhÃ¬n khÃ¡c nhau vá» performance cá»§a cÃ¡c models:

![Model Comparison Dashboard](models/model_comparison.png)

**Giáº£i thÃ­ch cÃ¡c biá»ƒu Ä‘á»“:**

1. **RMSE Comparison** (trÃªn trÃ¡i): LightGBM cÃ³ RMSE tháº¥p nháº¥t â†’ Sai sá»‘ nhá» nháº¥t
2. **RÂ² Comparison** (trÃªn giá»¯a): LightGBM cÃ³ RÂ² cao nháº¥t â†’ Giáº£i thÃ­ch nhiá»u variance nháº¥t
3. **MAE Comparison** (trÃªn pháº£i): LightGBM cÃ³ MAE tháº¥p nháº¥t â†’ Sai sá»‘ tuyá»‡t Ä‘á»‘i nhá» nháº¥t
4. **CV Score Comparison** (dÆ°á»›i trÃ¡i): LightGBM cÃ³ CV Score tháº¥p nháº¥t â†’ Model á»•n Ä‘á»‹nh nháº¥t
5. **All Metrics Comparison** (dÆ°á»›i giá»¯a): So sÃ¡nh táº¥t cáº£ metrics Ä‘Ã£ normalized
6. **Ranking Heatmap** (dÆ°á»›i pháº£i): LightGBM Ä‘á»©ng Ä‘áº§u táº¥t cáº£ metrics

#### TÃ³m Táº¯t Nhanh

Biá»ƒu Ä‘á»“ nÃ y highlight model tá»‘t nháº¥t vÃ  so sÃ¡nh RMSE vs RÂ²:

![Model Summary](models/model_summary.png)

**Nháº­n xÃ©t:**

- **BÃªn trÃ¡i**: RMSE vÃ  RÂ² Ä‘Æ°á»£c hiá»ƒn thá»‹ cáº¡nh nhau cho táº¥t cáº£ models
- **BÃªn pháº£i**: LightGBM Ä‘Æ°á»£c highlight mÃ u xanh lÃ¡ (green) - lÃ  best model
- CÃ³ thá»ƒ tháº¥y LightGBM cÃ³ **RMSE tháº¥p nháº¥t** vÃ  **RÂ² cao nháº¥t**

#### CÃ¡c Plots Bá»• Sung

NgoÃ i 2 plots chÃ­nh á»Ÿ trÃªn, dÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c plots bá»• sung giÃºp hiá»ƒu sÃ¢u hÆ¡n vá» model:

**1. Residuals Plot & Actual vs Predicted**

Biá»ƒu Ä‘á»“ nÃ y giÃºp kiá»ƒm tra cháº¥t lÆ°á»£ng model:

![Model Residuals and Predictions](models/model_residuals.png)

**Giáº£i thÃ­ch:**

- **BÃªn trÃ¡i (Residuals Plot)**:

  - ÄÆ°á»ng Ä‘á» nÃ©t Ä‘á»©t = Perfect prediction (residual = 0)
  - Äiá»ƒm xanh = Má»—i cÄƒn nhÃ  trong test set
  - âœ… Points phÃ¢n bá»‘ **ngáº«u nhiÃªn** quanh 0 â†’ Model tá»‘t, khÃ´ng bias
  - âŒ Points táº¡o pattern â†’ Model thiáº¿u features

- **BÃªn pháº£i (Actual vs Predicted)**:
  - ÄÆ°á»ng Ä‘á» nÃ©t Ä‘á»©t = Perfect prediction line
  - Äiá»ƒm xanh lÃ¡ = Má»—i cÄƒn nhÃ 
  - âœ… Points náº±m **gáº§n Ä‘Æ°á»ng Ä‘á»** â†’ Model dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c
  - âœ… Points phÃ¢n bá»‘ **Ä‘á»u 2 bÃªn** â†’ Model khÃ´ng bias

**Nháº­n xÃ©t vá» LightGBM:**

- Residuals phÃ¢n bá»‘ Ä‘á»u quanh 0 â†’ Model khÃ´ng cÃ³ bias rÃµ rÃ ng
- Actual vs Predicted: Háº§u háº¿t points náº±m gáº§n Ä‘Æ°á»ng perfect prediction â†’ Model dá»± Ä‘oÃ¡n ráº¥t chÃ­nh xÃ¡c!

**2. Feature Importance Plot**

Biá»ƒu Ä‘á»“ nÃ y cho tháº¥y features nÃ o quan trá»ng nháº¥t:

![Feature Importance](models/model_feature_importance.png)

**Giáº£i thÃ­ch:**

- Features á»Ÿ **trÃªn cÃ¹ng** = Quan trá»ng nháº¥t
- Features á»Ÿ **dÆ°á»›i** = Ãt quan trá»ng hÆ¡n
- Chiá»u dÃ i thanh = Má»©c Ä‘á»™ quan trá»ng

**Top Features quan trá»ng nháº¥t cá»§a LightGBM:**

1. **OverallQual** - Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ (quan trá»ng nháº¥t!)
2. **Neighborhood_target_enc** - Khu vá»±c (target encoding thÃ nh cÃ´ng)
3. **GrLivArea_log** - Diá»‡n tÃ­ch sá»‘ng (sau log transform)
4. **GarageArea_yj** - Diá»‡n tÃ­ch garage
5. **1stFlrSF_log** - Diá»‡n tÃ­ch táº§ng 1

**Insights:**

- âœ… **Cháº¥t lÆ°á»£ng vÃ  Vá»‹ trÃ­** lÃ  2 yáº¿u tá»‘ quan trá»ng nháº¥t
- âœ… Feature engineering thÃ nh cÃ´ng: Neighborhood Ä‘Æ°á»£c target encoding â†’ ráº¥t quan trá»ng
- âœ… CÃ¡c features má»›i (residuals, transformed features) cÅ©ng cÃ³ vai trÃ²

### ğŸ” Nháº­n XÃ©t Tá»•ng Quan

1. **ğŸ¥‡ LightGBM tháº¯ng Ã¡p Ä‘áº£o**

   - RMSE tháº¥p nháº¥t: 0.1249
   - RÂ² cao nháº¥t: 0.9058 (giáº£i thÃ­ch 90.58% variance)
   - CV Score tháº¥p nháº¥t: 0.01768 (á»•n Ä‘á»‹nh nháº¥t)

2. **ğŸ¥ˆ Lasso Ä‘á»©ng thá»© 2, gáº§n nhÆ° ngang LightGBM!**

   - ChÃªnh lá»‡ch RMSE chá»‰ 0.0009 (ráº¥t nhá»!)
   - RÂ² = 0.9045 (gáº§n nhÆ° LightGBM)

3. **Tree-based models (LightGBM, XGBoost) tá»‘t hÆ¡n linear models**

   - LightGBM vÃ  XGBoost Ä‘á»u top 4
   - Chá»©ng tá» dá»¯ liá»‡u cÃ³ pattern phá»©c táº¡p

4. **Linear models váº«n ráº¥t tá»‘t**

   - Lasso vÃ  ElasticNet Ä‘á»©ng top 3
   - PhÃ¹ há»£p lÃ m baseline

5. **Huber kÃ©m nháº¥t**
   - RÂ² = 0.7820 (tháº¥p hÆ¡n nhiá»u)
   - CÃ³ thá»ƒ do robust loss khÃ´ng phÃ¹ há»£p vá»›i dá»¯ liá»‡u Ä‘Ã£ clean

---

## ğŸ”¬ PhÃ¢n TÃ­ch Chi Tiáº¿t Tá»«ng Model

### ğŸ¥‡ LightGBM: NgÆ°á»i Tháº¯ng Cuá»™c

**Best Parameters:**

```python
{
    'subsample': 0.9,           # DÃ¹ng 90% samples má»—i tree (trÃ¡nh overfitting)
    'num_leaves': 200,          # Tá»‘i Ä‘a 200 lÃ¡ (Ä‘á»™ phá»©c táº¡p)
    'min_child_samples': 20,    # Tá»‘i thiá»ƒu 20 samples má»—i lÃ¡ (trÃ¡nh overfitting)
    'max_depth': 3,             # Äá»™ sÃ¢u tá»‘i Ä‘a = 3 (cÃ¢y nÃ´ng, generalization tá»‘t)
    'learning_rate': 0.1,       # Tá»‘c Ä‘á»™ há»c = 0.1 (vá»«a pháº£i)
    'colsample_bytree': 0.8     # DÃ¹ng 80% features má»—i tree (tÄƒng diversity)
}
```

**Táº¡i sao LightGBM tá»‘t nháº¥t?**

1. **Xá»­ lÃ½ Feature Interactions tá»‘t**

   - Dá»¯ liá»‡u nhÃ  Ä‘áº¥t cÃ³ nhiá»u interactions phá»©c táº¡p:
     - `OverallQual Ã— Neighborhood`: NhÃ  cháº¥t lÆ°á»£ng tá»‘t á»Ÿ khu tá»‘t = giÃ¡ ráº¥t cao
     - `GrLivArea Ã— HouseAge`: NhÃ  lá»›n cÅ© = giÃ¡ tháº¥p hÆ¡n nhÃ  lá»›n má»›i
   - Tree-based models tá»± Ä‘á»™ng báº¯t Ä‘Æ°á»£c cÃ¡c interactions nÃ y

2. **Hyperparameters Ä‘Æ°á»£c tune tá»‘t**

   - `max_depth=3`: CÃ¢y nÃ´ng â†’ TrÃ¡nh overfitting
   - `subsample=0.9`, `colsample_bytree=0.8`: ThÃªm regularization
   - Balance tá»‘t giá»¯a **bias** (Ä‘á»™ chÃ­nh xÃ¡c) vÃ  **variance** (overfitting)

3. **Performance vÆ°á»£t trá»™i**
   - RÂ² = 0.9058: Giáº£i thÃ­ch Ä‘Æ°á»£c **90.58% variance** trong giÃ¡ nhÃ 
   - CV Score = 0.01768: Tháº¥p nháº¥t â†’ Model á»•n Ä‘á»‹nh nháº¥t

**Trade-off:**

- âŒ **Interpretability**: KhÃ³ giáº£i thÃ­ch (black box)
- âŒ **Resource**: Cáº§n nhiá»u RAM/CPU hÆ¡n linear models
- âœ… **Accuracy**: Tá»‘t nháº¥t trong 6 models

#### ğŸ“Š Feature Importance cá»§a LightGBM

LightGBM cÃ³ thá»ƒ cho ta biáº¿t **features nÃ o quan trá»ng nháº¥t**:

**Top Features quan trá»ng nháº¥t** (theo feature importance):

1. **OverallQual**: Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ (quan trá»ng nháº¥t!)
2. **Neighborhood**: Khu vá»±c (Ä‘Ã£ Ä‘Æ°á»£c target encoding)
3. **GrLivArea**: Diá»‡n tÃ­ch sá»‘ng
4. **GarageArea**: Diá»‡n tÃ­ch garage
5. **TotalBsmtSF / BasementResid**: Diá»‡n tÃ­ch háº§m

**Ã nghÄ©a:**

- âœ… **Cháº¥t lÆ°á»£ng** vÃ  **Vá»‹ trÃ­** lÃ  2 yáº¿u tá»‘ quan trá»ng nháº¥t
- âœ… Feature engineering Ä‘Ã£ thÃ nh cÃ´ng (Neighborhood Ä‘Æ°á»£c target encoding â†’ ráº¥t quan trá»ng)
- âœ… CÃ¡c features má»›i táº¡o (BasementResid, GarageAreaPerCar) cÅ©ng cÃ³ vai trÃ²

> ğŸ’¡ **Tip**: Báº¡n cÃ³ thá»ƒ xem feature importance plot chi tiáº¿t báº±ng cÃ¡ch cháº¡y `create_additional_plots.py`

### ğŸ¥ˆ Lasso: Ráº¥t Gáº§n LightGBM!

**Best Parameters:**

```python
{'alpha': 0.01}  # L1 penalty vá»«a pháº£i â†’ Chá»n features quan trá»ng
```

**Äiá»ƒm ÄÃ¡ng ChÃº Ã:**

1. **ChÃªnh lá»‡ch ráº¥t nhá» vá»›i LightGBM**

   - RMSE: 0.1258 vs 0.1249 (chÃªnh 0.0009 - chá»‰ 0.7%!)
   - RÂ²: 0.9045 vs 0.9058 (chÃªnh 0.0013)

2. **Interpretability cao**

   - CÃ³ thá»ƒ xem **feature importance** qua há»‡ sá»‘
   - Features cÃ³ há»‡ sá»‘ = 0 â†’ KhÃ´ng quan trá»ng (Ä‘Ã£ bá»‹ loáº¡i bá»)
   - Features cÃ³ há»‡ sá»‘ lá»›n â†’ Quan trá»ng

3. **ÄÆ¡n giáº£n hÆ¡n LightGBM**
   - Chá»‰ 1 hyperparameter (`alpha`)
   - Train nhanh hÆ¡n
   - Dá»… deploy hÆ¡n

**Khi nÃ o nÃªn dÃ¹ng Lasso thay LightGBM?**

âœ… **NÃªn dÃ¹ng Lasso khi:**

- Cáº§n **explainability** (stakeholders muá»‘n hiá»ƒu táº¡i sao model dá»± Ä‘oÃ¡n nhÆ° váº­y)
- Deploy trÃªn **resource háº¡n cháº¿** (edge devices, mobile apps)
- Cáº§n **baseline Ä‘Æ¡n giáº£n** trÆ°á»›c khi thá»­ ensemble

âŒ **NÃªn dÃ¹ng LightGBM khi:**

- Æ¯u tiÃªn **accuracy** cao nháº¥t
- CÃ³ Ä‘á»§ resource
- CÃ³ thá»ƒ dÃ¹ng SHAP/LIME Ä‘á»ƒ explain

### ğŸ¥‰ ElasticNet: Káº¿t Há»£p Tá»‘t

**Best Parameters:**

```python
{
    'alpha': 0.1,
    'l1_ratio': 0.1  # 10% L1, 90% L2 â†’ Gáº§n giá»‘ng Ridge
}
```

**PhÃ¢n TÃ­ch:**

1. **l1_ratio = 0.1 â†’ Gáº§n nhÆ° Ridge**

   - 90% L2 regularization, 10% L1
   - NÃªn performance gáº§n vá»›i Ridge (nhÆ°ng tá»‘t hÆ¡n má»™t chÃºt)

2. **Táº¡i sao khÃ´ng pháº£i l1_ratio = 0.5?**

   - Dá»¯ liá»‡u sau feature engineering váº«n cÃ²n **multicollinearity**
   - L2 phÃ¹ há»£p vá»›i multicollinearity hÆ¡n L1
   - â†’ Model tá»± Ä‘á»™ng chá»n L2 nhiá»u hÆ¡n

3. **Performance tá»‘t**
   - RÂ² = 0.9017: Giáº£i thÃ­ch Ä‘Æ°á»£c 90.17% variance
   - Äá»©ng thá»© 3, tá»‘t hÆ¡n XGBoost

### 4ï¸âƒ£ XGBoost: Gáº§n nhÆ° LightGBM

**Best Parameters:**

```python
{
    'subsample': 0.9,
    'min_child_weight': 3,
    'max_depth': 3,
    'learning_rate': 0.1,
    'gamma': 0,
    'colsample_bytree': 0.8
}
```

**Nháº­n XÃ©t:**

1. **Performance tá»‘t nhÆ°ng kÃ©m LightGBM má»™t chÃºt**

   - RMSE: 0.1288 vs 0.1249 (chÃªnh 3.1%)
   - RÂ²: 0.8998 vs 0.9058

2. **Táº¡i sao kÃ©m hÆ¡n LightGBM?**

   - Thuáº­t toÃ¡n khÃ¡c má»™t chÃºt (XGBoost dÃ¹ng level-wise, LightGBM dÃ¹ng leaf-wise)
   - LightGBM thÆ°á»ng **nhanh hÆ¡n vÃ  chÃ­nh xÃ¡c hÆ¡n** vá»›i datasets vá»«a
   - Hyperparameters cÃ³ thá»ƒ chÆ°a tá»‘i Æ°u

3. **Váº«n lÃ  model ráº¥t tá»‘t**
   - Äá»©ng thá»© 4 trong 6 models
   - CÃ³ thá»ƒ cáº£i thiá»‡n vá»›i tuning ká»¹ hÆ¡n

### 5ï¸âƒ£ Ridge: Baseline Tá»‘t

**Best Parameters:**

```python
{'alpha': 100}  # Regularization máº¡nh â†’ Há»‡ sá»‘ nhá», á»•n Ä‘á»‹nh
```

**Nháº­n XÃ©t:**

1. **Performance Ä‘Ã¡ng khen**

   - RÂ² = 0.8933: Giáº£i thÃ­ch Ä‘Æ°á»£c 89.33% variance
   - ÄÃ¢y lÃ  **baseline ráº¥t tá»‘t** cho linear models

2. **Alpha = 100 â†’ Regularization máº¡nh**

   - Há»‡ sá»‘ cá»§a model sáº½ nhá» â†’ TrÃ¡nh overfitting
   - Model á»•n Ä‘á»‹nh, generalization tá»‘t

3. **Æ¯u Ä‘iá»ƒm**
   - âœ… ÄÆ¡n giáº£n, dá»… hiá»ƒu
   - âœ… Interpretable (xem Ä‘Æ°á»£c há»‡ sá»‘)
   - âœ… Nhanh, khÃ´ng tá»‘n resource

### 6ï¸âƒ£ Huber: KhÃ´ng PhÃ¹ Há»£p

**Best Parameters:**

```python
{
    'alpha': 0.1,
    'epsilon': 1.2,
    'fit_intercept': True
}
```

**Táº¡i Sao KÃ©m?**

1. **RÂ² = 0.7820 â†’ Tháº¥p nháº¥t**

   - Giáº£i thÃ­ch Ä‘Æ°á»£c chá»‰ 78.20% variance
   - KÃ©m hÆ¡n LightGBM ~13%

2. **Robust Loss khÃ´ng phÃ¹ há»£p**

   - Huber Ä‘Æ°á»£c thiáº¿t káº¿ cho **dá»¯ liá»‡u cÃ³ nhiá»u outliers**
   - NhÆ°ng dá»¯ liá»‡u cá»§a chÃºng ta **Ä‘Ã£ clean** (outliers Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ há»£p lÃ½)
   - â†’ Robust loss che máº¥t cÃ¡c tÃ­n hiá»‡u quan trá»ng

3. **KhÃ´ng nÃªn dÃ¹ng cho bÃ i toÃ¡n nÃ y**

---

## âœ… Káº¿t Luáº­n: Chá»n LightGBM

### TiÃªu ChÃ­ Chá»n Model

ChÃºng ta Ä‘Ã¡nh giÃ¡ models theo 4 tiÃªu chÃ­:

| **TiÃªu chÃ­**            | **LightGBM**             | **Lasso**       | **Ghi chÃº**          |
| ----------------------- | ------------------------ | --------------- | -------------------- |
| âœ… **Accuracy**         | RMSE = 0.1249 (tá»‘t nháº¥t) | RMSE = 0.1258   | LightGBM tháº¯ng       |
| âœ… **Stability**        | CV = 0.01768 (tháº¥p nháº¥t) | CV = 0.02043    | LightGBM á»•n Ä‘á»‹nh hÆ¡n |
| âœ… **Generalization**   | Test â‰ˆ CV                | Test â‰ˆ CV       | Cáº£ 2 Ä‘á»u tá»‘t         |
| âš ï¸ **Interpretability** | Tháº¥p (black box)         | Cao (xem há»‡ sá»‘) | Lasso tháº¯ng          |

### Quyáº¿t Äá»‹nh Cuá»‘i CÃ¹ng

**âœ… Chá»n LightGBM vÃ¬:**

1. **Accuracy cao nháº¥t**: RMSE 0.1249, RÂ² 0.9058
2. **Stability tá»‘t nháº¥t**: CV Score 0.01768 (tháº¥p nháº¥t)
3. **Generalization tá»‘t**: Test performance gáº§n CV performance
4. **Interpretability cÃ³ thá»ƒ bÃ¹**: DÃ¹ng SHAP/LIME Ä‘á»ƒ explain

**Files Ä‘Ã£ lÆ°u:**

- âœ… `models/best_model.pkl`: Model Ä‘Ã£ train
- âœ… `models/best_model_features.json`: Danh sÃ¡ch features
- âœ… `models/best_model_config.json`: Configuration

### Next Steps

Sau khi chá»n LightGBM, báº¡n cÃ³ thá»ƒ:

1. **Deploy vÃ o Production**

   - Load model tá»« `best_model.pkl`
   - Predict giÃ¡ nhÃ  má»›i
   - Xem code trong `src/Serving.py`

2. **Explain Predictions**

   - DÃ¹ng SHAP values Ä‘á»ƒ giáº£i thÃ­ch
   - Xem code trong `src/Explainability.py`

3. **Monitor Performance**

   - Theo dÃµi model qua thá»i gian
   - Retrain khi cÃ³ data má»›i

4. **Táº¡o ThÃªm Plots Chi Tiáº¿t**
   - Cháº¡y `python create_additional_plots.py` Ä‘á»ƒ táº¡o:
     - Residuals plot (kiá»ƒm tra model quality)
     - Feature importance plot (xem features nÃ o quan trá»ng)

---

## ğŸ“ Tá»•ng Káº¿t

Trong bÃ i viáº¿t nÃ y, chÃºng ta Ä‘Ã£ há»c:

âœ… **Táº¡i sao cáº§n Model Selection**: KhÃ´ng cÃ³ model hoÃ n háº£o cho má»i bÃ i toÃ¡n

âœ… **Metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡**: RMSE, MAE, RÂ²

âœ… **Cross-Validation**: Kiá»ƒm tra model á»•n Ä‘á»‹nh

âœ… **Hyperparameter Tuning**: Grid Search vs Randomized Search

âœ… **So sÃ¡nh 6 models**: LightGBM tháº¯ng, nhÆ°ng Lasso cÅ©ng ráº¥t gáº§n

âœ… **Chá»n model tá»‘t nháº¥t**: LightGBM vá»›i RÂ² = 0.9058

**BÃ i há»c quan trá»ng:**

- ğŸ¯ **KhÃ´ng cÃ³ model nÃ o lÃ  hoÃ n háº£o**: Má»—i model cÃ³ trade-off riÃªng
- ğŸ¯ **Tune hyperparameters quan trá»ng**: CÃ¹ng má»™t model, tune khÃ¡c â†’ káº¿t quáº£ khÃ¡c
- ğŸ¯ **So sÃ¡nh nhiá»u models**: Äá»«ng chá»‰ train 1 model, hÃ£y so sÃ¡nh nhiá»u models
- ğŸ¯ **Accuracy khÃ´ng pháº£i táº¥t cáº£**: Cáº§n cÃ¢n nháº¯c interpretability, resource, v.v.

---

**Happy Learning! ğŸš€**

Náº¿u cÃ³ cÃ¢u há»i, hÃ£y xem code chi tiáº¿t táº¡i:

- `src/Modeling.py`: Implementation Ä‘áº§y Ä‘á»§
- `models/model_comparison.csv`: Káº¿t quáº£ chi tiáº¿t
- `models/model_comparison.png`: Dashboard so sÃ¡nh chi tiáº¿t (6 biá»ƒu Ä‘á»“)
- `models/model_summary.png`: TÃ³m táº¯t nhanh RMSE vs RÂ²
- `create_additional_plots.py`: Script táº¡o thÃªm plots (residuals, feature importance)
- `reports/ModelReport.md`: BÃ¡o cÃ¡o ngáº¯n gá»n

---

### ğŸ“Œ TÃ³m Táº¯t: Táº¥t Cáº£ Plots ÄÃ£ ÄÆ°á»£c ThÃªm VÃ o Blog

**Tá»•ng káº¿t cÃ¡c plots trong blog:**

1. âœ… **`model_comparison.png`** - Dashboard so sÃ¡nh chi tiáº¿t 6 models (6 biá»ƒu Ä‘á»“)
2. âœ… **`model_summary.png`** - TÃ³m táº¯t nhanh RMSE vs RÂ²
3. âœ… **`model_residuals.png`** - Residuals Plot & Actual vs Predicted (Ä‘Ã£ thÃªm vÃ o blog)
4. âœ… **`model_feature_importance.png`** - Feature Importance Plot (Ä‘Ã£ thÃªm vÃ o blog)

**Táº¥t cáº£ plots Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p Ä‘áº§y Ä‘á»§ vÃ o blog vá»›i giáº£i thÃ­ch chi tiáº¿t!** ğŸ¯

ğŸ’¡ **LÆ°u Ã½**: CÃ¡c plots bá»• sung giÃºp blog **toÃ n diá»‡n vÃ  dá»… hiá»ƒu hÆ¡n** cho ngÆ°á»i má»›i há»c AI!
