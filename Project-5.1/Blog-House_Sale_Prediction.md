# I. Giá»›i thiá»‡u:

Báº¡n Ä‘Ã£ bao giá» tá»± há»i: **"Táº¡i sao cÄƒn nhÃ  nÃ y láº¡i Ä‘áº¯t gáº¥p 4 láº§n cÄƒn kia?"**
Trong project nÃ y, bá»n mÃ¬nh khÃ´ng chá»‰ build má»™t model dá»± bÃ¡o giÃ¡ nhÃ . Bá»n mÃ¬nh xÃ¢y dá»±ng má»™t **production-ready pipeline** tá»« A-Z, vá»›i focus vÃ o:

- ğŸ¯ **Data Quality:** KhÃ´ng bá» qua má»™t null value, má»™t logical error nÃ o
- ğŸ”’ **Zero Leakage:** Early split + cross-fit encoding Ä‘Ãºng chuáº©n
- ğŸ“Š **Feature Engineering:** Má»—i feature má»›i Ä‘á»u cÃ³ Ã½ nghÄ©a real-estate rÃµ rÃ ng
- ğŸ¨ **Transformation:** Giáº£m skewness tá»« 2.009 â†’ 0.205 (89.8%!)
- ğŸš€ **Production-Ready:** Modular code, config-driven, fully reproducible

**Äiá»ƒm khÃ¡c biá»‡t:** KhÃ´ng pháº£i tutorial "lÃ m theo", mÃ  lÃ  má»™t **case study thá»±c chiáº¿n** vá»›i decision rationale, trade-offs, vÃ  lessons learned tá»«ng bÆ°á»›c.

# II. CÃ¡c thÃ¡ch thá»©c:

#### 1ï¸âƒ£Â **Missing Values Chaos**

```
Total nulls: 7,829 (6.6% of dataset!)

Top missing features:
  PoolQC       99.5% missing (1,453/1,460)
  MiscFeature  96.3% missing (1,406/1,460)
  Alley        93.8% missing (1,369/1,460)
  Fence        80.8% missing (1,179/1,460)
  FireplaceQu  47.3% missing (690/1,460)
```

ğŸ’¡Â **Insight:**Â Missing â‰  Error!Â `PoolQC`Â missing nghÄ©a lÃ  "no pool", khÃ´ng pháº£i lá»—i nháº­p liá»‡u.

#### 2ï¸âƒ£Â **Logical Inconsistencies**

```python
# Example: MasVnrArea vs MasVnrType
Case 1: MasVnrArea = 0, MasVnrType = 'BrkFace' âŒ
  â†’ KhÃ´ng cÃ³ veneer nhÆ°ng láº¡i cÃ³ type? DELETE!

Case 2: MasVnrArea = 288, MasVnrType = NULL âš ï¸
  â†’ CÃ³ veneer nhÆ°ng thiáº¿u type? FILL with mode!
```

#### 3ï¸âƒ£Â **Extreme Skewness**

```
SalePrice skewness: 2.009 (highly right-skewed)
  â†’ Violates normality assumption cho linear regression
  â†’ Cáº§n transform!
```

![Missing Values and Logical Errors Illustration](images/Pasted%20image%2020251028104423.png)

#### 4ï¸âƒ£Â **High Cardinality Categoricals**

```
Neighborhood: 25 unique values
Exterior2nd:  16 unique values
  â†’ One-hot encoding = 41 sparse columns!
  â†’ Giáº£i phÃ¡p: Target encoding
```

#### 5ï¸âƒ£ High Correlation Features

CÃ¡c cáº·p biáº¿n cÃ³ tÆ°Æ¡ng quan > 80%:

| ID   | **Feature 1** | **Feature 2** | **Correlation** |
| ---- | ------------- | ------------- | --------------- |
| 988  | GarageCars    | GarageArea    | 0.882475        |
| 1025 | GarageArea    | GarageCars    | 0.882475        |
| 246  | YearBuilt     | GarageYrBlt   | 0.825667        |
| 931  | GarageYrBlt   | YearBuilt     | 0.825667        |
| 614  | GrLivArea     | TotRmsAbvGrd  | 0.825489        |
| 867  | TotRmsAbvGrd  | GrLivArea     | 0.825489        |
| 493  | 1stFlrSF      | TotalBsmtSF   | 0.819530        |
| 456  | TotalBsmtSF   | 1stFlrSF      | 0.819530        |

# III. CÃ¡ch xá»­ lÃ½ vÃ  cÃ¡c bÆ°á»›c trong Data Preparationeparation

## 1. Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAW DATA (1,460 Ã— 81)                                       â”‚
â”‚  â”œâ”€ 7,829 nulls                                              â”‚
â”‚  â”œâ”€ Logical errors                                           â”‚
â”‚  â””â”€ Mixed data types                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: PREPROCESSING                                       â”‚
â”‚  â”œâ”€ Fix MasVnr logic (delete 2 rows)                        â”‚
â”‚  â”œâ”€ Fill 6,940 nulls                                        â”‚
â”‚  â”œâ”€ Fix Garage consistency                                  â”‚
â”‚  â””â”€ Train/Test Split (85/15)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                          â”‚
                  â–¼                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TRAIN       â”‚            â”‚ TEST        â”‚
         â”‚ 1,239 Ã— 81  â”‚            â”‚ 219 Ã— 81    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                          â”‚
                  â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: FEATURE ENGINEERING                                 â”‚
â”‚  â”œâ”€ Create derived features (+6)                            â”‚
â”‚  â”œâ”€ Orthogonalize residuals                                 â”‚
â”‚  â””â”€ Binary flags (Has* features)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: TRANSFORMATION                                      â”‚
â”‚  â”œâ”€ log1p(SalePrice): skew 2.009 â†’ 0.205                   â”‚
â”‚  â”œâ”€ log1p for 15 features                                   â”‚
â”‚  â”œâ”€ Yeo-Johnson for 9 features                              â”‚
â”‚  â””â”€ Binning: KitchenAbvGr                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: ENCODING                                            â”‚
â”‚  â”œâ”€ Ordinal: 17 features (Quality scales)                   â”‚
â”‚  â”œâ”€ One-Hot: 110 features (Nominal cats)                    â”‚
â”‚  â”œâ”€ Target Encoding: 2 features (Cross-fit K=5)            â”‚
â”‚  â””â”€ StandardScaler: All 176 features                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FINAL: PRODUCTION-READY                                     â”‚
â”‚  â”œâ”€ 1,239 Ã— 173 (train)                                     â”‚
â”‚  â”œâ”€ 219 Ã— 173 (test)                                        â”‚
â”‚  â”œâ”€ 0 nulls, 0 logical errors                               â”‚
â”‚  â””â”€ Zero leakage, fully reproducible                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Preprocessing (Step 1)

#### Sá»­a logic MasVnrType/MasVnrArea:

**Implementation:**

```python
def fix_masonry_veneer_logic(self):
	original_len = len(self.df)

	# Get mode cá»§a MasVnrType (for Case 2)
	mode_type = self.df['MasVnrType'].mode()[0]

	# Case 1: Area=0, Typeâ‰ NULL â†’ DELETE
	case1_mask = (self.df['MasVnrArea'] == 0) & (self.df['MasVnrType'].notna())
	case1_count = case1_mask.sum()
	self.df = self.df[~case1_mask]

	# Case 2: Area>0, Type=NULL â†’ FILL mode
	case2_mask = (self.df['MasVnrArea'] > 0) & (self.df['MasVnrType'].isna())
	case2_count = case2_mask.sum()
	self.df.loc[case2_mask, 'MasVnrType'] = mode_type

	# Case 3: Both NULL â†’ 'None'
	case3_mask = (self.df['MasVnrArea'].isna()) | (self.df['MasVnrType'].isna())
	case3_count = case3_mask.sum()
	if case3_count > 0:
		self.df.loc[case3_mask, 'MasVnrType'] = 'None'
		self.df.loc[case3_mask, 'MasVnrArea'] = 0

	deleted = original_len - len(self.df)

	return self
```

=> XoÃ¡ 2 dÃ²ng mÃ¢u thuáº«n; cÃ²n **1458 Ã— 81**.

| **Before**                        | **After**      | **Action**       |
| --------------------------------- | -------------- | ---------------- |
| Id=689: Area=0, Type='BrkFace' âŒ | DELETED        | Logical error    |
| Id=1242: Area=0, Type='Stone' âŒ  | DELETED        | Logical error    |
| Id=625: Area=288, Type=NULL âœ…    | Type='BrkFace' | Filled with mode |

#### Fill missing values cÃ³ chá»§ Ä‘Ã­ch:

Danh má»¥c (Categories) â†’ `'None'`; biáº¿n Ä‘áº¿m/diá»‡n tÃ­ch (Numerics) â†’ `0`; sá»‘ khÃ¡c â†’ **median**.

**Implementation:**

```python
def fill_missing_values(self):
	total_nulls_before = self.df.isnull().sum().sum()

	# Categorical columns â†’ 'None'
	categorical_cols = self.df.select_dtypes(include=['object']).columns
	cat_nulls = self.df[categorical_cols].isnull().sum().sum()

	if cat_nulls > 0:
		for col in categorical_cols:
			if self.df[col].isnull().sum() > 0:
				self.df[col] = self.df[col].fillna('None')

	# Numeric columns: count/area â†’ 0, others â†’ median
	numeric_cols = self.df.select_dtypes(include=[np.number]).columns

	# Count/area columns â†’ 0
	count_area_cols = ['GarageCars', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2',
					'BsmtUnfSF', 'TotalBsmtSF', 'FullBath', 'HalfBath',
					'BedroomAbvGr', 'KitchenAbvGr', 'WoodDeckSF', 'OpenPorchSF',
					'EnclosedPorch', 'ScreenPorch', '3SsnPorch', 'PoolArea',
					'MasVnrArea', '2ndFlrSF', 'BsmtFullBath', 'BsmtHalfBath']

	for col in count_area_cols:
		if col in self.df.columns and self.df[col].isnull().sum() > 0:
			self.df[col] = self.df[col].fillna(0)

	count_area_nulls = sum(self.df[col].isnull().sum() \
		for col in count_area_cols if col in self.df.columns)

	# Other numeric â†’ median
	other_numeric_nulls = self.df[numeric_cols].isnull().sum().sum()
	if other_numeric_nulls > 0:
		for col in numeric_cols:
			if self.df[col].isnull().sum() > 0:
				median_val = self.df[col].median()
				self.df[col] = self.df[col].fillna(median_val)

	total_nulls_after = self.df.isnull().sum().sum()

	return self
```

#### Garage consistency:

KhÃ³a cháº·t logic náº¿u `GarageArea=0` thÃ¬ toÃ n bá»™ thuá»™c tÃ­nh garage = `'None'`.

**Implementation:**

```python
def fix_garage_logic(self):
	garage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']

	# Find rows without garage
	no_garage_mask = self.df['GarageArea'] == 0
	no_garage_count = no_garage_mask.sum()

	# Set 'None' for rows without garage
	for col in garage_cols:
		if col in self.df.columns:
			self.df.loc[no_garage_mask, col] = 'None'

	# Fill remaining nulls vá»›i mode
	for col in garage_cols:
		if col in self.df.columns and self.df[col].isnull().sum() > 0:
			mode_val = self.df[col].mode()[0] if len(self.df[col].mode()) > \
															0 else 'None'
			null_count = self.df[col].isnull().sum()
			self.df[col] = self.df[col].fillna(mode_val)

	return self
```

#### Chia 85/15 ngay sau khi dá»¯ liá»‡u sáº¡ch cÆ¡ báº£n

KhÃ³a test, chia sá»›m ngÄƒn leakage cho má»i bÆ°á»›c sau.

**Implementation:**

```python
def run_preprocessing(self):
	from src.Preprocessing import Preprocessor
	from sklearn.model_selection import train_test_split

	# Load raw data
	df_raw = pd.read_csv(self.raw_data_path)

	# Run preprocessing pipeline
	preprocessor = Preprocessor(df_raw)
	df_clean = (preprocessor
				.fix_masonry_veneer_logic()
				.fill_missing_values()
				.fix_garage_logic()
				.get_summary()
				.get_dataframe())

	# Save cleaned data
	preprocessed_path = self.processed_dir / 'train_preprocessed.csv'
	df_clean.to_csv(preprocessed_path, index=False)

	# Split into train/test (85/15)
	target = df_clean['SalePrice']
	X = df_clean.drop('SalePrice', axis=1)

	X_train, X_test, y_train, y_test = train_test_split(
		X, target, test_size=0.15, random_state=42
	)

	train_df = pd.concat([X_train, y_train], axis=1)
	test_df = pd.concat([X_test, y_test], axis=1)

	train_path = self.processed_dir / 'train_data.csv'
	test_path = self.processed_dir / 'test_data.csv'

	train_df.to_csv(train_path, index=False)
	test_df.to_csv(test_path, index=False)

	return True
```

#### Káº¿t quáº£ Preprocessing:

```
âœ… Shape: 1,460 â†’ 1,458 (deleted 2 logical errors)
âœ… Nulls: 6,940 â†’ 0 (100% clean)
âœ… Garage consistency: Fixed 81 rows
âœ… Train/Test: Split early (no leakage)
```

## 3. Feature Engineering (Step 2)

**Ã tÆ°á»Ÿng cá»‘t lÃµi:** vá»›i cáº·p biáº¿n dá»… **trÃ¹ng thÃ´ng tin** (multicollinearity), ta â€œ**partial-out**â€ pháº§n Ä‘Ã£ giáº£i thÃ­ch bá»Ÿi biáº¿n cÆ¡ sá»Ÿ, chá»‰ giá»¯ **pháº§n dÆ° Ä‘á»™c láº­p** (residual). Ta lÃ m cho biáº¿n má»›i **Ã­t phá»¥ thuá»™c** vÃ o biáº¿n gá»‘c, giÃºp mÃ´ hÃ¬nh há»c â€œtÃ­n hiá»‡u tháº­tâ€ thay vÃ¬ láº·p láº¡i cÃ¹ng má»™t thÃ´ng tin.

#### Garage Features

```
â”œâ”€ Táº¡o: GarageAreaPerCar = GarageArea / GarageCars (Quy mÃ´ Garage)
â”œâ”€ Táº¡o: HasGarage (binary flag)
â””â”€ Bá»: GarageCars (multicollinear vá»›i GarageArea)
```

**Implementation:**

```python
def engineer_garage_features(self):
	# Feature 1: GarageAreaPerCar
	self.df['GarageAreaPerCar'] = np.where(
		self.df['GarageCars'] > 0,
		self.df['GarageArea'] / self.df['GarageCars'],
		0
	)
	self.new_features.append('GarageAreaPerCar')

	# Feature 2: HasGarage
	self.df['HasGarage'] = (self.df['GarageArea'] > 0).astype(int)
	self.new_features.append('HasGarage')

	# Drop GarageCars
	if 'GarageCars' in self.df.columns:
		self.df = self.df.drop('GarageCars', axis=1)

	return self
```

**Äá»‹nh nghÄ©a**:

- **GarageAreaPerCar**: Äo â€œÄ‘á»™ rá»™ng rÃ£iâ€ cá»§a garage. ~250â€“300 sqft/car = chuáº©n; 400â€“500+ = rá»™ng/luxury.
- **HasGarage**: 1 náº¿u nhÃ  cÃ³ garage (GarageArea > 0), ngÆ°á»£c láº¡i 0.
  **Thay tháº¿**: Drop `GarageCars` (trÃ¹ng thÃ´ng tin vá»›i `GarageArea`); dÃ¹ng `GarageAreaPerCar` + `HasGarage` Ä‘á»ƒ tÃ¡ch scale vÃ  existence.

#### Area Features

```
â”œâ”€ Táº¡o: AvgRoomSize = GrLivArea / TotRmsAbvGrd
â””â”€ Bá»: TotRmsAbvGrd (multicollinear vá»›i GrLivArea)
```

**Implementation**:

```python
def engineer_area_features(self):
	# Feature 1: AvgRoomSize
	self.df['AvgRoomSize'] = np.where(
		self.df['TotRmsAbvGrd'] > 0,
		self.df['GrLivArea'] / self.df['TotRmsAbvGrd'],
		0
	)
	self.new_features.append('AvgRoomSize')

	# Drop TotRmsAbvGrd
	if 'TotRmsAbvGrd' in self.df.columns:
		self.df = self.df.drop('TotRmsAbvGrd', axis=1)

	return self
```

**Äá»‹nh nghÄ©a:** Pháº£n Ã¡nh cáº£m giÃ¡c rá»™ngâ€“cháº­t cá»§a khÃ´ng gian sá»‘ng (â‰ˆ200 = vá»«a; 300+ = rá»™ng rÃ£i/luxury).

**Thay tháº¿:** Drop `TotRmsAbvGrd` (corr cao vá»›i `GrLivArea`), giá»¯ `GrLivArea` (scale) + `AvgRoomSize` (efficiency).

#### Basement Features

```
â”œâ”€ Táº¡o: HasBasement (binary flag)
â”œâ”€ Táº¡o: BasementResid (orthogonalized vá»›i 1stFlrSF + HasBasement)
â””â”€ Bá»: TotalBsmtSF (multicollinear vá»›i 1stFlrSF)
```

**Implementation:**

```python
def engineer_basement_features(self):
	# Feature 1: HasBasement
	self.df['HasBasement'] = (self.df['TotalBsmtSF'] > 0).astype(int)
	self.new_features.append('HasBasement')

	# Feature 2: BasementResid (orthogonalized)
	X_basement = self.df[['1stFlrSF', 'HasBasement']].values
	y_bsmt = self.df['TotalBsmtSF'].values

	# DÃ¹ng há»“i quy tuyáº¿n tÃ­nh Ä‘á»ƒ loáº¡i bá» pháº§n giáº£i thÃ­ch Ä‘Æ°á»£c cá»§a TotalBsmtSF
	# dá»±a trÃªn 1stFlrSF vÃ  HasBasement
	model = LinearRegression()
	model.fit(X_basement, y_bsmt)

	# BasementResid lÃ  pháº§n dÆ° sau há»“i quy: y_bsmt - model.predict(X_basement),
	# giÃºp orthogonal hÃ³a Ä‘áº·c trÆ°ng diá»‡n tÃ­ch háº§m
	self.df['BasementResid'] = y_bsmt - model.predict(X_basement)
	self.new_features.append('BasementResid')

	# Drop TotalBsmtSF
	if 'TotalBsmtSF' in self.df.columns:
		self.df = self.df.drop('TotalBsmtSF', axis=1)

	return self
```

**Äá»‹nh nghÄ©a:**

- Orthogonalization (trá»±c giao hÃ³a): táº¡o residual Ä‘á»ƒ loáº¡i phá»¥ thuá»™c tuyáº¿n tÃ­nh â†’ tÃ­n hiá»‡u Ä‘á»™c láº­p, giáº£m VIF.
- BasementResid > 0: Basement lá»›n hÆ¡n ká»³ vá»ng; < 0: nhá» hÆ¡n ká»³ vá»ng.
  **Thay tháº¿:** Drop `TotalBsmtSF`; dÃ¹ng `HasBasement` + `BasementResid` (r vá»›i `1stFlrSF` tá»« 0.815 â†’ 0.000).

#### Age Features

```
â”œâ”€ Táº¡o: HouseAge (nÄƒm tá»« khi xÃ¢y dá»±ng)
â”œâ”€ Táº¡o: GarageLag (garage construction lag)
â”œâ”€ Táº¡o: GarageSameAsHouse (binary flag)
â””â”€ Bá»: YearBuilt, GarageYrBlt (redundant)
```

**Implementation:**

```python
def engineer_age_features(self):
	# Feature 1: HouseAge
	self.df['HouseAge'] = self.df['YrSold'] - self.df['YearBuilt']
	self.new_features.append('HouseAge')

	# Feature 2: GarageLag
	self.df['GarageLag'] = self.df['GarageYrBlt'] - self.df['YearBuilt']
	self.new_features.append('GarageLag')

	# Feature 3: GarageSameAsHouse
	self.df['GarageSameAsHouse'] = (self.df['GarageYrBlt'] == \
		self.df['YearBuilt']).astype(int)
	self.new_features.append('GarageSameAsHouse')

	# Drop raw year features
	cols_to_drop = []
	for col in ['YearBuilt', 'GarageYrBlt', 'GarageAge']:
		if col in self.df.columns:
			self.df = self.df.drop(col, axis=1)
			cols_to_drop.append(col)

	return self
```

**Äá»‹nh nghÄ©a**: DÃ¹ng tuá»•i tÆ°Æ¡ng Ä‘á»‘i thay vÃ¬ nÄƒm tuyá»‡t Ä‘á»‘i (1950 vs 2000). `GarageLag` lÃ  chá»‰ bÃ¡o renovation/upgrade.
**Thay tháº¿**: Drop `YearBuilt`, `GarageYrBlt`; dÃ¹ng `HouseAge`, `GarageLag`, `GarageSameAsHouse`.

#### Quality Features

```
â”œâ”€ Fireplace: HasFireplace + ExtraFireplaces
â”œâ”€ Masonry: HasMasonryVeneer + MasVnrAreaResid (orthogonalized)
â”œâ”€ Second Floor: Has2ndFlr + SecondFlrShare_resid (orthogonalized)
â””â”€ Bá»: Raw features (highly correlated)
```

**Implementation:**

```python
def engineer_quality_features(self):
	# ===== FIREPLACE =====
	self.df['HasFireplace'] = (self.df['Fireplaces'] > 0).astype(int)
	self.new_features.append('HasFireplace')
	self.df['ExtraFireplaces'] = np.maximum(self.df['Fireplaces'] - 1, 0)
	self.new_features.append('ExtraFireplaces')

	if 'Fireplaces' in self.df.columns:
		self.df = self.df.drop('Fireplaces', axis=1)

	# ===== MASONRY VENEER =====
	self.df['MasVnrArea'] = self.df['MasVnrArea'].fillna(0)
	self.df['HasMasonryVeneer'] = (self.df['MasVnrArea'] > 0).astype(int)
	self.new_features.append('HasMasonryVeneer')

	# MasVnrAreaResid (orthogonalized)
	X_masonry = self.df[['HasMasonryVeneer', 'OverallQual']].values
	y_masonry = self.df['MasVnrArea'].values

	model_m = LinearRegression()
	model_m.fit(X_masonry, y_masonry)

	self.df['MasVnrAreaResid'] = y_masonry - model_m.predict(X_masonry)
	self.new_features.append('MasVnrAreaResid')

	if 'MasVnrArea' in self.df.columns:
		self.df = self.df.drop('MasVnrArea', axis=1)

	# ===== SECOND FLOOR =====
	self.df['Has2ndFlr'] = (self.df['2ndFlrSF'] > 0).astype(int)
	self.new_features.append('Has2ndFlr')

	# SecondFlrShare_resid (orthogonalized)
	second_flr_share = np.where(
		self.df['GrLivArea'] > 0,
		self.df['2ndFlrSF'] / self.df['GrLivArea'],
		0
	)

	X_share = self.df[['Has2ndFlr']].values

	model_s = LinearRegression()
	model_s.fit(X_share, second_flr_share)

	self.df['SecondFlrShare_resid'] = second_flr_share - model_s.predict(X_share)
	self.new_features.append('SecondFlrShare_resid')

	if '2ndFlrSF' in self.df.columns:
		self.df = self.df.drop('2ndFlrSF', axis=1)

	return self
```

**Äá»‹nh nghÄ©a:**

- `HasFireplace` + `ExtraFireplaces` - model há»c riÃªng â€œcÃ³/khÃ´ngâ€ (comfort) vÃ  â€œbao nhiÃªu thÃªmâ€ (luxury).
- `MasVnrAreaResid` = veneer â€œdÆ°â€ sau khi Ä‘Ã£ tÃ­nh Ä‘áº¿n quality vÃ  viá»‡c cÃ³ veneer; giá»¯ tÃ­n hiá»‡u Ä‘á»™c láº­p.
- `Has2ndFlr` + `SecondFlrShare_resid` - tÃ¡ch â€œcÃ³ táº§ng 2â€ vÃ  â€œtá»· trá»ng táº§ng 2â€ khá»i quy mÃ´ tá»•ng, trÃ¡nh giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n.
  **Thay tháº¿:** Drop `Fireplaces`, `MasVnrArea`, `2ndFlrSF`t

**Káº¿t quáº£:**

> - Efficiency metric: Tá»· lá»‡ hiá»‡u suáº¥t (vÃ­ dá»¥ diá»‡n tÃ­ch/xe, diá»‡n tÃ­ch/phÃ²ng).
> - Orthogonalization/Residual: Pháº§n sai lá»‡ch khÃ´ng giáº£i thÃ­ch bá»Ÿi biáº¿n ná»n táº£ng â†’ giáº£m multicollinearity.
> - Binary flag: Biáº¿n 0/1 biá»ƒu diá»…n sá»± tá»“n táº¡i cá»§a Ä‘áº·c Ä‘iá»ƒm.

![Feature Engineering Illustration](images/Pasted%20image%2020251028142135.png)

## 4. Transformation (Step 3)

#### Target Transformation

**Implementation**:

```python
def _transform_target(self):
	if 'SalePrice' in self.train_data.columns:
		# LÆ°u giÃ¡ trá»‹ ban Ä‘áº§u
		y_train = self.train_data['SalePrice'].values
		y_test = self.test_data['SalePrice'].values

		# DÃ¹ng log1p Ä‘á»ƒ giáº£m skew
		y_train_transformed = np.log1p(y_train)
		y_test_transformed = np.log1p(y_test)

		# Ghi Ä‘Ã¨ dá»¯ liá»‡u
		self.train_data['SalePrice'] = y_train_transformed
		self.test_data['SalePrice'] = y_test_transformed

	return self
```

```
â”œâ”€ SalePrice â†’ log1p(SalePrice)
â”œâ”€ Skewness: 2.009 â†’ 0.205 (giáº£m 89.8%)
â””â”€ Káº¿t quáº£: Nearly symmetric distribution âœ“
```

![Feature Engineering Example 1](images/Pasted%20image%2020251028142603.png)
![Feature Engineering Example 2](images/Pasted%20image%2020251028142715.png)

#### Feature Transformations

**Implementation:**

```python
def _bin_kitchen_abvgr(self):
	# HÃ m phÃ¢n nhÃ³m KitchenAbvGr thÃ nh 3 bin:
	# - 0 vÃ  1: bin 0
	# - 2: bin 1
	# - >=3: bin 2
	def bin_kitchen(x):
		if x <= 1:
			return 0
		elif x == 2:
			return 1
		else:
			return 2

	# Táº¡o cá»™t binned má»›i
	self.train_data['KitchenAbvGr_Binned'] = \
		self.train_data['KitchenAbvGr'].apply(bin_kitchen)
	self.test_data['KitchenAbvGr_Binned'] = \
		self.test_data['KitchenAbvGr'].apply(bin_kitchen)

	# Táº¡o cá» "HasMultiKitchen": 1 náº¿u cÃ³ â‰¥2 kitchen
	self.train_data['HasMultiKitchen'] = \
		(self.train_data['KitchenAbvGr'] >= 2).astype(int)
	self.test_data['HasMultiKitchen'] = \
		(self.test_data['KitchenAbvGr'] >= 2).astype(int)

	# XÃ³a cá»™t cÅ© KitchenAbvGr
	self.train_data = self.train_data.drop('KitchenAbvGr', axis=1)
	self.test_data = self.test_data.drop('KitchenAbvGr', axis=1)

	return self

def _transform_features_log(self):
	# Chá»n ra nhá»¯ng Ä‘áº·c trÆ°ng Ã¡p dá»¥ng log1p
	log_features = [f for f, s in self.feature_strategy.items() if s == 'log']

	for feat in log_features:
		if feat in self.train_data.columns:
			# Skew trÆ°á»›c biáº¿n Ä‘á»•i
			original_skew = skew(self.train_data[feat].dropna())

			# Biáº¿n Ä‘á»•i báº±ng log1p, ghi sang cá»™t má»›i
			self.train_data[f'{feat}_log'] = np.log1p(self.train_data[feat])
			self.test_data[f'{feat}_log'] = np.log1p(self.test_data[feat])

			# Skew sau biáº¿n Ä‘á»•i
			transformed_skew = skew(self.train_data[f'{feat}_log'].dropna())

			# Loáº¡i bá» Ä‘áº·c trÆ°ng gá»‘c (chá»‰ giá»¯ Ä‘áº·c trÆ°ng má»›i)
			self.train_data = self.train_data.drop(feat, axis=1)
			self.test_data = self.test_data.drop(feat, axis=1)

	return self

def _transform_features_yeo_johnson(self):
	from sklearn.preprocessing import PowerTransformer

	# Chá»n ra Ä‘áº·c trÆ°ng cáº§n biáº¿n Ä‘á»•i Yeo-Johnson hoáº·c log1p cho zero-inflated
	yj_features = [f for f, s in self.feature_strategy.items()
	if s in ['yeo_johnson', 'log_zero_inflated']]
	# Chá»‰ láº¥y nhá»¯ng cá»™t hiá»‡n diá»‡n thá»±c táº¿ trong data
	numeric_df_train = self.train_data[[f for f in \
		yj_features if f in self.train_data.columns]].copy()
	numeric_df_test = self.test_data[[f for f in \
		yj_features if f in self.test_data.columns]].copy()

	# Táº¡o transformer (khÃ´ng chuáº©n hÃ³a mean/std)
	pt = PowerTransformer(method='yeo-johnson', standardize=False)

	# Fit transformer trÃªn train data
	pt.fit(numeric_df_train)

	# Ãp dá»¥ng biáº¿n Ä‘á»•i trÃªn cáº£ hai táº­p train/test
	train_transformed = pt.transform(numeric_df_train)
	test_transformed = pt.transform(numeric_df_test)

	# ThÃªm cá»™t má»›i vÃ  lÆ°u láº¡i thÃ´ng tin biáº¿n Ä‘á»•i
	for idx, feat in enumerate([f for f in \
			yj_features if f in self.train_data.columns]):
		original_skew = skew(numeric_df_train[feat].dropna())
		transformed_skew = skew(train_transformed[:, idx])

		self.train_data[f'{feat}_yj'] = train_transformed[:, idx]
		self.test_data[f'{feat}_yj'] = test_transformed[:, idx]

		# XÃ³a Ä‘áº·c trÆ°ng gá»‘c
		if feat in self.train_data.columns:
			self.train_data = self.train_data.drop(feat, axis=1)
		if feat in self.test_data.columns:
			self.test_data = self.test_data.drop(feat, axis=1)

	return self
```

```
â”œâ”€ Log1p: 15 features (táº¥t cáº£ positive values)
â”œâ”€ Yeo-Johnson: 9 features (zero-inflated/negative)
â”œâ”€ Binning: KitchenAbvGr â†’ KitchenAbvGr_Binned
â””â”€ No transform: Binary flags + residuals (orthogonal)
```

**Káº¿t quáº£:**
![Feature Transform After Log1p](images/Pasted%20image%2020251028144755.png)

**Log1p**: PhÃ¢nÂ phá»‘iÂ lá»‡chÂ pháº£iÂ (GrLivArea, LotArea,Â 1stFlrSF, AvgRoomSize) trá»ŸÂ nÃªnÂ Ä‘á»‘i xá»©ngÂ hÆ¡n rÃµÂ rá»‡t; Ä‘uÃ´iÂ pháº£i ngáº¯nÂ láº¡i,

![Feature Transform After Log for Other Features](images/Pasted%20image%2020251028145402.png)

**Yeo-Johnson:** CÃ¡c biáº¿n zero/Ä‘áº¿m/Ã¢mÂ (GarageArea, FullBath, HouseAge, GarageLag) trÆ¡nÂ tru hÆ¡n, giáº£m spikeÂ táº¡iÂ 0; phÃ¢n phá»‘i gáº§n GaussianÂ hÆ¡n.

![Feature Transform After Yeo-Johnson](images/Pasted%20image%2020251028145413.png)

**Binning**:Â KitchenAbvGrÂ â†’Â KitchenAbvGr_BinnedÂ gomÂ 0â€“1 thÃ nhÂ binÂ 0, 2 thÃ nhÂ binÂ 1,Â 3+ thÃ nh binÂ 2; meanÂ SalePriceÂ theo binÂ xÃ¡c nháº­nÂ insight: multi-kitchen thÆ°á»ng ráº»Â hÆ¡n trong Ames (duplex/thuÃª).

**LÃ­ do chá»n KitchenAbvGr Ä‘á»ƒ Binning: Extreme Imbalance (95% + 5%Â + 0.1%)**

- One-hot encodingÂ â†’Â 3-4 sparseÂ columns vá»›iÂ 95% zerosÂ (lÃ£ngÂ phÃ­)
- BinningÂ â†’Â 3 meaningfulÂ groupsÂ (baseline, duplex, rare)

| **Bin** | **Original** | **Count**   | **Mean Price** | **Meaning**         |
| ------- | ------------ | ----------- | -------------- | ------------------- |
| 0       | 0-1 kitchen  | 1,178 (95%) | $169,211       | Standard home       |
| 1       | 2 kitchens   | 60 (5%)     | $125,530       | Duplex/Multi-family |
| 2       | 3+ kitchens  | 1 (0.08%)   | $113,000       | Rare/luxury         |

ğŸ’¡Â **Counterintuitive:**Â More kitchens = LOWER price trong dataset nÃ y! â†’ VÃ¬ duplex thÆ°á»ng á»Ÿ neighborhoods giÃ¡ tháº¥p hÆ¡n single-family homes á»Ÿ Ames, Iowa.

**Káº¿t quáº£:**

- ÄÃ£ xá»­ lÃ½ skewness cho 24 biáº¿n Ä‘áº§u vÃ o
- Biáº¿n má»¥c tiÃªu phÃ¢n phá»‘i gáº§n Ä‘á»‘i xá»©ng (skewness xáº¥p xá»‰ 0.2)
- Chiáº¿n lÆ°á»£c cross-fit Ä‘áº£m báº£o khÃ´ng rÃ² rá»‰ dá»¯ liá»‡u huáº¥n luyá»‡n
- QuÃ¡ trÃ¬nh biáº¿n Ä‘á»•i luÃ´n cÃ³ thá»ƒ thá»±c hiá»‡n láº¡i Ä‘Æ°á»£c dá»… dÃ ng

## 5. Encoding Strategies (Step 4)

### 5.1 ThÃ¡ch thá»©c: 43 Features Categorical

**PhÃ¢n tÃ­ch theo Cardinality:**

- Cardinality tháº¥p (â‰¤5): 27 features
- Cardinality trung bÃ¬nh (6-10): 13 features
- Cardinality cao (>10): 3 features â†’ `Neighborhood`, `Exterior1st`, `Exterior2nd`

**Váº¥n Ä‘á»:** One-hot encoding táº¥t cáº£ 43 features sáº½ táº¡o ra quÃ¡ nhiá»u cá»™t sparse!

### 5.2 Ordinal Encoding (17 features)

```
â”œâ”€ Quality scales: ExterQual, KitchenQual, BsmtQual, etc.
â”œâ”€ Mapping: Ex (Xá»‹n) > Gd (Good) > TA (Trung bÃ¬nh) > Fa (Táº¡m) > Po (Poor)
â”œâ”€ Finish levels: GarageFinish (HoÃ n thÃ nh), BsmtFinType1/2 (Má»›i xong 1/2)
â””â”€ Shape/Slope: LotShape, LandSlope, PavedDrive (liÃªn quan tá»›i lÃ´ Ä‘áº¥t)
```

**Implementation**:

```python
from sklearn.preprocessing import OrdinalEncoder

# ----- OrdinalEncoder -----
# Ta cáº§n Ä‘Æ°a thá»© tá»± category cho tá»«ng cá»™t ordinal
# sklearn.OrdinalEncoder muá»‘n categories=[list_for_col1, list_for_col2, ...]
ordinal_categories = []
for col in ordinal_cols:
	mapping = self.ordinal_mappings[col] # ex: {'None': -1, 'Po':0, ...}
	# sáº¯p xáº¿p key theo giÃ¡ trá»‹ rank tÄƒng dáº§n
	ordered_levels = sorted(mapping.keys(), key=lambda k: mapping[k])

ordinal_categories.append(ordered_levels)
ordinal_encoder = OrdinalEncoder(
	categories=ordinal_categories,
	handle_unknown='use_encoded_value',
	unknown_value=-1,
	encoded_missing_value=-1,
)
```

![Ordinal Encoding Example](images/Pasted%20image%2020251028162143.png)
**BÆ°á»›c Nháº£y GiÃ¡: TA â†’ Ex = +154%**

- TA: $145.246 â†’ Ex: $368.929
- **ChÃªnh lá»‡ch: $223.683 (+154%)**
- **Giáº£i thÃ­ch:** NhÃ  cÃ³ cháº¥t lÆ°á»£ng ngoÃ i tháº¥t xuáº¥t sáº¯c bÃ¡n Ä‘Æ°á»£c vá»›i giÃ¡ **gáº¥p 2,5 láº§n** so vá»›i nhÃ  cháº¥t lÆ°á»£ng bÃ¬nh thÆ°á»ng!

### 5.3 Target Encoding (2 features)

**Problem:** Cardinality Cao

- **Neighborhood:** 25 loáº¡i duy nháº¥t
- **Exterior2nd:** 16 loáº¡i duy nháº¥t
- **Tá»•ng:** 41 loáº¡i â†’ One-hot sáº½ táº¡o 39 cá»™t sparse!
  **Solution:** Target Encoding vá»›i Cross-fit K-fold (K=5)

```python
from sklearn.preprocessing import TargetEncoder
from sklearn.impute import SimpleImputer

# ----- TargetEncoder -----
# Vá»›i TargetEncoder, ta cÅ©ng impute most_frequent trÆ°á»›c
# TargetEncoder trong sklearn cÃ³ tham sá»‘ cv=5 máº·c Ä‘á»‹nh Ä‘á»ƒ cross-fit
# vÃ  fit_transform() sáº½ tá»± dÃ¹ng cross-fitting Ä‘á»ƒ trÃ¡nh leakage.
# Khi Pipeline.fit_transform cháº¡y cho train, nÃ³ sáº½ gá»i Ä‘Ãºng logic nÃ y.
tgt_pipe = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('tgt', TargetEncoder(
		cv=5, # cross-fit K-fold =5
		smooth='auto', # smoothing shrink vá» global mean
		random_state=0,
	)),
])
```

**CÃ¡ch hoáº¡t Ä‘á»™ng tá»± Ä‘á»™ng:**

1. sklearn chia train thÃ nh 5 folds
2. Vá»›i má»—i fold, tÃ­nh mean tá»« 4 folds KHÃC
3. Ãp dá»¥ng vÃ o fold hiá»‡n táº¡i
   â†’ Má»—i fold KHÃ”NG BAO GIá»œ nhÃ¬n tháº¥y target cá»§a chÃ­nh nÃ³!
   â†’ **Zero leakage guaranteed!** âœ…

**Káº¿t quáº£:**

- **TÆ°Æ¡ng quan Neighborhood_target_enc:** r = +0.7397 (feature máº¡nh nháº¥t #2!)
- **TÆ°Æ¡ng quan Exterior2nd_target_enc:** r = +0.3965

### 5.4 One-hot Encoding (24 features -> 110 cá»™t)

**Implementation:**

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

# ----- OneHotEncoder -----
# Nhiá»u cá»™t nominal cÃ³ NA => Imputer(most_frequent) trÆ°á»›c OHE
ohe_pipe = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('ohe', OneHotEncoder(
		handle_unknown='ignore',
		drop='first', # trÃ¡nh multicollinearity quÃ¡ máº¡nh
		sparse_output=False, # output dense Ä‘á»ƒ scaler xá»­ lÃ½ Ä‘Æ°á»£c
	)),
])
```

**Xá»­ lÃ½ Sparse matrices:**

- Háº§u háº¿t cá»™t one-hot lÃ  sparse (nhiá»u zeros)
- Sklearn xá»­ lÃ½ hiá»‡u quáº£ vá»›i sparse matrices
- LÆ°u trá»¯ vÃ  tÃ­nh toÃ¡n tiáº¿t kiá»‡m bá»™ nhá»›

![One-hot Encoding Example](images/Pasted%20image%2020251028172509.png)

### 5.5 Feature Scaling

**Implementation:**

```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline.Pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# ----- ColumnTransformer -----
pre = ColumnTransformer(
	transformers=[
		('ord', ordinal_encoder, ordinal_cols),
		('ohe', ohe_pipe, ohe_cols),
		('tgt', tgt_pipe, target_cols),
		('num', num_pipe, numeric_cols),
	],
	remainder='drop',
)

# ----- Full pipeline -----

full = Pipeline(steps=[
	('pre', pre),
	('scaler', StandardScaler()),
])
```

**á»¨ng Dá»¥ng StandardScaler**

```
num__LowQualFinSF | mean = 0.000, std = 1.000
num__3SsnPorch | mean = 0.000, std = 1.000
num__PoolArea | mean = 0.000, std = 1.000
num__OverallQual_log | mean = -0.000, std = 1.000
```

**Táº¥t cáº£ 172 features Ä‘Æ°á»£c scale thÃ nh:** mean â‰ˆ 0, std â‰ˆ 1

**Target khÃ´ng Ä‘Æ°á»£c scale:**

- **SalePrice:** mean = 12.024, std = 0.397 âœ… **KHÃ”NG Ä‘Æ°á»£c scale!**
- **LÃ½ do:**
  1.  âœ… `log1p` Ä‘Ã£ xá»­ lÃ½ skewness
  2.  âœ… Giá»¯ thang log â†’ dá»… Ä‘áº£o ngÆ°á»£c (`expm1`)
  3.  âœ… Scale target lÃ  tÃ¹y chá»n cho linear models
  4.  âœ… Kháº£ nÄƒng giáº£i thÃ­ch: log(price) dá»… hÆ¡n cÃ¡c Ä‘Æ¡n vá»‹ chuáº©n hÃ³a

![Scaled Feature Example](images/Pasted%20image%2020251028173458.png)

## 6. Outlier Analysis

**PhÃ¢n tÃ­ch:** PhÃ¡t hiá»‡n outliers toÃ n diá»‡n sau transformation
**Quyáº¿t Ä‘á»‹nh:** âœ… GIá»® Táº¤T Cáº¢ OUTLIERS

#### Outliers trong Target Variable (SalePrice)

![Outlier Target Variable](images/Pasted%20image%2020251028175257.png)

```
â”œâ”€ Skewness: 0.205 (Äá»™ lá»‡ch nhá» - phÃ¢n phá»‘i gáº§n Ä‘á»‘i xá»©ng âœ”ï¸)
â”œâ”€ Sá»‘ máº«u náº±m ngoÃ i khoáº£ng IQR (Interquartile Range): 56 (chiáº¿m khoáº£ng 4.5%)
â”œâ”€ Sá»‘ máº«u cÃ³ z-score > 3: 21 (chiáº¿m 1.7%)
â””â”€ Nháº­n Ä‘á»‹nh: CÃ¡c outlier nÃ y pháº£n Ã¡nh sá»± Ä‘a dáº¡ng tá»± nhiÃªn cá»§a giÃ¡ nhÃ , khÃ´ng pháº£i lÃ  lá»—i hoáº·c báº¥t thÆ°á»ng cáº§n loáº¡i bá». Do váº­y, giá»¯ nguyÃªn táº¥t cáº£.
```

#### Outliers trong cÃ¡c Feature

![Outlier Features](images/Pasted%20image%2020251028180305.png)

```
â”œâ”€ CÃ¡c biáº¿n cá» nhá»‹ phÃ¢n (binary flags): 0% outliers (7 feature) â€“ khÃ´ng cÃ³ giÃ¡ trá»‹ báº¥t thÆ°á»ng
â”œâ”€ 24 feature khÃ´ng cÃ³ outlier
â”œâ”€ 12 feature cÃ³ ráº¥t Ã­t outlier (0-5%) âœ”ï¸ â€“ cháº¥p nháº­n Ä‘Æ°á»£c
â”œâ”€ 5 feature cÃ³ lÆ°á»£ng outlier vá»«a pháº£i (5-10%) âš  â€“ cáº§n lÆ°u Ã½ nhÆ°ng há»£p lÃ½
â””â”€ 2 feature cÃ³ tá»· lá»‡ outlier cao (>10%) âœ”ï¸ â€“ cÃ³ lÃ½ do giáº£i thÃ­ch rÃµ rÃ ng
```

#### Giáº£i thÃ­ch cá»¥ thá»ƒ vá»›i cÃ¡c feature cÃ³ nhiá»u outlier:

- **MasVnrAreaResid** (17.6%): Pháº§n dÆ° (sai sá»‘) giá»¯a diá»‡n tÃ­ch á»‘p tÆ°á»ng Ä‘Ã¡ thá»±c táº¿ vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n theo cÃ¡c tiÃªu chÃ­ cÃ²n láº¡i â€“ giÃ¡ trá»‹ lá»›n báº¥t thÆ°á»ng thá»ƒ hiá»‡n cÃ¡c cÄƒn nhÃ  cÃ³ pháº§n á»‘p tÆ°á»ng khÃ¡c biá»‡t háº³n so vá»›i xu hÆ°á»›ng chung.
- **BasementResid** (17.1%): Pháº§n dÆ° (sai sá»‘) liÃªn quan Ä‘áº¿n diá»‡n tÃ­ch háº§m (basement) â€“ outlier nghÄ©a lÃ  nhÃ  Ä‘Ã³ cÃ³ háº§m lá»›n/nhá» báº¥t thÆ°á»ng so vá»›i cÃ¡c Ä‘áº·c Ä‘iá»ƒm khÃ¡c.
- **GarageAreaPerCar** (9.3%): Diá»‡n tÃ­ch gara chia cho sá»‘ chá»— Ä‘á»ƒ xe â€“ outlier xuáº¥t hiá»‡n khi 1 chá»— nhÆ°ng gara láº¡i ráº¥t rá»™ng (ráº¥t â€œthá»«aâ€, thiáº¿t káº¿ láº¡) hoáº·c ngÆ°á»£c láº¡i.
- **OverallCond** (8.4%): Äiá»ƒm Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ vá» Ä‘iá»u kiá»‡n cÄƒn nhÃ  (thang báº­c 1-9) â€“ cÃ¡c Ä‘iá»ƒm cá»±c ká»³ cao hoáº·c tháº¥p thÆ°á»ng lÃ  outlier, pháº£n Ã¡nh báº¥t thÆ°á»ng vá» cháº¥t lÆ°á»£ng.
- **LotFrontage** (8.0%): Chiá»u rá»™ng máº·t tiá»n Ä‘áº¥t â€“ nhá»¯ng lÃ´ cÃ³ máº·t tiá»n ráº¥t rá»™ng (nhÃ  gÃ³c, biá»‡t thá»±) hoáº·c cá»±c háº¹p sáº½ bá»‹ xem lÃ  outlier.
- **MSSubClass** (7.1%): PhÃ¢n loáº¡i kiá»ƒu nhÃ  theo mÃ£ sá»‘ xÃ¢y dá»±ng â€“ má»™t sá»‘ mÃ£ Ã­t xuáº¥t hiá»‡n cÃ³ thá»ƒ táº¡o thÃ nh outlier do hiáº¿m tháº¥y trÃªn thá»‹ trÆ°á»ng.

#### Táº¡i sao giá»¯ láº¡i toÃ n bá»™ outlier?

- CÃ¡c thuáº­t toÃ¡n regularization nhÆ° Ridge/Lasso Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£m áº£nh hÆ°á»Ÿng tiÃªu cá»±c cá»§a outlier lÃªn model. L2 penalty (Ridge) sáº½ thu nhá» tÃ¡c Ä‘á»™ng cÃ¡c Ä‘iá»ƒm báº¥t thÆ°á»ng má»™t cÃ¡ch má»m dáº»o, L1 penalty (Lasso) tháº­m chÃ­ cÃ³ thá»ƒ loáº¡i bá» hoÃ n toÃ n feature nhiá»…u náº¿u cáº§n.
- Náº¿u loáº¡i bá» cÃ¡c outlier nÃ y, ta sáº½ máº¥t má»™t pháº§n thÃ´ng tin thá»±c táº¿ liÃªn quan tá»›i sá»± Ä‘a dáº¡ng hoáº·c trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a thá»‹ trÆ°á»ng nhÃ  Ä‘áº¥t.
- Sá»‘ lÆ°á»£ng sample lÃ  1239 â€“ náº¿u giá»¯ láº¡i táº¥t cáº£ sáº½ táº­n dá»¥ng tá»‘i Ä‘a dá»¯ liá»‡u.
- QuÃ¡ trÃ¬nh Cross-validation sáº½ tá»± Ä‘á»™ng chá»n tham sá»‘ Î± (há»‡ sá»‘ Ä‘iá»u chá»‰nh má»©c Ä‘á»™ regularization) sao cho phÃ¹ há»£p nháº¥t vá»›i cáº¥u trÃºc dá»¯ liá»‡u thá»±c, ká»ƒ cáº£ khi tá»“n táº¡i outlier.

# VII. Model Selection: TÃ¬m Kiáº¿m Model Tá»‘t Nháº¥t

## 1. Táº¡i Sao Cáº§n Model Selection?

**Váº¥n Ä‘á»**: KhÃ´ng cÃ³ model nÃ o lÃ  hoÃ n háº£o cho má»i bÃ i toÃ¡n. Má»—i model cÃ³ Ä‘iá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u riÃªng:

- **Linear Models (Ridge, Lasso)**: ÄÆ¡n giáº£n, dá»… giáº£i thÃ­ch nhÆ°ng cÃ³ thá»ƒ khÃ´ng báº¯t Ä‘Æ°á»£c pattern phá»©c táº¡p
- **Tree-based (LightGBM, XGBoost)**: Máº¡nh máº½, chÃ­nh xÃ¡c cao nhÆ°ng khÃ³ giáº£i thÃ­ch
- **Regularized Models**: CÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng giáº£i thÃ­ch

**Giáº£i phÃ¡p**: So sÃ¡nh **6 models khÃ¡c nhau** Ä‘á»ƒ tÃ¬m ra model phÃ¹ há»£p nháº¥t.

**Chiáº¿n lÆ°á»£c trong project nÃ y:**

1. âœ… Train 6 models: Ridge, Lasso, ElasticNet, Huber, LightGBM, XGBoost
2. âœ… Tune hyperparameters cho tá»«ng model
3. âœ… ÄÃ¡nh giÃ¡ báº±ng 5-fold Cross-Validation
4. âœ… So sÃ¡nh performance vÃ  chá»n model tá»‘t nháº¥t

## 2. Metrics: ÄÃ¡nh GiÃ¡ Model Tá»‘t Hay KhÃ´ng?

ChÃºng ta dÃ¹ng 3 metrics chÃ­nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:

### RMSE (Root Mean Squared Error)

```
RMSE = âˆš[Î£(y_thá»±c - y_dá»±_Ä‘oÃ¡n)Â² / n]
```

- **Ã nghÄ©a**: Sai sá»‘ trung bÃ¬nh (cÃ ng tháº¥p cÃ ng tá»‘t)
- **Æ¯u Ä‘iá»ƒm**: Pháº¡t náº·ng cÃ¡c lá»—i lá»›n (outliers cÃ³ áº£nh hÆ°á»Ÿng nhiá»u)
- **VÃ­ dá»¥**: RMSE = 0.125 nghÄ©a lÃ  sai sá»‘ trung bÃ¬nh khoáº£ng 0.125 (trong scale log)

### MAE (Mean Absolute Error)

```
MAE = Î£|y_thá»±c - y_dá»±_Ä‘oÃ¡n| / n
```

- **Ã nghÄ©a**: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh (cÃ ng tháº¥p cÃ ng tá»‘t)
- **Æ¯u Ä‘iá»ƒm**: KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng quÃ¡ nhiá»u bá»Ÿi outliers
- **VÃ­ dá»¥**: MAE = 0.084 nghÄ©a lÃ  sai sá»‘ trung bÃ¬nh 0.084

### RÂ² Score (R-squared)

```
RÂ² = 1 - (SS_res / SS_tot)
```

- **Ã nghÄ©a**: Tá»· lá»‡ variance Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi model (cÃ ng cao cÃ ng tá»‘t, tá»‘i Ä‘a = 1.0)
- **VÃ­ dá»¥**: RÂ² = 0.906 nghÄ©a lÃ  model giáº£i thÃ­ch Ä‘Æ°á»£c 90.6% sá»± biáº¿n thiÃªn cá»§a giÃ¡ nhÃ 
- **Æ¯u Ä‘iá»ƒm**: Dá»… hiá»ƒu, cÃ³ thá»ƒ so sÃ¡nh giá»¯a cÃ¡c models

### Cross-Validation Score

**Váº¥n Ä‘á»**: Náº¿u chá»‰ train/test má»™t láº§n, káº¿t quáº£ cÃ³ thá»ƒ "may máº¯n" hoáº·c "khÃ´ng may"

**Giáº£i phÃ¡p**: **5-Fold Cross-Validation**

```
Dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 5 pháº§n:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚  5  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Láº§n 1: Train trÃªn 2,3,4,5 â†’ Test trÃªn 1
Láº§n 2: Train trÃªn 1,3,4,5 â†’ Test trÃªn 2
...
Láº§n 5: Train trÃªn 1,2,3,4 â†’ Test trÃªn 5

â†’ TÃ­nh trung bÃ¬nh 5 káº¿t quáº£
```

**Táº¡i sao dÃ¹ng CV?**

- âœ… Kiá»ƒm tra model cÃ³ **overfitting** khÃ´ng
- âœ… ÄÃ¡nh giÃ¡ **stability** cá»§a model
- âœ… TÃ¬m hyperparameters tá»‘t nháº¥t má»™t cÃ¡ch **khÃ¡ch quan**

## 3. Hyperparameter Tuning Strategy

### Chiáº¿n LÆ°á»£c 1: Grid Search (Cho Linear Models)

**Ã tÆ°á»Ÿng**: Thá»­ **Táº¤T Cáº¢** cÃ¡c combinations cá»§a hyperparameters

**VÃ­ dá»¥ vá»›i Ridge:**

```python
ridge_params = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100]
}
# 7 giÃ¡ trá»‹ alpha Ã— 5 CV folds = 35 láº§n train

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge

search = GridSearchCV(
    Ridge(random_state=42),
    ridge_params,
    cv=5,                           # 5-fold cross-validation
    scoring='neg_mean_squared_error',
    n_jobs=-1
)
search.fit(X_train, y_train)

print(f"Best alpha: {search.best_params_['alpha']}")
# Output: Best alpha: 100
```

**Táº¡i sao dÃ¹ng Grid Search cho Linear Models?**

- âœ… KhÃ´ng gian tham sá»‘ nhá» (1-2 tham sá»‘)
- âœ… CÃ³ thá»ƒ thá»­ háº¿t â†’ TÃ¬m Ä‘Æ°á»£c **global optimum**
- âœ… KhÃ´ng tá»‘n quÃ¡ nhiá»u thá»i gian

### Chiáº¿n LÆ°á»£c 2: Randomized Search (Cho Tree Models)

**Váº¥n Ä‘á»**: Tree models cÃ³ **khÃ´ng gian tham sá»‘ ráº¥t lá»›n**

**VÃ­ dá»¥ vá»›i LightGBM:**

```python
lgb_params = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],      # 4 giÃ¡ trá»‹
    'num_leaves': [31, 50, 100, 200],             # 4 giÃ¡ trá»‹
    'max_depth': [3, 5, 7, 10],                   # 4 giÃ¡ trá»‹
    'min_child_samples': [20, 50, 100],           # 3 giÃ¡ trá»‹
    'subsample': [0.8, 0.9, 1.0],                 # 3 giÃ¡ trá»‹
    'colsample_bytree': [0.8, 0.9, 1.0]          # 3 giÃ¡ trá»‹
}
# Tá»•ng: 4Ã—4Ã—4Ã—3Ã—3Ã—3 = 4,320 combinations!
# Grid Search: 4,320 Ã— 5 CV = 21,600 láº§n train â†’ QUÃ NHIá»€U!
```

**Giáº£i phÃ¡p**: Randomized Search - Chá»‰ thá»­ **30 combinations ngáº«u nhiÃªn**

```python
from sklearn.model_selection import RandomizedSearchCV
import lightgbm as lgb

search = RandomizedSearchCV(
    lgb.LGBMRegressor(random_state=42, verbose=-1),
    lgb_params,
    n_iter=30,                     # Chá»‰ thá»­ 30 combinations
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42
)
search.fit(X_train, y_train)

print(f"Best params: {search.best_params_}")
# Output: {
#     'subsample': 0.9,
#     'num_leaves': 200,
#     'min_child_samples': 20,
#     'max_depth': 3,
#     'learning_rate': 0.1,
#     'colsample_bytree': 0.8
# }
```

**Táº¡i sao dÃ¹ng Randomized Search cho Tree Models?**

- âœ… KhÃ´ng gian tham sá»‘ **ráº¥t lá»›n** (6 tham sá»‘)
- âœ… Grid Search sáº½ tá»‘n quÃ¡ nhiá»u thá»i gian
- âœ… Randomized Search thÆ°á»ng tÃ¬m Ä‘Æ°á»£c vÃ¹ng tá»‘t vá»›i Ã­t iterations hÆ¡n

### So SÃ¡nh Grid Search vs Randomized Search

| **TiÃªu chÃ­**           | **Grid Search**        | **Randomized Search** |
| ---------------------- | ---------------------- | --------------------- |
| **KhÃ´ng gian tham sá»‘** | Nhá» (1-2 tham sá»‘)      | Lá»›n (5+ tham sá»‘)      |
| **Sá»‘ láº§n thá»­**         | Táº¥t cáº£ combinations    | Chá»‰ má»™t pháº§n (n_iter) |
| **Káº¿t quáº£**            | Global optimum         | Good enough           |
| **Thá»i gian**          | Cháº­m vá»›i nhiá»u tham sá»‘ | Nhanh hÆ¡n             |
| **Khi nÃ o dÃ¹ng?**      | Linear models          | Tree-based models     |

## 4. Káº¿t Quáº£ So SÃ¡nh 6 Models

Sau khi train táº¥t cáº£ 6 models vá»›i hyperparameter tuning, Ä‘Ã¢y lÃ  káº¿t quáº£:

| **Rank** | **Model**      | **RMSE**   | **MAE**    | **RÂ²**     | **CV Score** |
| -------- | -------------- | ---------- | ---------- | ---------- | ------------ |
| ğŸ¥‡ **1** | **LightGBM**   | **0.1249** | **0.0839** | **0.9058** | **0.01768**  |
| ğŸ¥ˆ **2** | **Lasso**      | 0.1258     | 0.0859     | 0.9045     | 0.02043      |
| ğŸ¥‰ **3** | **ElasticNet** | 0.1276     | 0.0879     | 0.9017     | 0.02020      |
| 4ï¸âƒ£       | **XGBoost**    | 0.1288     | 0.0854     | 0.8998     | 0.01825      |
| 5ï¸âƒ£       | **Ridge**      | 0.1329     | 0.0883     | 0.8933     | 0.02222      |
| 6ï¸âƒ£       | **Huber**      | 0.1901     | 0.0897     | 0.7820     | 0.04617      |

### ğŸ“ˆ Visualization

**Dashboard So SÃ¡nh Chi Tiáº¿t:**

![Model Comparison Dashboard](models/model_comparison.png)

Biá»ƒu Ä‘á»“ nÃ y hiá»ƒn thá»‹ 6 gÃ³c nhÃ¬n khÃ¡c nhau:

1. **RMSE Comparison**: LightGBM cÃ³ RMSE tháº¥p nháº¥t
2. **RÂ² Comparison**: LightGBM cÃ³ RÂ² cao nháº¥t (90.58%)
3. **MAE Comparison**: LightGBM cÃ³ MAE tháº¥p nháº¥t
4. **CV Score Comparison**: LightGBM á»•n Ä‘á»‹nh nháº¥t (CV Score = 0.01768)
5. **All Metrics Comparison**: So sÃ¡nh táº¥t cáº£ metrics Ä‘Ã£ normalized
6. **Ranking Heatmap**: LightGBM Ä‘á»©ng Ä‘áº§u táº¥t cáº£ metrics

**TÃ³m Táº¯t Nhanh:**

![Model Summary](models/model_summary.png)

- **BÃªn trÃ¡i**: RMSE vÃ  RÂ² Ä‘Æ°á»£c hiá»ƒn thá»‹ cáº¡nh nhau cho táº¥t cáº£ models
- **BÃªn pháº£i**: LightGBM Ä‘Æ°á»£c highlight mÃ u xanh lÃ¡ - lÃ  best model

**Residuals Plot & Actual vs Predicted:**

![Model Residuals and Predictions](models/model_residuals.png)

**Giáº£i thÃ­ch:**

- **BÃªn trÃ¡i (Residuals Plot)**:

  - âœ… Points phÃ¢n bá»‘ **ngáº«u nhiÃªn** quanh 0 â†’ Model khÃ´ng bias
  - âœ… KhÃ´ng cÃ³ pattern rÃµ rÃ ng â†’ Model khÃ´ng thiáº¿u features

- **BÃªn pháº£i (Actual vs Predicted)**:
  - âœ… Points náº±m **gáº§n Ä‘Æ°á»ng Ä‘á»** (perfect prediction) â†’ Model dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c
  - âœ… Points phÃ¢n bá»‘ **Ä‘á»u 2 bÃªn** â†’ Model khÃ´ng bias

**Feature Importance Plot:**

![Feature Importance](models/model_feature_importance.png)

**Top 5 Features quan trá»ng nháº¥t cá»§a LightGBM:**

1. **OverallQual** - Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ (quan trá»ng nháº¥t!)
2. **Neighborhood_target_enc** - Khu vá»±c (target encoding thÃ nh cÃ´ng)
3. **GrLivArea_log** - Diá»‡n tÃ­ch sá»‘ng (sau log transform)
4. **GarageArea_yj** - Diá»‡n tÃ­ch garage
5. **1stFlrSF_log** - Diá»‡n tÃ­ch táº§ng 1

**Insights:**

- âœ… **Cháº¥t lÆ°á»£ng vÃ  Vá»‹ trÃ­** lÃ  2 yáº¿u tá»‘ quan trá»ng nháº¥t
- âœ… Feature engineering thÃ nh cÃ´ng: Neighborhood Ä‘Æ°á»£c target encoding â†’ ráº¥t quan trá»ng
- âœ… CÃ¡c features má»›i (residuals, transformed features) cÅ©ng cÃ³ vai trÃ²

### ğŸ” Nháº­n XÃ©t Tá»•ng Quan

1. **ğŸ¥‡ LightGBM tháº¯ng Ã¡p Ä‘áº£o**

   - RMSE tháº¥p nháº¥t: 0.1249
   - RÂ² cao nháº¥t: 0.9058 (giáº£i thÃ­ch 90.58% variance)
   - CV Score tháº¥p nháº¥t: 0.01768 (á»•n Ä‘á»‹nh nháº¥t)

2. **ğŸ¥ˆ Lasso Ä‘á»©ng thá»© 2, gáº§n nhÆ° ngang LightGBM!**

   - ChÃªnh lá»‡ch RMSE chá»‰ 0.0009 (ráº¥t nhá» - chá»‰ 0.7%!)
   - RÂ² = 0.9045 (gáº§n nhÆ° LightGBM)
   - Æ¯u Ä‘iá»ƒm: Interpretable (cÃ³ thá»ƒ xem feature coefficients)

3. **Tree-based models (LightGBM, XGBoost) tá»‘t hÆ¡n linear models**

   - LightGBM vÃ  XGBoost Ä‘á»u top 4
   - Chá»©ng tá» dá»¯ liá»‡u cÃ³ pattern phá»©c táº¡p, cáº§n model máº¡nh Ä‘á»ƒ báº¯t Ä‘Æ°á»£c

4. **Linear models váº«n ráº¥t tá»‘t**

   - Lasso vÃ  ElasticNet Ä‘á»©ng top 3
   - PhÃ¹ há»£p lÃ m baseline hoáº·c khi cáº§n interpretability

5. **Huber kÃ©m nháº¥t**
   - RÂ² = 0.7820 (tháº¥p hÆ¡n nhiá»u)
   - CÃ³ thá»ƒ do robust loss khÃ´ng phÃ¹ há»£p vá»›i dá»¯ liá»‡u Ä‘Ã£ clean

## 5. PhÃ¢n TÃ­ch Chi Tiáº¿t Best Model: LightGBM

### Best Parameters

```python
{
    'subsample': 0.9,           # DÃ¹ng 90% samples má»—i tree (trÃ¡nh overfitting)
    'num_leaves': 200,          # Tá»‘i Ä‘a 200 lÃ¡ (Ä‘á»™ phá»©c táº¡p)
    'min_child_samples': 20,    # Tá»‘i thiá»ƒu 20 samples má»—i lÃ¡ (trÃ¡nh overfitting)
    'max_depth': 3,             # Äá»™ sÃ¢u tá»‘i Ä‘a = 3 (cÃ¢y nÃ´ng, generalization tá»‘t)
    'learning_rate': 0.1,       # Tá»‘c Ä‘á»™ há»c = 0.1 (vá»«a pháº£i)
    'colsample_bytree': 0.8     # DÃ¹ng 80% features má»—i tree (tÄƒng diversity)
}
```

### Táº¡i Sao LightGBM Tá»‘t Nháº¥t?

1. **Xá»­ lÃ½ Feature Interactions tá»‘t**

   - Dá»¯ liá»‡u nhÃ  Ä‘áº¥t cÃ³ nhiá»u interactions phá»©c táº¡p:
     - `OverallQual Ã— Neighborhood`: NhÃ  cháº¥t lÆ°á»£ng tá»‘t á»Ÿ khu tá»‘t = giÃ¡ ráº¥t cao
     - `GrLivArea Ã— HouseAge`: NhÃ  lá»›n cÅ© = giÃ¡ tháº¥p hÆ¡n nhÃ  lá»›n má»›i
   - Tree-based models tá»± Ä‘á»™ng báº¯t Ä‘Æ°á»£c cÃ¡c interactions nÃ y

2. **Hyperparameters Ä‘Æ°á»£c tune tá»‘t**

   - `max_depth=3`: CÃ¢y nÃ´ng â†’ TrÃ¡nh overfitting
   - `subsample=0.9`, `colsample_bytree=0.8`: ThÃªm regularization
   - Balance tá»‘t giá»¯a **bias** (Ä‘á»™ chÃ­nh xÃ¡c) vÃ  **variance** (overfitting)

3. **Performance vÆ°á»£t trá»™i**
   - RÂ² = 0.9058: Giáº£i thÃ­ch Ä‘Æ°á»£c **90.58% variance** trong giÃ¡ nhÃ 
   - CV Score = 0.01768: Tháº¥p nháº¥t â†’ Model á»•n Ä‘á»‹nh nháº¥t

**Trade-off:**

- âŒ **Interpretability**: KhÃ³ giáº£i thÃ­ch (black box) â†’ Cáº§n SHAP/LIME Ä‘á»ƒ explain
- âŒ **Resource**: Cáº§n nhiá»u RAM/CPU hÆ¡n linear models
- âœ… **Accuracy**: Tá»‘t nháº¥t trong 6 models

### So SÃ¡nh vá»›i Lasso (Model #2)

| **TiÃªu chÃ­**            | **LightGBM**             | **Lasso**       |
| ----------------------- | ------------------------ | --------------- |
| âœ… **Accuracy**         | RMSE = 0.1249 (tá»‘t nháº¥t) | RMSE = 0.1258   |
| âœ… **Stability**        | CV = 0.01768 (tháº¥p nháº¥t) | CV = 0.02043    |
| âœ… **Generalization**   | Test â‰ˆ CV                | Test â‰ˆ CV       |
| âš ï¸ **Interpretability** | Tháº¥p (black box)         | Cao (xem há»‡ sá»‘) |

**Khi nÃ o nÃªn dÃ¹ng Lasso thay LightGBM?**

âœ… **NÃªn dÃ¹ng Lasso khi:**

- Cáº§n **explainability** (stakeholders muá»‘n hiá»ƒu táº¡i sao model dá»± Ä‘oÃ¡n nhÆ° váº­y)
- Deploy trÃªn **resource háº¡n cháº¿** (edge devices, mobile apps)
- Cáº§n **baseline Ä‘Æ¡n giáº£n** trÆ°á»›c khi thá»­ ensemble

âŒ **NÃªn dÃ¹ng LightGBM khi:**

- Æ¯u tiÃªn **accuracy** cao nháº¥t
- CÃ³ Ä‘á»§ resource
- CÃ³ thá»ƒ dÃ¹ng SHAP/LIME Ä‘á»ƒ explain

## 6. Káº¿t Luáº­n: Chá»n LightGBM

### Quyáº¿t Äá»‹nh Cuá»‘i CÃ¹ng

**âœ… Chá»n LightGBM vÃ¬:**

1. **Accuracy cao nháº¥t**: RMSE 0.1249, RÂ² 0.9058
2. **Stability tá»‘t nháº¥t**: CV Score 0.01768 (tháº¥p nháº¥t)
3. **Generalization tá»‘t**: Test performance gáº§n CV performance
4. **Interpretability cÃ³ thá»ƒ bÃ¹**: DÃ¹ng SHAP/LIME Ä‘á»ƒ explain predictions

**Files Ä‘Ã£ lÆ°u:**

- âœ… `models/best_model.pkl`: Model Ä‘Ã£ train
- âœ… `models/best_model_features.json`: Danh sÃ¡ch features
- âœ… `models/best_model_config.json`: Configuration

### BÃ i Há»c Quan Trá»ng

- ğŸ¯ **KhÃ´ng cÃ³ model nÃ o lÃ  hoÃ n háº£o**: Má»—i model cÃ³ trade-off riÃªng
- ğŸ¯ **Tune hyperparameters quan trá»ng**: CÃ¹ng má»™t model, tune khÃ¡c â†’ káº¿t quáº£ khÃ¡c
- ğŸ¯ **So sÃ¡nh nhiá»u models**: Äá»«ng chá»‰ train 1 model, hÃ£y so sÃ¡nh nhiá»u models
- ğŸ¯ **Accuracy khÃ´ng pháº£i táº¥t cáº£**: Cáº§n cÃ¢n nháº¯c interpretability, resource, v.v.

**Next Steps:**

1. **Deploy vÃ o Production**: Load model tá»« `best_model.pkl`, predict giÃ¡ nhÃ  má»›i
2. **Explain Predictions**: DÃ¹ng SHAP values Ä‘á»ƒ giáº£i thÃ­ch
3. **Monitor Performance**: Theo dÃµi model qua thá»i gian, retrain khi cÃ³ data má»›i
