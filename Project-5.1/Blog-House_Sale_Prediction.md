# I. Giá»›i thiá»‡u:

Báº¡n Ä‘Ã£ bao giá» tá»± há»i: **"Táº¡i sao cÄƒn nhÃ  nÃ y láº¡i Ä‘áº¯t gáº¥p 4 láº§n cÄƒn kia?"**
Trong project nÃ y, tÃ´i khÃ´ng chá»‰ build má»™t model dá»± bÃ¡o giÃ¡ nhÃ . TÃ´i xÃ¢y dá»±ng má»™t **production-ready pipeline** tá»« A-Z, vá»›i focus vÃ o:

- ğŸ¯ **Data Quality:** KhÃ´ng bá» qua má»™t null value, má»™t logical error nÃ o
- ğŸ”’ **Zero Leakage:** Early split + cross-fit encoding Ä‘Ãºng chuáº©n
- ğŸ“Š **Feature Engineering:** Má»—i feature má»›i Ä‘á»u cÃ³ Ã½ nghÄ©a real-estate rÃµ rÃ ng
- ğŸ¨ **Transformation:** Giáº£m skewness tá»« 2.009 â†’ 0.205 (89.8%!)
- ğŸš€ **Production-Ready:** Modular code, config-driven, fully reproducible
  
**Äiá»ƒm khÃ¡c biá»‡t:** KhÃ´ng pháº£i tutorial "lÃ m theo", mÃ  lÃ  má»™t **case study thá»±c chiáº¿n** vá»›i decision rationale, trade-offs, vÃ  lessons learned tá»«ng bÆ°á»›c.
# II. CÃ¡c thÃ¡ch thá»©c:

#### 1ï¸âƒ£Â **Missing Values Chaos**

```
Total nulls: 7,829 (6.6% of dataset!)

Top missing features:
  PoolQC       99.5% missing (1,453/1,460)
  MiscFeature  96.3% missing (1,406/1,460)
  Alley        93.8% missing (1,369/1,460)
  Fence        80.8% missing (1,179/1,460)
  FireplaceQu  47.3% missing (690/1,460)
```

ğŸ’¡Â **Insight:**Â Missing â‰  Error!Â `PoolQC`Â missing nghÄ©a lÃ  "no pool", khÃ´ng pháº£i lá»—i nháº­p liá»‡u.

#### 2ï¸âƒ£Â **Logical Inconsistencies**

```python
# Example: MasVnrArea vs MasVnrType
Case 1: MasVnrArea = 0, MasVnrType = 'BrkFace' âŒ
  â†’ KhÃ´ng cÃ³ veneer nhÆ°ng láº¡i cÃ³ type? DELETE!
  
Case 2: MasVnrArea = 288, MasVnrType = NULL âš ï¸
  â†’ CÃ³ veneer nhÆ°ng thiáº¿u type? FILL with mode!
```

#### 3ï¸âƒ£Â **Extreme Skewness**

```
SalePrice skewness: 2.009 (highly right-skewed)
  â†’ Violates normality assumption cho linear regression
  â†’ Cáº§n transform!
```

![Missing Values and Logical Errors Illustration](images/Pasted%20image%2020251028104423.png)

#### 4ï¸âƒ£Â **High Cardinality Categoricals**

```
Neighborhood: 25 unique values
Exterior2nd:  16 unique values
  â†’ One-hot encoding = 41 sparse columns!
  â†’ Giáº£i phÃ¡p: Target encoding
```

#### 5ï¸âƒ£ High Correlation Features
CÃ¡c cáº·p biáº¿n cÃ³ tÆ°Æ¡ng quan > 80%:

| ID   | **Feature 1** | **Feature 2** | **Correlation** |
| ---- | ------------- | ------------- | --------------- |
| 988  | GarageCars    | GarageArea    | 0.882475        |
| 1025 | GarageArea    | GarageCars    | 0.882475        |
| 246  | YearBuilt     | GarageYrBlt   | 0.825667        |
| 931  | GarageYrBlt   | YearBuilt     | 0.825667        |
| 614  | GrLivArea     | TotRmsAbvGrd  | 0.825489        |
| 867  | TotRmsAbvGrd  | GrLivArea     | 0.825489        |
| 493  | 1stFlrSF      | TotalBsmtSF   | 0.819530        |
| 456  | TotalBsmtSF   | 1stFlrSF      | 0.819530        |

# III. CÃ¡ch xá»­ lÃ½ vÃ  cÃ¡c bÆ°á»›c trong Data Preparationeparation
## 1. Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAW DATA (1,460 Ã— 81)                                       â”‚
â”‚  â”œâ”€ 7,829 nulls                                              â”‚
â”‚  â”œâ”€ Logical errors                                           â”‚
â”‚  â””â”€ Mixed data types                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: PREPROCESSING                                       â”‚
â”‚  â”œâ”€ Fix MasVnr logic (delete 2 rows)                        â”‚
â”‚  â”œâ”€ Fill 6,940 nulls                                        â”‚
â”‚  â”œâ”€ Fix Garage consistency                                  â”‚
â”‚  â””â”€ Train/Test Split (85/15)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                          â”‚
                  â–¼                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TRAIN       â”‚            â”‚ TEST        â”‚
         â”‚ 1,239 Ã— 81  â”‚            â”‚ 219 Ã— 81    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                          â”‚
                  â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: FEATURE ENGINEERING                                 â”‚
â”‚  â”œâ”€ Create derived features (+6)                            â”‚
â”‚  â”œâ”€ Orthogonalize residuals                                 â”‚
â”‚  â””â”€ Binary flags (Has* features)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: TRANSFORMATION                                      â”‚
â”‚  â”œâ”€ log1p(SalePrice): skew 2.009 â†’ 0.205                   â”‚
â”‚  â”œâ”€ log1p for 15 features                                   â”‚
â”‚  â”œâ”€ Yeo-Johnson for 9 features                              â”‚
â”‚  â””â”€ Binning: KitchenAbvGr                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: ENCODING                                            â”‚
â”‚  â”œâ”€ Ordinal: 17 features (Quality scales)                   â”‚
â”‚  â”œâ”€ One-Hot: 110 features (Nominal cats)                    â”‚
â”‚  â”œâ”€ Target Encoding: 2 features (Cross-fit K=5)            â”‚
â”‚  â””â”€ StandardScaler: All 176 features                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FINAL: PRODUCTION-READY                                     â”‚
â”‚  â”œâ”€ 1,239 Ã— 173 (train)                                     â”‚
â”‚  â”œâ”€ 219 Ã— 173 (test)                                        â”‚
â”‚  â”œâ”€ 0 nulls, 0 logical errors                               â”‚
â”‚  â””â”€ Zero leakage, fully reproducible                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## 2. Preprocessing (Step 1)

#### Sá»­a logic MasVnrType/MasVnrArea: 

**Implementation:**
```python
def fix_masonry_veneer_logic(self):
	original_len = len(self.df)
	
	# Get mode cá»§a MasVnrType (for Case 2)
	mode_type = self.df['MasVnrType'].mode()[0]
	
	# Case 1: Area=0, Typeâ‰ NULL â†’ DELETE
	case1_mask = (self.df['MasVnrArea'] == 0) & (self.df['MasVnrType'].notna())
	case1_count = case1_mask.sum()
	self.df = self.df[~case1_mask]
	
	# Case 2: Area>0, Type=NULL â†’ FILL mode
	case2_mask = (self.df['MasVnrArea'] > 0) & (self.df['MasVnrType'].isna())
	case2_count = case2_mask.sum()
	self.df.loc[case2_mask, 'MasVnrType'] = mode_type
	
	# Case 3: Both NULL â†’ 'None'
	case3_mask = (self.df['MasVnrArea'].isna()) | (self.df['MasVnrType'].isna())
	case3_count = case3_mask.sum()
	if case3_count > 0:
		self.df.loc[case3_mask, 'MasVnrType'] = 'None'
		self.df.loc[case3_mask, 'MasVnrArea'] = 0
	
	deleted = original_len - len(self.df)
	
	return self
```
=> XoÃ¡ 2 dÃ²ng mÃ¢u thuáº«n; cÃ²n **1458 Ã— 81**.

| **Before**                       | **After**      | **Action**       |
| -------------------------------- | -------------- | ---------------- |
| Id=689: Area=0, Type='BrkFace' âŒ | DELETED        | Logical error    |
| Id=1242: Area=0, Type='Stone' âŒ  | DELETED        | Logical error    |
| Id=625: Area=288, Type=NULL âœ…    | Type='BrkFace' | Filled with mode |

#### Fill missing values cÃ³ chá»§ Ä‘Ã­ch: 

Danh má»¥c (Categories) â†’ `'None'`; biáº¿n Ä‘áº¿m/diá»‡n tÃ­ch (Numerics) â†’ `0`; sá»‘ khÃ¡c â†’ **median**.

**Implementation:**
```python
def fill_missing_values(self):
	total_nulls_before = self.df.isnull().sum().sum()
	
	# Categorical columns â†’ 'None'
	categorical_cols = self.df.select_dtypes(include=['object']).columns
	cat_nulls = self.df[categorical_cols].isnull().sum().sum()
	
	if cat_nulls > 0:
		for col in categorical_cols:
			if self.df[col].isnull().sum() > 0:
				self.df[col] = self.df[col].fillna('None')
	
	# Numeric columns: count/area â†’ 0, others â†’ median
	numeric_cols = self.df.select_dtypes(include=[np.number]).columns
	
	# Count/area columns â†’ 0
	count_area_cols = ['GarageCars', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2',
					'BsmtUnfSF', 'TotalBsmtSF', 'FullBath', 'HalfBath',
					'BedroomAbvGr', 'KitchenAbvGr', 'WoodDeckSF', 'OpenPorchSF',
					'EnclosedPorch', 'ScreenPorch', '3SsnPorch', 'PoolArea',
					'MasVnrArea', '2ndFlrSF', 'BsmtFullBath', 'BsmtHalfBath']
	
	for col in count_area_cols:
		if col in self.df.columns and self.df[col].isnull().sum() > 0:
			self.df[col] = self.df[col].fillna(0)
	
	count_area_nulls = sum(self.df[col].isnull().sum() \
		for col in count_area_cols if col in self.df.columns)
	
	# Other numeric â†’ median
	other_numeric_nulls = self.df[numeric_cols].isnull().sum().sum()
	if other_numeric_nulls > 0:
		for col in numeric_cols:
			if self.df[col].isnull().sum() > 0:
				median_val = self.df[col].median()
				self.df[col] = self.df[col].fillna(median_val)
	
	total_nulls_after = self.df.isnull().sum().sum()
	
	return self
```

#### Garage consistency:

KhÃ³a cháº·t logic náº¿u `GarageArea=0` thÃ¬ toÃ n bá»™ thuá»™c tÃ­nh garage = `'None'`.

**Implementation:**
```python
def fix_garage_logic(self):
	garage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']
	
	# Find rows without garage
	no_garage_mask = self.df['GarageArea'] == 0
	no_garage_count = no_garage_mask.sum()
	
	# Set 'None' for rows without garage
	for col in garage_cols:
		if col in self.df.columns:
			self.df.loc[no_garage_mask, col] = 'None'
	
	# Fill remaining nulls vá»›i mode
	for col in garage_cols:
		if col in self.df.columns and self.df[col].isnull().sum() > 0:
			mode_val = self.df[col].mode()[0] if len(self.df[col].mode()) > \
															0 else 'None'
			null_count = self.df[col].isnull().sum()
			self.df[col] = self.df[col].fillna(mode_val)
	
	return self
```
#### Chia 85/15 ngay sau khi dá»¯ liá»‡u sáº¡ch cÆ¡ báº£n

KhÃ³a test, chia sá»›m ngÄƒn leakage cho má»i bÆ°á»›c sau.

**Implementation:**
```python
def run_preprocessing(self):
	from src.Preprocessing import Preprocessor
	from sklearn.model_selection import train_test_split
	
	# Load raw data
	df_raw = pd.read_csv(self.raw_data_path)
	
	# Run preprocessing pipeline
	preprocessor = Preprocessor(df_raw)
	df_clean = (preprocessor
				.fix_masonry_veneer_logic()
				.fill_missing_values()
				.fix_garage_logic()
				.get_summary()
				.get_dataframe())
	
	# Save cleaned data
	preprocessed_path = self.processed_dir / 'train_preprocessed.csv'
	df_clean.to_csv(preprocessed_path, index=False)
	
	# Split into train/test (85/15)
	target = df_clean['SalePrice']
	X = df_clean.drop('SalePrice', axis=1)
	
	X_train, X_test, y_train, y_test = train_test_split(
		X, target, test_size=0.15, random_state=42
	)
	
	train_df = pd.concat([X_train, y_train], axis=1)
	test_df = pd.concat([X_test, y_test], axis=1)
	
	train_path = self.processed_dir / 'train_data.csv'
	test_path = self.processed_dir / 'test_data.csv'
	
	train_df.to_csv(train_path, index=False)
	test_df.to_csv(test_path, index=False)
	
	return True
```

#### Káº¿t quáº£ Preprocessing:
```
âœ… Shape: 1,460 â†’ 1,458 (deleted 2 logical errors)
âœ… Nulls: 6,940 â†’ 0 (100% clean)
âœ… Garage consistency: Fixed 81 rows
âœ… Train/Test: Split early (no leakage)
```
## 3.  Feature Engineering (Step 2)

**Ã tÆ°á»Ÿng cá»‘t lÃµi:** vá»›i cáº·p biáº¿n dá»… **trÃ¹ng thÃ´ng tin** (multicollinearity), ta â€œ**partial-out**â€ pháº§n Ä‘Ã£ giáº£i thÃ­ch bá»Ÿi biáº¿n cÆ¡ sá»Ÿ, chá»‰ giá»¯ **pháº§n dÆ° Ä‘á»™c láº­p** (residual). Ta lÃ m cho biáº¿n má»›i **Ã­t phá»¥ thuá»™c** vÃ o biáº¿n gá»‘c, giÃºp mÃ´ hÃ¬nh há»c â€œtÃ­n hiá»‡u tháº­tâ€ thay vÃ¬ láº·p láº¡i cÃ¹ng má»™t thÃ´ng tin.

#### Garage Features
```
â”œâ”€ Táº¡o: GarageAreaPerCar = GarageArea / GarageCars (Quy mÃ´ Garage)
â”œâ”€ Táº¡o: HasGarage (binary flag)
â””â”€ Bá»: GarageCars (multicollinear vá»›i GarageArea)
```
**Implementation:**
```python
def engineer_garage_features(self):
	# Feature 1: GarageAreaPerCar
	self.df['GarageAreaPerCar'] = np.where(
		self.df['GarageCars'] > 0,
		self.df['GarageArea'] / self.df['GarageCars'],
		0
	)
	self.new_features.append('GarageAreaPerCar')
	
	# Feature 2: HasGarage
	self.df['HasGarage'] = (self.df['GarageArea'] > 0).astype(int)
	self.new_features.append('HasGarage')
	
	# Drop GarageCars
	if 'GarageCars' in self.df.columns:
		self.df = self.df.drop('GarageCars', axis=1)
	
	return self
```
**Äá»‹nh nghÄ©a**:
- **GarageAreaPerCar**: Äo â€œÄ‘á»™ rá»™ng rÃ£iâ€ cá»§a garage. ~250â€“300 sqft/car = chuáº©n; 400â€“500+ = rá»™ng/luxury.
- **HasGarage**: 1 náº¿u nhÃ  cÃ³ garage (GarageArea > 0), ngÆ°á»£c láº¡i 0.
**Thay tháº¿**: Drop `GarageCars` (trÃ¹ng thÃ´ng tin vá»›i `GarageArea`); dÃ¹ng `GarageAreaPerCar` + `HasGarage` Ä‘á»ƒ tÃ¡ch scale vÃ  existence.
#### Area Features
```
â”œâ”€ Táº¡o: AvgRoomSize = GrLivArea / TotRmsAbvGrd
â””â”€ Bá»: TotRmsAbvGrd (multicollinear vá»›i GrLivArea)
```
**Implementation**:
```python
def engineer_area_features(self):
	# Feature 1: AvgRoomSize
	self.df['AvgRoomSize'] = np.where(
		self.df['TotRmsAbvGrd'] > 0,
		self.df['GrLivArea'] / self.df['TotRmsAbvGrd'],
		0
	)
	self.new_features.append('AvgRoomSize')
	
	# Drop TotRmsAbvGrd
	if 'TotRmsAbvGrd' in self.df.columns:
		self.df = self.df.drop('TotRmsAbvGrd', axis=1)
	
	return self
```
**Äá»‹nh nghÄ©a:** Pháº£n Ã¡nh cáº£m giÃ¡c rá»™ngâ€“cháº­t cá»§a khÃ´ng gian sá»‘ng (â‰ˆ200 = vá»«a; 300+ = rá»™ng rÃ£i/luxury).

**Thay tháº¿:** Drop `TotRmsAbvGrd` (corr cao vá»›i `GrLivArea`), giá»¯ `GrLivArea` (scale) + `AvgRoomSize` (efficiency).
#### Basement Features
```
â”œâ”€ Táº¡o: HasBasement (binary flag)
â”œâ”€ Táº¡o: BasementResid (orthogonalized vá»›i 1stFlrSF + HasBasement)
â””â”€ Bá»: TotalBsmtSF (multicollinear vá»›i 1stFlrSF)
```
**Implementation:**
```python
def engineer_basement_features(self):
	# Feature 1: HasBasement
	self.df['HasBasement'] = (self.df['TotalBsmtSF'] > 0).astype(int)
	self.new_features.append('HasBasement')
	
	# Feature 2: BasementResid (orthogonalized)
	X_basement = self.df[['1stFlrSF', 'HasBasement']].values
	y_bsmt = self.df['TotalBsmtSF'].values
	
	# DÃ¹ng há»“i quy tuyáº¿n tÃ­nh Ä‘á»ƒ loáº¡i bá» pháº§n giáº£i thÃ­ch Ä‘Æ°á»£c cá»§a TotalBsmtSF 
	# dá»±a trÃªn 1stFlrSF vÃ  HasBasement
	model = LinearRegression()
	model.fit(X_basement, y_bsmt)
	
	# BasementResid lÃ  pháº§n dÆ° sau há»“i quy: y_bsmt - model.predict(X_basement), 
	# giÃºp orthogonal hÃ³a Ä‘áº·c trÆ°ng diá»‡n tÃ­ch háº§m
	self.df['BasementResid'] = y_bsmt - model.predict(X_basement)
	self.new_features.append('BasementResid')
	
	# Drop TotalBsmtSF
	if 'TotalBsmtSF' in self.df.columns:
		self.df = self.df.drop('TotalBsmtSF', axis=1)
	
	return self
```
**Äá»‹nh nghÄ©a:**
- Orthogonalization (trá»±c giao hÃ³a): táº¡o residual Ä‘á»ƒ loáº¡i phá»¥ thuá»™c tuyáº¿n tÃ­nh â†’ tÃ­n hiá»‡u Ä‘á»™c láº­p, giáº£m VIF.
- BasementResid > 0: Basement lá»›n hÆ¡n ká»³ vá»ng; < 0: nhá» hÆ¡n ká»³ vá»ng.
**Thay tháº¿:** Drop `TotalBsmtSF`; dÃ¹ng `HasBasement` + `BasementResid` (r vá»›i `1stFlrSF` tá»« 0.815 â†’ 0.000).
#### Age Features
```
â”œâ”€ Táº¡o: HouseAge (nÄƒm tá»« khi xÃ¢y dá»±ng)
â”œâ”€ Táº¡o: GarageLag (garage construction lag)
â”œâ”€ Táº¡o: GarageSameAsHouse (binary flag)
â””â”€ Bá»: YearBuilt, GarageYrBlt (redundant)
```
**Implementation:**
```python
def engineer_age_features(self):
	# Feature 1: HouseAge
	self.df['HouseAge'] = self.df['YrSold'] - self.df['YearBuilt']
	self.new_features.append('HouseAge')
	
	# Feature 2: GarageLag
	self.df['GarageLag'] = self.df['GarageYrBlt'] - self.df['YearBuilt']
	self.new_features.append('GarageLag')
	
	# Feature 3: GarageSameAsHouse
	self.df['GarageSameAsHouse'] = (self.df['GarageYrBlt'] == \
		self.df['YearBuilt']).astype(int)
	self.new_features.append('GarageSameAsHouse')
	
	# Drop raw year features
	cols_to_drop = []
	for col in ['YearBuilt', 'GarageYrBlt', 'GarageAge']:
		if col in self.df.columns:
			self.df = self.df.drop(col, axis=1)
			cols_to_drop.append(col)
		
	return self
```
**Äá»‹nh nghÄ©a**: DÃ¹ng tuá»•i tÆ°Æ¡ng Ä‘á»‘i thay vÃ¬ nÄƒm tuyá»‡t Ä‘á»‘i (1950 vs 2000). `GarageLag` lÃ  chá»‰ bÃ¡o renovation/upgrade.
**Thay tháº¿**: Drop `YearBuilt`, `GarageYrBlt`; dÃ¹ng `HouseAge`, `GarageLag`, `GarageSameAsHouse`.
#### Quality Features
```
â”œâ”€ Fireplace: HasFireplace + ExtraFireplaces
â”œâ”€ Masonry: HasMasonryVeneer + MasVnrAreaResid (orthogonalized)
â”œâ”€ Second Floor: Has2ndFlr + SecondFlrShare_resid (orthogonalized)
â””â”€ Bá»: Raw features (highly correlated)
```
**Implementation:**
```python
def engineer_quality_features(self):
	# ===== FIREPLACE =====
	self.df['HasFireplace'] = (self.df['Fireplaces'] > 0).astype(int)
	self.new_features.append('HasFireplace')
	self.df['ExtraFireplaces'] = np.maximum(self.df['Fireplaces'] - 1, 0)
	self.new_features.append('ExtraFireplaces')
	
	if 'Fireplaces' in self.df.columns:
		self.df = self.df.drop('Fireplaces', axis=1)
	
	# ===== MASONRY VENEER =====
	self.df['MasVnrArea'] = self.df['MasVnrArea'].fillna(0)
	self.df['HasMasonryVeneer'] = (self.df['MasVnrArea'] > 0).astype(int)
	self.new_features.append('HasMasonryVeneer')
	
	# MasVnrAreaResid (orthogonalized)
	X_masonry = self.df[['HasMasonryVeneer', 'OverallQual']].values
	y_masonry = self.df['MasVnrArea'].values
	
	model_m = LinearRegression()
	model_m.fit(X_masonry, y_masonry)
	
	self.df['MasVnrAreaResid'] = y_masonry - model_m.predict(X_masonry)
	self.new_features.append('MasVnrAreaResid')
	
	if 'MasVnrArea' in self.df.columns:
		self.df = self.df.drop('MasVnrArea', axis=1)
	
	# ===== SECOND FLOOR =====
	self.df['Has2ndFlr'] = (self.df['2ndFlrSF'] > 0).astype(int)
	self.new_features.append('Has2ndFlr')
	
	# SecondFlrShare_resid (orthogonalized)
	second_flr_share = np.where(
		self.df['GrLivArea'] > 0,
		self.df['2ndFlrSF'] / self.df['GrLivArea'],
		0
	)
	
	X_share = self.df[['Has2ndFlr']].values
	
	model_s = LinearRegression()
	model_s.fit(X_share, second_flr_share)
	
	self.df['SecondFlrShare_resid'] = second_flr_share - model_s.predict(X_share)
	self.new_features.append('SecondFlrShare_resid')
	
	if '2ndFlrSF' in self.df.columns:
		self.df = self.df.drop('2ndFlrSF', axis=1)
	
	return self
```
**Äá»‹nh nghÄ©a:** 
- `HasFireplace` + `ExtraFireplaces` - model há»c riÃªng â€œcÃ³/khÃ´ngâ€ (comfort) vÃ  â€œbao nhiÃªu thÃªmâ€ (luxury).
- `MasVnrAreaResid` = veneer â€œdÆ°â€ sau khi Ä‘Ã£ tÃ­nh Ä‘áº¿n quality vÃ  viá»‡c cÃ³ veneer; giá»¯ tÃ­n hiá»‡u Ä‘á»™c láº­p.
- `Has2ndFlr` + `SecondFlrShare_resid` - tÃ¡ch â€œcÃ³ táº§ng 2â€ vÃ  â€œtá»· trá»ng táº§ng 2â€ khá»i quy mÃ´ tá»•ng, trÃ¡nh giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n.
**Thay tháº¿:** Drop `Fireplaces`, `MasVnrArea`, `2ndFlrSF`t

**Káº¿t quáº£:**
> - Efficiency metric: Tá»· lá»‡ hiá»‡u suáº¥t (vÃ­ dá»¥ diá»‡n tÃ­ch/xe, diá»‡n tÃ­ch/phÃ²ng).
> - Orthogonalization/Residual: Pháº§n sai lá»‡ch khÃ´ng giáº£i thÃ­ch bá»Ÿi biáº¿n ná»n táº£ng â†’ giáº£m multicollinearity.
> - Binary flag: Biáº¿n 0/1 biá»ƒu diá»…n sá»± tá»“n táº¡i cá»§a Ä‘áº·c Ä‘iá»ƒm.

![Feature Engineering Illustration](images/Pasted%20image%2020251028142135.png)
## 4. Transformation (Step 3)
#### Target Transformation

**Implementation**:
```python
def _transform_target(self):
	if 'SalePrice' in self.train_data.columns:
		# LÆ°u giÃ¡ trá»‹ ban Ä‘áº§u
		y_train = self.train_data['SalePrice'].values
		y_test = self.test_data['SalePrice'].values
		
		# DÃ¹ng log1p Ä‘á»ƒ giáº£m skew
		y_train_transformed = np.log1p(y_train)
		y_test_transformed = np.log1p(y_test)
		
		# Ghi Ä‘Ã¨ dá»¯ liá»‡u
		self.train_data['SalePrice'] = y_train_transformed
		self.test_data['SalePrice'] = y_test_transformed

	return self
```

```
â”œâ”€ SalePrice â†’ log1p(SalePrice)
â”œâ”€ Skewness: 2.009 â†’ 0.205 (giáº£m 89.8%)
â””â”€ Káº¿t quáº£: Nearly symmetric distribution âœ“
```
![Feature Engineering Example 1](images/Pasted%20image%2020251028142603.png)
![Feature Engineering Example 2](images/Pasted%20image%2020251028142715.png)
#### Feature Transformations

**Implementation:**
```python
def _bin_kitchen_abvgr(self):
	# HÃ m phÃ¢n nhÃ³m KitchenAbvGr thÃ nh 3 bin:
	# - 0 vÃ  1: bin 0
	# - 2: bin 1
	# - >=3: bin 2
	def bin_kitchen(x):
		if x <= 1:
			return 0
		elif x == 2:
			return 1
		else:
			return 2
	
	# Táº¡o cá»™t binned má»›i
	self.train_data['KitchenAbvGr_Binned'] = \
		self.train_data['KitchenAbvGr'].apply(bin_kitchen)
	self.test_data['KitchenAbvGr_Binned'] = \ 
		self.test_data['KitchenAbvGr'].apply(bin_kitchen)
	
	# Táº¡o cá» "HasMultiKitchen": 1 náº¿u cÃ³ â‰¥2 kitchen
	self.train_data['HasMultiKitchen'] = \
		(self.train_data['KitchenAbvGr'] >= 2).astype(int)
	self.test_data['HasMultiKitchen'] = \
		(self.test_data['KitchenAbvGr'] >= 2).astype(int)
	
	# XÃ³a cá»™t cÅ© KitchenAbvGr
	self.train_data = self.train_data.drop('KitchenAbvGr', axis=1)
	self.test_data = self.test_data.drop('KitchenAbvGr', axis=1)

	return self

def _transform_features_log(self):
	# Chá»n ra nhá»¯ng Ä‘áº·c trÆ°ng Ã¡p dá»¥ng log1p
	log_features = [f for f, s in self.feature_strategy.items() if s == 'log']
	
	for feat in log_features:
		if feat in self.train_data.columns:
			# Skew trÆ°á»›c biáº¿n Ä‘á»•i
			original_skew = skew(self.train_data[feat].dropna())
			
			# Biáº¿n Ä‘á»•i báº±ng log1p, ghi sang cá»™t má»›i
			self.train_data[f'{feat}_log'] = np.log1p(self.train_data[feat])
			self.test_data[f'{feat}_log'] = np.log1p(self.test_data[feat])
			
			# Skew sau biáº¿n Ä‘á»•i
			transformed_skew = skew(self.train_data[f'{feat}_log'].dropna()) 
			
			# Loáº¡i bá» Ä‘áº·c trÆ°ng gá»‘c (chá»‰ giá»¯ Ä‘áº·c trÆ°ng má»›i)
			self.train_data = self.train_data.drop(feat, axis=1)
			self.test_data = self.test_data.drop(feat, axis=1)
			
	return self

def _transform_features_yeo_johnson(self):
	from sklearn.preprocessing import PowerTransformer
	
	# Chá»n ra Ä‘áº·c trÆ°ng cáº§n biáº¿n Ä‘á»•i Yeo-Johnson hoáº·c log1p cho zero-inflated
	yj_features = [f for f, s in self.feature_strategy.items()
	if s in ['yeo_johnson', 'log_zero_inflated']]
	# Chá»‰ láº¥y nhá»¯ng cá»™t hiá»‡n diá»‡n thá»±c táº¿ trong data
	numeric_df_train = self.train_data[[f for f in \
		yj_features if f in self.train_data.columns]].copy()
	numeric_df_test = self.test_data[[f for f in \
		yj_features if f in self.test_data.columns]].copy()
	
	# Táº¡o transformer (khÃ´ng chuáº©n hÃ³a mean/std)
	pt = PowerTransformer(method='yeo-johnson', standardize=False)
	
	# Fit transformer trÃªn train data
	pt.fit(numeric_df_train)
	
	# Ãp dá»¥ng biáº¿n Ä‘á»•i trÃªn cáº£ hai táº­p train/test
	train_transformed = pt.transform(numeric_df_train)
	test_transformed = pt.transform(numeric_df_test)
	
	# ThÃªm cá»™t má»›i vÃ  lÆ°u láº¡i thÃ´ng tin biáº¿n Ä‘á»•i
	for idx, feat in enumerate([f for f in \
			yj_features if f in self.train_data.columns]):
		original_skew = skew(numeric_df_train[feat].dropna())
		transformed_skew = skew(train_transformed[:, idx])
		
		self.train_data[f'{feat}_yj'] = train_transformed[:, idx]
		self.test_data[f'{feat}_yj'] = test_transformed[:, idx]
		
		# XÃ³a Ä‘áº·c trÆ°ng gá»‘c
		if feat in self.train_data.columns:
			self.train_data = self.train_data.drop(feat, axis=1)
		if feat in self.test_data.columns:
			self.test_data = self.test_data.drop(feat, axis=1)
	
	return self
```

```
â”œâ”€ Log1p: 15 features (táº¥t cáº£ positive values)
â”œâ”€ Yeo-Johnson: 9 features (zero-inflated/negative)
â”œâ”€ Binning: KitchenAbvGr â†’ KitchenAbvGr_Binned
â””â”€ No transform: Binary flags + residuals (orthogonal)
```
**Káº¿t quáº£:**
![Feature Transform After Log1p](images/Pasted%20image%2020251028144755.png)

**Log1p**: PhÃ¢nÂ phá»‘iÂ lá»‡chÂ pháº£iÂ (GrLivArea, LotArea,Â 1stFlrSF, AvgRoomSize) trá»ŸÂ nÃªnÂ Ä‘á»‘i xá»©ngÂ hÆ¡n rÃµÂ rá»‡t; Ä‘uÃ´iÂ pháº£i ngáº¯nÂ láº¡i,

![Feature Transform After Log for Other Features](images/Pasted%20image%2020251028145402.png)

**Yeo-Johnson:** CÃ¡c biáº¿n zero/Ä‘áº¿m/Ã¢mÂ (GarageArea, FullBath, HouseAge, GarageLag) trÆ¡nÂ tru hÆ¡n, giáº£m spikeÂ táº¡iÂ 0; phÃ¢n phá»‘i gáº§n GaussianÂ hÆ¡n.

![Feature Transform After Yeo-Johnson](images/Pasted%20image%2020251028145413.png)

**Binning**:Â KitchenAbvGrÂ â†’Â KitchenAbvGr_BinnedÂ gomÂ 0â€“1 thÃ nhÂ binÂ 0, 2 thÃ nhÂ binÂ 1,Â 3+ thÃ nh binÂ 2; meanÂ SalePriceÂ theo binÂ xÃ¡c nháº­nÂ insight: multi-kitchen thÆ°á»ng ráº»Â hÆ¡n trong Ames (duplex/thuÃª).

**LÃ­ do chá»n KitchenAbvGr Ä‘á»ƒ Binning: Extreme Imbalance (95% + 5%Â + 0.1%)**
- One-hot encodingÂ â†’Â 3-4 sparseÂ columns vá»›iÂ 95% zerosÂ (lÃ£ngÂ phÃ­)
- BinningÂ â†’Â 3 meaningfulÂ groupsÂ (baseline, duplex, rare)

| **Bin** | **Original** | **Count**   | **Mean Price** | **Meaning**         |
| ------- | ------------ | ----------- | -------------- | ------------------- |
| 0       | 0-1 kitchen  | 1,178 (95%) | $169,211       | Standard home       |
| 1       | 2 kitchens   | 60 (5%)     | $125,530       | Duplex/Multi-family |
| 2       | 3+ kitchens  | 1 (0.08%)   | $113,000       | Rare/luxury         |

ğŸ’¡Â **Counterintuitive:**Â More kitchens = LOWER price trong dataset nÃ y! â†’ VÃ¬ duplex thÆ°á»ng á»Ÿ neighborhoods giÃ¡ tháº¥p hÆ¡n single-family homes á»Ÿ Ames, Iowa.

**Káº¿t quáº£:**
- ÄÃ£ xá»­ lÃ½ skewness cho 24 biáº¿n Ä‘áº§u vÃ o
- Biáº¿n má»¥c tiÃªu phÃ¢n phá»‘i gáº§n Ä‘á»‘i xá»©ng (skewness xáº¥p xá»‰ 0.2)
- Chiáº¿n lÆ°á»£c cross-fit Ä‘áº£m báº£o khÃ´ng rÃ² rá»‰ dá»¯ liá»‡u huáº¥n luyá»‡n
- QuÃ¡ trÃ¬nh biáº¿n Ä‘á»•i luÃ´n cÃ³ thá»ƒ thá»±c hiá»‡n láº¡i Ä‘Æ°á»£c dá»… dÃ ng
## 5. Encoding Strategies (Step 4)

### 5.1 ThÃ¡ch thá»©c: 43 Features Categorical

**PhÃ¢n tÃ­ch theo Cardinality:**
- Cardinality tháº¥p (â‰¤5): 27 features
- Cardinality trung bÃ¬nh (6-10): 13 features
- Cardinality cao (>10): 3 features â†’ `Neighborhood`, `Exterior1st`, `Exterior2nd`
  
**Váº¥n Ä‘á»:** One-hot encoding táº¥t cáº£ 43 features sáº½ táº¡o ra quÃ¡ nhiá»u cá»™t sparse!

### 5.2 Ordinal Encoding (17 features)

```
â”œâ”€ Quality scales: ExterQual, KitchenQual, BsmtQual, etc.
â”œâ”€ Mapping: Ex (Xá»‹n) > Gd (Good) > TA (Trung bÃ¬nh) > Fa (Táº¡m) > Po (Poor)
â”œâ”€ Finish levels: GarageFinish (HoÃ n thÃ nh), BsmtFinType1/2 (Má»›i xong 1/2)
â””â”€ Shape/Slope: LotShape, LandSlope, PavedDrive (liÃªn quan tá»›i lÃ´ Ä‘áº¥t)
```
**Implementation**:
```python
from sklearn.preprocessing import OrdinalEncoder

# ----- OrdinalEncoder -----
# Ta cáº§n Ä‘Æ°a thá»© tá»± category cho tá»«ng cá»™t ordinal
# sklearn.OrdinalEncoder muá»‘n categories=[list_for_col1, list_for_col2, ...]
ordinal_categories = []
for col in ordinal_cols:
	mapping = self.ordinal_mappings[col] # ex: {'None': -1, 'Po':0, ...}
	# sáº¯p xáº¿p key theo giÃ¡ trá»‹ rank tÄƒng dáº§n
	ordered_levels = sorted(mapping.keys(), key=lambda k: mapping[k])

ordinal_categories.append(ordered_levels)
ordinal_encoder = OrdinalEncoder(
	categories=ordinal_categories,
	handle_unknown='use_encoded_value',
	unknown_value=-1,
	encoded_missing_value=-1,
)
```
![Ordinal Encoding Example](images/Pasted%20image%2020251028162143.png)
**BÆ°á»›c Nháº£y GiÃ¡: TA â†’ Ex = +154%**
- TA: $145.246 â†’ Ex: $368.929
- **ChÃªnh lá»‡ch: $223.683 (+154%)**
- **Giáº£i thÃ­ch:** NhÃ  cÃ³ cháº¥t lÆ°á»£ng ngoÃ i tháº¥t xuáº¥t sáº¯c bÃ¡n Ä‘Æ°á»£c vá»›i giÃ¡ **gáº¥p 2,5 láº§n** so vá»›i nhÃ  cháº¥t lÆ°á»£ng bÃ¬nh thÆ°á»ng!

### 5.3 Target Encoding (2 features)

**Problem:** Cardinality Cao
- **Neighborhood:** 25 loáº¡i duy nháº¥t
- **Exterior2nd:** 16 loáº¡i duy nháº¥t
- **Tá»•ng:** 41 loáº¡i â†’ One-hot sáº½ táº¡o 39 cá»™t sparse!
**Solution:** Target Encoding vá»›i Cross-fit K-fold (K=5)
```python
from sklearn.preprocessing import TargetEncoder
from sklearn.impute import SimpleImputer

# ----- TargetEncoder -----
# Vá»›i TargetEncoder, ta cÅ©ng impute most_frequent trÆ°á»›c
# TargetEncoder trong sklearn cÃ³ tham sá»‘ cv=5 máº·c Ä‘á»‹nh Ä‘á»ƒ cross-fit
# vÃ  fit_transform() sáº½ tá»± dÃ¹ng cross-fitting Ä‘á»ƒ trÃ¡nh leakage.
# Khi Pipeline.fit_transform cháº¡y cho train, nÃ³ sáº½ gá»i Ä‘Ãºng logic nÃ y.
tgt_pipe = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('tgt', TargetEncoder(
		cv=5, # cross-fit K-fold =5
		smooth='auto', # smoothing shrink vá» global mean
		random_state=0,
	)),
])
```
**CÃ¡ch hoáº¡t Ä‘á»™ng tá»± Ä‘á»™ng:**
1. sklearn chia train thÃ nh 5 folds
2. Vá»›i má»—i fold, tÃ­nh mean tá»« 4 folds KHÃC
3. Ãp dá»¥ng vÃ o fold hiá»‡n táº¡i
â†’ Má»—i fold KHÃ”NG BAO GIá»œ nhÃ¬n tháº¥y target cá»§a chÃ­nh nÃ³!
â†’ **Zero leakage guaranteed!** âœ…

**Káº¿t quáº£:**
- **TÆ°Æ¡ng quan Neighborhood_target_enc:** r = +0.7397 (feature máº¡nh nháº¥t #2!)
- **TÆ°Æ¡ng quan Exterior2nd_target_enc:** r = +0.3965
### 5.4 One-hot Encoding (24 features -> 110 cá»™t)

**Implementation:**
```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

# ----- OneHotEncoder -----
# Nhiá»u cá»™t nominal cÃ³ NA => Imputer(most_frequent) trÆ°á»›c OHE
ohe_pipe = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('ohe', OneHotEncoder(
		handle_unknown='ignore',
		drop='first', # trÃ¡nh multicollinearity quÃ¡ máº¡nh
		sparse_output=False, # output dense Ä‘á»ƒ scaler xá»­ lÃ½ Ä‘Æ°á»£c
	)),
])
```

**Xá»­ lÃ½ Sparse matrices:**
- Háº§u háº¿t cá»™t one-hot lÃ  sparse (nhiá»u zeros)
- Sklearn xá»­ lÃ½ hiá»‡u quáº£ vá»›i sparse matrices
- LÆ°u trá»¯ vÃ  tÃ­nh toÃ¡n tiáº¿t kiá»‡m bá»™ nhá»›

![One-hot Encoding Example](images/Pasted%20image%2020251028172509.png)
### 5.5 Feature Scaling

**Implementation:**
```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline.Pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# ----- ColumnTransformer -----
pre = ColumnTransformer(
	transformers=[
		('ord', ordinal_encoder, ordinal_cols),
		('ohe', ohe_pipe, ohe_cols),
		('tgt', tgt_pipe, target_cols),
		('num', num_pipe, numeric_cols),
	],
	remainder='drop',
)

# ----- Full pipeline -----

full = Pipeline(steps=[
	('pre', pre),
	('scaler', StandardScaler()),
])
```
**á»¨ng Dá»¥ng StandardScaler**
```
num__LowQualFinSF | mean = 0.000, std = 1.000
num__3SsnPorch | mean = 0.000, std = 1.000
num__PoolArea | mean = 0.000, std = 1.000
num__OverallQual_log | mean = -0.000, std = 1.000
```
**Táº¥t cáº£ 172 features Ä‘Æ°á»£c scale thÃ nh:** mean â‰ˆ 0, std â‰ˆ 1

**Target khÃ´ng Ä‘Æ°á»£c scale:**
- **SalePrice:** mean = 12.024, std = 0.397 âœ… **KHÃ”NG Ä‘Æ°á»£c scale!**
- **LÃ½ do:**
	1. âœ… `log1p` Ä‘Ã£ xá»­ lÃ½ skewness
	2. âœ… Giá»¯ thang log â†’ dá»… Ä‘áº£o ngÆ°á»£c (`expm1`)
	3. âœ… Scale target lÃ  tÃ¹y chá»n cho linear models
	4. âœ… Kháº£ nÄƒng giáº£i thÃ­ch: log(price) dá»… hÆ¡n cÃ¡c Ä‘Æ¡n vá»‹ chuáº©n hÃ³a

![Scaled Feature Example](images/Pasted%20image%2020251028173458.png)
## 6. Outlier Analysis

**PhÃ¢n tÃ­ch:** PhÃ¡t hiá»‡n outliers toÃ n diá»‡n sau transformation
**Quyáº¿t Ä‘á»‹nh:** âœ… GIá»® Táº¤T Cáº¢ OUTLIERS
#### Outliers trong Target Variable (SalePrice)

![Outlier Target Variable](images/Pasted%20image%2020251028175257.png)
```
â”œâ”€ Skewness: 0.205 (Äá»™ lá»‡ch nhá» - phÃ¢n phá»‘i gáº§n Ä‘á»‘i xá»©ng âœ”ï¸)
â”œâ”€ Sá»‘ máº«u náº±m ngoÃ i khoáº£ng IQR (Interquartile Range): 56 (chiáº¿m khoáº£ng 4.5%)
â”œâ”€ Sá»‘ máº«u cÃ³ z-score > 3: 21 (chiáº¿m 1.7%)
â””â”€ Nháº­n Ä‘á»‹nh: CÃ¡c outlier nÃ y pháº£n Ã¡nh sá»± Ä‘a dáº¡ng tá»± nhiÃªn cá»§a giÃ¡ nhÃ , khÃ´ng pháº£i lÃ  lá»—i hoáº·c báº¥t thÆ°á»ng cáº§n loáº¡i bá». Do váº­y, giá»¯ nguyÃªn táº¥t cáº£.
```
#### Outliers trong cÃ¡c Feature

![Outlier Features](images/Pasted%20image%2020251028180305.png)
```
â”œâ”€ CÃ¡c biáº¿n cá» nhá»‹ phÃ¢n (binary flags): 0% outliers (7 feature) â€“ khÃ´ng cÃ³ giÃ¡ trá»‹ báº¥t thÆ°á»ng
â”œâ”€ 24 feature khÃ´ng cÃ³ outlier
â”œâ”€ 12 feature cÃ³ ráº¥t Ã­t outlier (0-5%) âœ”ï¸ â€“ cháº¥p nháº­n Ä‘Æ°á»£c
â”œâ”€ 5 feature cÃ³ lÆ°á»£ng outlier vá»«a pháº£i (5-10%) âš  â€“ cáº§n lÆ°u Ã½ nhÆ°ng há»£p lÃ½
â””â”€ 2 feature cÃ³ tá»· lá»‡ outlier cao (>10%) âœ”ï¸ â€“ cÃ³ lÃ½ do giáº£i thÃ­ch rÃµ rÃ ng
```
#### Giáº£i thÃ­ch cá»¥ thá»ƒ vá»›i cÃ¡c feature cÃ³ nhiá»u outlier:

- **MasVnrAreaResid** (17.6%): Pháº§n dÆ° (sai sá»‘) giá»¯a diá»‡n tÃ­ch á»‘p tÆ°á»ng Ä‘Ã¡ thá»±c táº¿ vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n theo cÃ¡c tiÃªu chÃ­ cÃ²n láº¡i â€“ giÃ¡ trá»‹ lá»›n báº¥t thÆ°á»ng thá»ƒ hiá»‡n cÃ¡c cÄƒn nhÃ  cÃ³ pháº§n á»‘p tÆ°á»ng khÃ¡c biá»‡t háº³n so vá»›i xu hÆ°á»›ng chung.
- **BasementResid** (17.1%): Pháº§n dÆ° (sai sá»‘) liÃªn quan Ä‘áº¿n diá»‡n tÃ­ch háº§m (basement) â€“ outlier nghÄ©a lÃ  nhÃ  Ä‘Ã³ cÃ³ háº§m lá»›n/nhá» báº¥t thÆ°á»ng so vá»›i cÃ¡c Ä‘áº·c Ä‘iá»ƒm khÃ¡c.
- **GarageAreaPerCar** (9.3%): Diá»‡n tÃ­ch gara chia cho sá»‘ chá»— Ä‘á»ƒ xe â€“ outlier xuáº¥t hiá»‡n khi 1 chá»— nhÆ°ng gara láº¡i ráº¥t rá»™ng (ráº¥t â€œthá»«aâ€, thiáº¿t káº¿ láº¡) hoáº·c ngÆ°á»£c láº¡i.
- **OverallCond** (8.4%): Äiá»ƒm Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ vá» Ä‘iá»u kiá»‡n cÄƒn nhÃ  (thang báº­c 1-9) â€“ cÃ¡c Ä‘iá»ƒm cá»±c ká»³ cao hoáº·c tháº¥p thÆ°á»ng lÃ  outlier, pháº£n Ã¡nh báº¥t thÆ°á»ng vá» cháº¥t lÆ°á»£ng.
- **LotFrontage** (8.0%): Chiá»u rá»™ng máº·t tiá»n Ä‘áº¥t â€“ nhá»¯ng lÃ´ cÃ³ máº·t tiá»n ráº¥t rá»™ng (nhÃ  gÃ³c, biá»‡t thá»±) hoáº·c cá»±c háº¹p sáº½ bá»‹ xem lÃ  outlier.
- **MSSubClass** (7.1%): PhÃ¢n loáº¡i kiá»ƒu nhÃ  theo mÃ£ sá»‘ xÃ¢y dá»±ng â€“ má»™t sá»‘ mÃ£ Ã­t xuáº¥t hiá»‡n cÃ³ thá»ƒ táº¡o thÃ nh outlier do hiáº¿m tháº¥y trÃªn thá»‹ trÆ°á»ng.
#### Táº¡i sao giá»¯ láº¡i toÃ n bá»™ outlier?

- CÃ¡c thuáº­t toÃ¡n regularization nhÆ° Ridge/Lasso Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£m áº£nh hÆ°á»Ÿng tiÃªu cá»±c cá»§a outlier lÃªn model. L2 penalty (Ridge) sáº½ thu nhá» tÃ¡c Ä‘á»™ng cÃ¡c Ä‘iá»ƒm báº¥t thÆ°á»ng má»™t cÃ¡ch má»m dáº»o, L1 penalty (Lasso) tháº­m chÃ­ cÃ³ thá»ƒ loáº¡i bá» hoÃ n toÃ n feature nhiá»…u náº¿u cáº§n.
- Náº¿u loáº¡i bá» cÃ¡c outlier nÃ y, ta sáº½ máº¥t má»™t pháº§n thÃ´ng tin thá»±c táº¿ liÃªn quan tá»›i sá»± Ä‘a dáº¡ng hoáº·c trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a thá»‹ trÆ°á»ng nhÃ  Ä‘áº¥t.
- Sá»‘ lÆ°á»£ng sample lÃ  1239 â€“ náº¿u giá»¯ láº¡i táº¥t cáº£ sáº½ táº­n dá»¥ng tá»‘i Ä‘a dá»¯ liá»‡u.
- QuÃ¡ trÃ¬nh Cross-validation sáº½ tá»± Ä‘á»™ng chá»n tham sá»‘ Î± (há»‡ sá»‘ Ä‘iá»u chá»‰nh má»©c Ä‘á»™ regularization) sao cho phÃ¹ há»£p nháº¥t vá»›i cáº¥u trÃºc dá»¯ liá»‡u thá»±c, ká»ƒ cáº£ khi tá»“n táº¡i outlier.

