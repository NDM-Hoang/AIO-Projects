{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ FPT Stock Prediction with Market Regime Clustering\n",
    "\n",
    "## Approach\n",
    "1. **HMM Clustering (60-day window)**: Ph√°t hi·ªán 4 market regimes\n",
    "2. **Regime-specific MultiDLinear**: Train model ri√™ng cho m·ªói regime\n",
    "3. **Ensemble Prediction**: K·∫øt h·ª£p predictions theo regime hi·ªán t·∫°i\n",
    "4. **Forecast 100 days**: T·∫°o submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn import hmm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/FPT_train.csv')\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['time'].min()} ‚Üí {df['time'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for model\n",
    "df['close_log'] = np.log(df['close'])\n",
    "df['open_log'] = np.log(df['open'])\n",
    "df['high_log'] = np.log(df['high'])\n",
    "df['low_log'] = np.log(df['low'])\n",
    "df['volume_log'] = np.log(df['volume'] + 1)\n",
    "df['hl_spread'] = (df['high'] - df['low']) / df['close']\n",
    "df['oc_spread'] = (df['close'] - df['open']) / df['open']\n",
    "df = df.ffill().bfill()\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM Market Regime Detection (60-day window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regime features with 60-day rolling window\n",
    "REGIME_WINDOW = 60\n",
    "\n",
    "# Return trong window\n",
    "df['return_60d'] = df['close'].pct_change(REGIME_WINDOW) * 100\n",
    "\n",
    "# Volatility (annualized)\n",
    "df['daily_ret'] = df['close'].pct_change()\n",
    "df['volatility_60d'] = df['daily_ret'].rolling(REGIME_WINDOW).std() * np.sqrt(252) * 100\n",
    "\n",
    "# Trend strength\n",
    "def calc_trend(series):\n",
    "    x = np.arange(len(series))\n",
    "    slope = np.polyfit(x, series, 1)[0]\n",
    "    return slope / series.mean() * 100\n",
    "\n",
    "df['trend_60d'] = df['close'].rolling(REGIME_WINDOW).apply(calc_trend, raw=False)\n",
    "\n",
    "# Drop NaN rows for regime detection\n",
    "df_regime = df.dropna(subset=['return_60d', 'volatility_60d', 'trend_60d']).copy()\n",
    "print(f\"Data for regime detection: {len(df_regime)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit HMM\n",
    "N_REGIMES = 4\n",
    "\n",
    "regime_features = ['return_60d', 'volatility_60d', 'trend_60d']\n",
    "X_regime = df_regime[regime_features].values\n",
    "\n",
    "# Scale features\n",
    "regime_scaler = StandardScaler()\n",
    "X_regime_scaled = regime_scaler.fit_transform(X_regime)\n",
    "\n",
    "# Fit HMM\n",
    "hmm_model = hmm.GaussianHMM(\n",
    "    n_components=N_REGIMES,\n",
    "    covariance_type=\"full\",\n",
    "    n_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "hmm_model.fit(X_regime_scaled)\n",
    "\n",
    "# Predict regimes\n",
    "df_regime['regime'] = hmm_model.predict(X_regime_scaled)\n",
    "\n",
    "print(\"HMM fitted successfully!\")\n",
    "print(f\"Transition matrix:\\n{hmm_model.transmat_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign meaningful names to regimes\n",
    "regime_stats = df_regime.groupby('regime').agg({\n",
    "    'return_60d': 'mean',\n",
    "    'volatility_60d': 'mean'\n",
    "})\n",
    "\n",
    "# Sort by return\n",
    "sorted_regimes = regime_stats['return_60d'].sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create mapping\n",
    "REGIME_NAMES = {}\n",
    "REGIME_COLORS = {}\n",
    "regime_templates = [\n",
    "    ('Rally', '#1B5E20'),\n",
    "    ('Uptrend', '#4CAF50'),\n",
    "    ('Sideway', '#9E9E9E'),\n",
    "    ('Downtrend', '#C62828')\n",
    "]\n",
    "\n",
    "for i, regime in enumerate(sorted_regimes):\n",
    "    REGIME_NAMES[regime] = regime_templates[i][0]\n",
    "    REGIME_COLORS[regime] = regime_templates[i][1]\n",
    "\n",
    "df_regime['regime_name'] = df_regime['regime'].map(REGIME_NAMES)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGIME SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for regime in sorted_regimes:\n",
    "    name = REGIME_NAMES[regime]\n",
    "    count = (df_regime['regime'] == regime).sum()\n",
    "    pct = count / len(df_regime) * 100\n",
    "    ret = regime_stats.loc[regime, 'return_60d']\n",
    "    vol = regime_stats.loc[regime, 'volatility_60d']\n",
    "    print(f\"\\n{name} (Regime {regime}):\")\n",
    "    print(f\"  Count: {count} days ({pct:.1f}%)\")\n",
    "    print(f\"  Return 60d: {ret:+.1f}%\")\n",
    "    print(f\"  Volatility: {vol:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regimes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Price with regime colors\n",
    "ax = axes[0]\n",
    "ax.plot(df_regime['time'], df_regime['close'], 'k-', linewidth=1)\n",
    "\n",
    "# Add regime background\n",
    "prev_regime = None\n",
    "start_idx = 0\n",
    "for i in range(len(df_regime)):\n",
    "    current = df_regime.iloc[i]['regime']\n",
    "    if current != prev_regime:\n",
    "        if prev_regime is not None:\n",
    "            ax.axvspan(df_regime.iloc[start_idx]['time'],\n",
    "                      df_regime.iloc[i-1]['time'],\n",
    "                      alpha=0.3, color=REGIME_COLORS[prev_regime])\n",
    "        start_idx = i\n",
    "        prev_regime = current\n",
    "ax.axvspan(df_regime.iloc[start_idx]['time'],\n",
    "          df_regime.iloc[-1]['time'],\n",
    "          alpha=0.3, color=REGIME_COLORS[prev_regime])\n",
    "\n",
    "ax.set_ylabel('Price (VND)')\n",
    "ax.set_title('FPT Price with Market Regimes (60-day HMM)', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "import matplotlib.patches as mpatches\n",
    "legend_elements = [mpatches.Patch(facecolor=REGIME_COLORS[r], alpha=0.5,\n",
    "                                   label=f\"{REGIME_NAMES[r]} ({regime_stats.loc[r, 'return_60d']:+.1f}%)\")\n",
    "                   for r in sorted_regimes]\n",
    "ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "# Regime distribution over time\n",
    "ax = axes[1]\n",
    "colors = [REGIME_COLORS[r] for r in df_regime['regime']]\n",
    "ax.scatter(df_regime['time'], df_regime['regime'], c=colors, s=10, alpha=0.5)\n",
    "ax.set_yticks(sorted_regimes)\n",
    "ax.set_yticklabels([REGIME_NAMES[r] for r in sorted_regimes])\n",
    "ax.set_ylabel('Regime')\n",
    "ax.set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge regime back to main df\n",
    "# For rows without regime (first 60 days), use the first available regime\n",
    "df['regime'] = np.nan\n",
    "df.loc[df_regime.index, 'regime'] = df_regime['regime'].values\n",
    "df['regime'] = df['regime'].ffill().bfill().astype(int)\n",
    "\n",
    "print(f\"Regime distribution in full dataset:\")\n",
    "print(df['regime'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "train_cutoff = int(len(df) * 0.7)\n",
    "\n",
    "# Univariate scaler for target\n",
    "uni_scaler = StandardScaler()\n",
    "uni_scaler.fit(df['close_log'].values[:train_cutoff].reshape(-1, 1))\n",
    "df['close_scaled'] = uni_scaler.transform(df['close_log'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Multivariate scaler for features\n",
    "multi_cols = ['open_log', 'high_log', 'low_log', 'close_log', 'volume_log', 'hl_spread', 'oc_spread']\n",
    "multi_scaler = StandardScaler()\n",
    "multi_scaler.fit(df[multi_cols].values[:train_cutoff])\n",
    "multi_scaled = multi_scaler.transform(df[multi_cols].values)\n",
    "\n",
    "n_features = len(multi_cols)\n",
    "close_idx = 3  # Index of close_log in multi_cols\n",
    "\n",
    "print(f\"Train cutoff: {train_cutoff}\")\n",
    "print(f\"N features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset & Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateDataset(Dataset):\n",
    "    \"\"\"Dataset cho multivariate input v·ªõi regime filtering\"\"\"\n",
    "    def __init__(self, features, target, regimes, seq_len, pred_len, filter_regime=None):\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.target = target.astype(np.float32)\n",
    "        self.regimes = regimes\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Create valid indices\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(features) - seq_len - pred_len + 1):\n",
    "            # Get regime of prediction period (last day of input)\n",
    "            regime = regimes[i + seq_len - 1]\n",
    "            if filter_regime is None or regime == filter_regime:\n",
    "                self.valid_indices.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.valid_indices[idx]\n",
    "        x = self.features[i: i + self.seq_len]\n",
    "        y = self.target[i + self.seq_len: i + self.seq_len + self.pred_len]\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDLinear(nn.Module):\n",
    "    \"\"\"Multivariate DLinear with trend-seasonal decomposition\"\"\"\n",
    "    def __init__(self, seq_len, pred_len, n_features, close_idx=3):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.close_idx = close_idx\n",
    "        self.kernel_size = max(3, seq_len // 4)\n",
    "        self.fc_trend = nn.Linear(seq_len * n_features, pred_len)\n",
    "        self.fc_seasonal = nn.Linear(seq_len * n_features, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, n_features)\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Decompose close feature\n",
    "        close_seq = x[:, :, self.close_idx]  # (B, seq_len)\n",
    "        trend = close_seq.unfold(-1, self.kernel_size, 1).mean(-1)\n",
    "        pad_left = (self.seq_len - trend.size(-1)) // 2\n",
    "        pad_right = self.seq_len - trend.size(-1) - pad_left\n",
    "        trend = F.pad(trend, (pad_left, pad_right), mode='replicate')\n",
    "        \n",
    "        # Create trend/seasonal versions\n",
    "        seasonal = close_seq - trend\n",
    "        x_trend = x.clone()\n",
    "        x_seasonal = x.clone()\n",
    "        x_trend[:, :, self.close_idx] = trend\n",
    "        x_seasonal[:, :, self.close_idx] = seasonal\n",
    "        \n",
    "        return self.fc_trend(x_trend.reshape(B, -1)) + self.fc_seasonal(x_seasonal.reshape(B, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=80):\n",
    "        self.patience = patience\n",
    "        self.best_val = float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_val - 1e-5:\n",
    "            self.best_val = val_loss\n",
    "            self.wait = 0\n",
    "            self.best_state = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, patience, device, verbose=True):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=30)\n",
    "    stopper = EarlyStopping(patience)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(bx)\n",
    "            loss = criterion(pred, by)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for bx, by in val_loader:\n",
    "                bx, by = bx.to(device), by.to(device)\n",
    "                val_loss += criterion(model(bx), by).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        if stopper.step(val_loss, model):\n",
    "            if verbose:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if stopper.best_state:\n",
    "        model.load_state_dict(stopper.best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Regime-Specific Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEQ_LEN = 7  # Input sequence length\n",
    "PRED_LEN = 100  # Prediction horizon\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 500\n",
    "PATIENCE = 80\n",
    "LR = 1e-3\n",
    "# TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "# Get regimes array\n",
    "regimes = df['regime'].values\n",
    "target = multi_scaled[:, close_idx]  # close_log scaled\n",
    "\n",
    "print(f\"SEQ_LEN: {SEQ_LEN}\")\n",
    "print(f\"PRED_LEN: {PRED_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model for each regime\n",
    "regime_models = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING REGIME-SPECIFIC MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for regime in sorted_regimes:\n",
    "    name = REGIME_NAMES[regime]\n",
    "    print(f\"\\n‚ñ∂ Training model for {name} (Regime {regime})...\")\n",
    "    \n",
    "    # Create dataset filtered by regime\n",
    "    dataset = MultivariateDataset(\n",
    "        multi_scaled, target, regimes, \n",
    "        SEQ_LEN, PRED_LEN, filter_regime=regime\n",
    "    )\n",
    "    \n",
    "    if len(dataset) < 10:\n",
    "        print(f\"    ‚ö†Ô∏è Not enough samples ({len(dataset)}), skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Split\n",
    "    total = len(dataset)\n",
    "    val_len = int(total * VAL_RATIO)\n",
    "    train_len = total - val_len \n",
    "    \n",
    "    train_subset = torch.utils.data.Subset(dataset, list(range(train_len)))\n",
    "    val_subset = torch.utils.data.Subset(dataset, list(range(train_len, train_len + val_len)))\n",
    "    \n",
    "    if len(val_subset) == 0:\n",
    "        print(f\"    ‚ö†Ô∏è Not enough validation samples, using last 20%...\")\n",
    "        train_len = int(total * 0.8)\n",
    "        train_subset = torch.utils.data.Subset(dataset, list(range(train_len)))\n",
    "        val_subset = torch.utils.data.Subset(dataset, list(range(train_len, total)))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"    Samples: {len(dataset)} (train={len(train_subset)}, val={len(val_subset)})\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = MultiDLinear(SEQ_LEN, PRED_LEN, n_features, close_idx)\n",
    "    model = train_model(model, train_loader, val_loader, NUM_EPOCHS, LR, PATIENCE, device)\n",
    "    \n",
    "    regime_models[regime] = model\n",
    "    print(f\"    ‚úÖ Model trained!\")\n",
    "\n",
    "print(f\"\\nTotal models trained: {len(regime_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also train a global model (all data)\n",
    "print(\"\\n‚ñ∂ Training GLOBAL model (all regimes)...\")\n",
    "\n",
    "dataset_all = MultivariateDataset(\n",
    "    multi_scaled, target, regimes,\n",
    "    SEQ_LEN, PRED_LEN, filter_regime=None\n",
    ")\n",
    "\n",
    "total = len(dataset_all)\n",
    "train_len = int(total * 0.9)\n",
    "train_subset = torch.utils.data.Subset(dataset_all, list(range(train_len)))\n",
    "val_subset = torch.utils.data.Subset(dataset_all, list(range(train_len, total)))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"    Samples: {len(dataset_all)} (train={len(train_subset)}, val={len(val_subset)})\")\n",
    "\n",
    "global_model = MultiDLinear(SEQ_LEN, PRED_LEN, n_features, close_idx)\n",
    "global_model = train_model(global_model, train_loader, val_loader, NUM_EPOCHS, LR, PATIENCE, device)\n",
    "print(\"    ‚úÖ Global model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, input_data, device):\n",
    "    \"\"\"Generate prediction from input data\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(input_data.astype(np.float32)).unsqueeze(0).to(device)\n",
    "        pred = model(x).cpu().numpy().flatten()\n",
    "    return pred\n",
    "\n",
    "\n",
    "def inverse_transform_prediction(pred_scaled, scaler):\n",
    "    \"\"\"Convert scaled prediction back to price\"\"\"\n",
    "    pred_log = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "    pred_price = np.exp(pred_log)\n",
    "    return pred_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current regime (last day)\n",
    "current_regime = df['regime'].iloc[-1]\n",
    "current_regime_name = REGIME_NAMES[current_regime]\n",
    "\n",
    "print(f\"Current market regime: {current_regime_name} (Regime {current_regime})\")\n",
    "\n",
    "# Get last SEQ_LEN data points\n",
    "input_data = multi_scaled[-SEQ_LEN:].copy()\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Use regime-specific model\n",
    "if current_regime in regime_models:\n",
    "    pred_regime = predict_with_model(regime_models[current_regime], input_data, device)\n",
    "    prices_regime = inverse_transform_prediction(pred_regime, uni_scaler)\n",
    "    print(f\"\\nRegime-specific prediction ({current_regime_name}):\")\n",
    "    print(f\"  Range: {prices_regime.min():.2f} - {prices_regime.max():.2f}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No specific model for {current_regime_name}, using global model\")\n",
    "    prices_regime = None\n",
    "\n",
    "# Strategy 2: Use global model\n",
    "pred_global = predict_with_model(global_model, input_data, device)\n",
    "prices_global = inverse_transform_prediction(pred_global, uni_scaler)\n",
    "print(f\"\\nGlobal model prediction:\")\n",
    "print(f\"  Range: {prices_global.min():.2f} - {prices_global.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Weighted ensemble based on regime confidence\n",
    "# Get regime probabilities for current state\n",
    "current_features = df_regime[regime_features].iloc[-1:].values\n",
    "current_features_scaled = regime_scaler.transform(current_features)\n",
    "regime_probs = hmm_model.predict_proba(current_features_scaled)[0]\n",
    "\n",
    "print(\"\\nRegime probabilities:\")\n",
    "for r in sorted_regimes:\n",
    "    print(f\"  {REGIME_NAMES[r]}: {regime_probs[r]*100:.1f}%\")\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_pred = np.zeros(PRED_LEN)\n",
    "total_weight = 0\n",
    "\n",
    "for regime, model in regime_models.items():\n",
    "    weight = regime_probs[regime]\n",
    "    pred = predict_with_model(model, input_data, device)\n",
    "    ensemble_pred += weight * pred\n",
    "    total_weight += weight\n",
    "\n",
    "# Add global model with remaining weight\n",
    "global_weight = max(0, 1 - total_weight)\n",
    "if global_weight > 0:\n",
    "    ensemble_pred += global_weight * pred_global\n",
    "\n",
    "prices_ensemble = inverse_transform_prediction(ensemble_pred, uni_scaler)\n",
    "print(f\"\\nEnsemble prediction (weighted by regime probability):\")\n",
    "print(f\"  Range: {prices_ensemble.min():.2f} - {prices_ensemble.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose final prediction\n",
    "# Use ensemble as primary, fallback to global if needed\n",
    "final_prices = prices_ensemble\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Method: Weighted Ensemble\")\n",
    "print(f\"Range: {final_prices.min():.2f} - {final_prices.max():.2f} VND\")\n",
    "print(f\"First 5 days: {final_prices[:5]}\")\n",
    "print(f\"Last 5 days: {final_prices[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Submission & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "os.makedirs('submissions', exist_ok=True)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': np.arange(1, PRED_LEN + 1),\n",
    "    'close': final_prices\n",
    "})\n",
    "\n",
    "submission_path = f'submissions/submission_hmm_regime_multidlinear_{SEQ_LEN}d.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "print(f\"\\nPreview:\")\n",
    "display(submission.head(10))\n",
    "display(submission.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Historical + Forecast\n",
    "ax = axes[0]\n",
    "hist_days = 200\n",
    "hist_dates = df['time'].iloc[-hist_days:]\n",
    "hist_prices = df['close'].iloc[-hist_days:]\n",
    "\n",
    "last_date = df['time'].iloc[-1]\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=PRED_LEN, freq='B')\n",
    "\n",
    "ax.plot(hist_dates, hist_prices, 'b-', linewidth=2, label='Historical')\n",
    "ax.plot(future_dates, final_prices, 'r--', linewidth=2, label='Forecast (Ensemble)')\n",
    "\n",
    "# Also show individual predictions\n",
    "if prices_regime is not None:\n",
    "    ax.plot(future_dates, prices_regime, 'g:', linewidth=1, alpha=0.7, \n",
    "            label=f'Regime-specific ({current_regime_name})')\n",
    "ax.plot(future_dates, prices_global, 'orange', linestyle=':', linewidth=1, alpha=0.7,\n",
    "        label='Global model')\n",
    "\n",
    "ax.axvline(last_date, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Close Price (VND)')\n",
    "ax.set_title(f'FPT Stock Price Forecast - HMM Regime Clustering + MultiDLinear', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Comparison of methods\n",
    "ax = axes[1]\n",
    "ax.plot(range(1, PRED_LEN+1), final_prices, 'r-', linewidth=2, label='Ensemble (Final)')\n",
    "if prices_regime is not None:\n",
    "    ax.plot(range(1, PRED_LEN+1), prices_regime, 'g--', linewidth=1.5, \n",
    "            label=f'Regime: {current_regime_name}')\n",
    "ax.plot(range(1, PRED_LEN+1), prices_global, 'b--', linewidth=1.5, label='Global')\n",
    "\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Predicted Price (VND)')\n",
    "ax.set_title('Comparison of Prediction Methods', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('submissions/forecast_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved to: submissions/forecast_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüîç Market Regime Detection:\")\n",
    "print(f\"   Method: HMM with 60-day window\")\n",
    "print(f\"   Regimes: {N_REGIMES}\")\n",
    "for r in sorted_regimes:\n",
    "    count = (df['regime'] == r).sum()\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   - {REGIME_NAMES[r]}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìà Current State:\")\n",
    "print(f\"   Regime: {current_regime_name}\")\n",
    "print(f\"   Last Price: {df['close'].iloc[-1]:.2f} VND\")\n",
    "\n",
    "print(f\"\\nüéØ Prediction:\")\n",
    "print(f\"   Method: Weighted Ensemble\")\n",
    "print(f\"   Horizon: {PRED_LEN} days\")\n",
    "print(f\"   Price Range: {final_prices.min():.2f} - {final_prices.max():.2f} VND\")\n",
    "pct_change = (final_prices[-1] / df['close'].iloc[-1] - 1) * 100\n",
    "print(f\"   Expected Change: {pct_change:+.1f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ Output:\")\n",
    "print(f\"   Submission: {submission_path}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
