{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPT Stock Prediction: HMM Regime + MultiDLinear\n",
    "\n",
    "**Pipeline:**\n",
    "1. HMM Clustering (60-day window) → 4 regimes\n",
    "2. Train MultiDLinear per regime\n",
    "3. Ensemble prediction → 100 days forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, random, warnings\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn import hmm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "CONFIG = {\n",
    "    'data_path': 'data/FPT_train.csv',\n",
    "    'regime_window': 60,\n",
    "    'n_regimes': 4,\n",
    "    'seq_len': 30,\n",
    "    'pred_len': 100,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 500,\n",
    "    'patience': 80,\n",
    "    'lr': 1e-3,\n",
    "    'train_ratio': 0.7\n",
    "}\n",
    "print(\"Config:\", CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(CONFIG['data_path'])\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# Features\n",
    "df['close_log'] = np.log(df['close'])\n",
    "df['open_log'] = np.log(df['open'])\n",
    "df['high_log'] = np.log(df['high'])\n",
    "df['low_log'] = np.log(df['low'])\n",
    "df['volume_log'] = np.log(df['volume'] + 1)\n",
    "df['hl_spread'] = (df['high'] - df['low']) / df['close']\n",
    "df['oc_spread'] = (df['close'] - df['open']) / df['open']\n",
    "\n",
    "print(f\"Data: {len(df)} rows, {df['time'].min().date()} → {df['time'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HMM Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regime features\n",
    "W = CONFIG['regime_window']\n",
    "\n",
    "df['return_W'] = df['close'].pct_change(W) * 100\n",
    "df['vol_W'] = df['close'].pct_change().rolling(W).std() * np.sqrt(252) * 100\n",
    "df['trend_W'] = df['close'].rolling(W).apply(\n",
    "    lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] / x.mean() * 100, raw=False\n",
    ")\n",
    "\n",
    "# HMM\n",
    "df_hmm = df.dropna(subset=['return_W', 'vol_W', 'trend_W']).copy()\n",
    "X_hmm = df_hmm[['return_W', 'vol_W', 'trend_W']].values\n",
    "\n",
    "hmm_scaler = StandardScaler()\n",
    "X_hmm_scaled = hmm_scaler.fit_transform(X_hmm)\n",
    "\n",
    "model_hmm = hmm.GaussianHMM(n_components=CONFIG['n_regimes'], covariance_type='full', \n",
    "                            n_iter=1000, random_state=42)\n",
    "model_hmm.fit(X_hmm_scaled)\n",
    "df_hmm['regime'] = model_hmm.predict(X_hmm_scaled)\n",
    "\n",
    "# Name regimes by return\n",
    "stats = df_hmm.groupby('regime')['return_W'].mean().sort_values(ascending=False)\n",
    "REGIME_MAP = {r: i for i, r in enumerate(stats.index)}\n",
    "REGIME_NAMES = ['Rally', 'Uptrend', 'Sideway', 'Downtrend']\n",
    "REGIME_COLORS = ['#1B5E20', '#4CAF50', '#9E9E9E', '#C62828']\n",
    "\n",
    "df_hmm['regime_id'] = df_hmm['regime'].map(REGIME_MAP)\n",
    "df_hmm['regime_name'] = df_hmm['regime_id'].map(lambda x: REGIME_NAMES[x])\n",
    "\n",
    "# Merge back\n",
    "df['regime'] = np.nan\n",
    "df.loc[df_hmm.index, 'regime'] = df_hmm['regime'].values\n",
    "df['regime'] = df['regime'].ffill().bfill().astype(int)\n",
    "df['regime_id'] = df['regime'].map(REGIME_MAP)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nRegime Summary:\")\n",
    "for rid in range(CONFIG['n_regimes']):\n",
    "    orig = [k for k, v in REGIME_MAP.items() if v == rid][0]\n",
    "    ret = stats.loc[orig]\n",
    "    cnt = (df_hmm['regime'] == orig).sum()\n",
    "    print(f\"  {REGIME_NAMES[rid]}: {cnt} days ({cnt/len(df_hmm)*100:.1f}%), Return: {ret:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.plot(df['time'], df['close'], 'k-', lw=1)\n",
    "\n",
    "prev, start = None, 0\n",
    "for i in range(len(df)):\n",
    "    curr = df.iloc[i]['regime_id']\n",
    "    if curr != prev:\n",
    "        if prev is not None:\n",
    "            ax.axvspan(df.iloc[start]['time'], df.iloc[i-1]['time'], \n",
    "                      alpha=0.3, color=REGIME_COLORS[int(prev)])\n",
    "        start, prev = i, curr\n",
    "ax.axvspan(df.iloc[start]['time'], df.iloc[-1]['time'], alpha=0.3, color=REGIME_COLORS[int(prev)])\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "ax.legend(handles=[mpatches.Patch(color=c, alpha=0.5, label=n) \n",
    "                   for n, c in zip(REGIME_NAMES, REGIME_COLORS)], loc='upper left')\n",
    "ax.set_title('FPT Price with HMM Regimes (60d)', fontweight='bold')\n",
    "ax.set_ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "train_cut = int(len(df) * CONFIG['train_ratio'])\n",
    "\n",
    "# Target scaler\n",
    "target_scaler = StandardScaler()\n",
    "target_scaler.fit(df['close_log'].values[:train_cut].reshape(-1, 1))\n",
    "\n",
    "# Feature scaler\n",
    "FEAT_COLS = ['open_log', 'high_log', 'low_log', 'close_log', 'volume_log', 'hl_spread', 'oc_spread']\n",
    "feat_scaler = StandardScaler()\n",
    "feat_scaler.fit(df[FEAT_COLS].values[:train_cut])\n",
    "X_scaled = feat_scaler.transform(df[FEAT_COLS].values)\n",
    "\n",
    "n_features = len(FEAT_COLS)\n",
    "close_idx = 3\n",
    "target = X_scaled[:, close_idx]\n",
    "regimes = df['regime'].values\n",
    "\n",
    "print(f\"Features: {n_features}, Train cutoff: {train_cut}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class RegimeDataset(Dataset):\n",
    "    def __init__(self, X, y, regimes, seq_len, pred_len, regime_filter=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len, self.pred_len = seq_len, pred_len\n",
    "        \n",
    "        self.indices = [i for i in range(len(X) - seq_len - pred_len + 1)\n",
    "                        if regime_filter is None or regimes[i + seq_len - 1] == regime_filter]\n",
    "    \n",
    "    def __len__(self): return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        return (torch.from_numpy(self.X[i:i+self.seq_len]),\n",
    "                torch.from_numpy(self.y[i+self.seq_len:i+self.seq_len+self.pred_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiDLinear Model\n",
    "class MultiDLinear(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, n_feat, close_idx=3):\n",
    "        super().__init__()\n",
    "        self.seq_len, self.close_idx = seq_len, close_idx\n",
    "        self.kernel = max(3, seq_len // 4)\n",
    "        self.fc_t = nn.Linear(seq_len * n_feat, pred_len)\n",
    "        self.fc_s = nn.Linear(seq_len * n_feat, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        c = x[:, :, self.close_idx]\n",
    "        t = c.unfold(-1, self.kernel, 1).mean(-1)\n",
    "        pad_l = (self.seq_len - t.size(-1)) // 2\n",
    "        pad_r = self.seq_len - t.size(-1) - pad_l\n",
    "        t = F.pad(t, (pad_l, pad_r), mode='replicate')\n",
    "        s = c - t\n",
    "        \n",
    "        xt, xs = x.clone(), x.clone()\n",
    "        xt[:, :, self.close_idx], xs[:, :, self.close_idx] = t, s\n",
    "        return self.fc_t(xt.reshape(B, -1)) + self.fc_s(xs.reshape(B, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.patience, self.best, self.wait, self.state = patience, float('inf'), 0, None\n",
    "    def step(self, loss, model):\n",
    "        if loss < self.best - 1e-5:\n",
    "            self.best, self.wait, self.state = loss, 0, deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "def train(model, train_dl, val_dl, epochs, lr, patience):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=30)\n",
    "    stopper = EarlyStopping(patience)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for bx, by in train_dl:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = sum(criterion(model(bx.to(device)), by.to(device)).item() \n",
    "                       for bx, by in val_dl) / len(val_dl)\n",
    "        sched.step(val_loss)\n",
    "        if stopper.step(val_loss, model): break\n",
    "    \n",
    "    if stopper.state: model.load_state_dict(stopper.state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Regime-Specific Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train per regime\n",
    "models = {}\n",
    "unique_regimes = sorted(df['regime'].unique())\n",
    "\n",
    "print(\"Training regime-specific models...\\n\")\n",
    "for regime in unique_regimes:\n",
    "    rid = REGIME_MAP[regime]\n",
    "    name = REGIME_NAMES[rid]\n",
    "    \n",
    "    ds = RegimeDataset(X_scaled, target, regimes, CONFIG['seq_len'], CONFIG['pred_len'], regime)\n",
    "    if len(ds) < 20:\n",
    "        print(f\"{name}: Skip (only {len(ds)} samples)\")\n",
    "        continue\n",
    "    \n",
    "    n = len(ds)\n",
    "    tr_n = int(n * 0.8)\n",
    "    tr_dl = DataLoader(torch.utils.data.Subset(ds, range(tr_n)), CONFIG['batch_size'], shuffle=True)\n",
    "    va_dl = DataLoader(torch.utils.data.Subset(ds, range(tr_n, n)), CONFIG['batch_size'])\n",
    "    \n",
    "    m = MultiDLinear(CONFIG['seq_len'], CONFIG['pred_len'], n_features, close_idx)\n",
    "    m = train(m, tr_dl, va_dl, CONFIG['epochs'], CONFIG['lr'], CONFIG['patience'])\n",
    "    models[regime] = m\n",
    "    print(f\"{name}: Trained ({n} samples)\")\n",
    "\n",
    "# Global model\n",
    "print(\"\\nTraining global model...\")\n",
    "ds_all = RegimeDataset(X_scaled, target, regimes, CONFIG['seq_len'], CONFIG['pred_len'])\n",
    "n = len(ds_all)\n",
    "tr_n = int(n * 0.9)\n",
    "tr_dl = DataLoader(torch.utils.data.Subset(ds_all, range(tr_n)), CONFIG['batch_size'], shuffle=True)\n",
    "va_dl = DataLoader(torch.utils.data.Subset(ds_all, range(tr_n, n)), CONFIG['batch_size'])\n",
    "\n",
    "global_model = MultiDLinear(CONFIG['seq_len'], CONFIG['pred_len'], n_features, close_idx)\n",
    "global_model = train(global_model, tr_dl, va_dl, CONFIG['epochs'], CONFIG['lr'], CONFIG['patience'])\n",
    "print(f\"Global: Trained ({n} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inp = torch.from_numpy(x.astype(np.float32)).unsqueeze(0).to(device)\n",
    "        return model(inp).cpu().numpy().flatten()\n",
    "\n",
    "def to_price(pred_scaled, scaler):\n",
    "    return np.exp(scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten())\n",
    "\n",
    "# Input data\n",
    "input_x = X_scaled[-CONFIG['seq_len']:]\n",
    "current_regime = df['regime'].iloc[-1]\n",
    "\n",
    "# Get HMM probabilities\n",
    "last_feat = df_hmm[['return_W', 'vol_W', 'trend_W']].iloc[-1:].values\n",
    "probs = model_hmm.predict_proba(hmm_scaler.transform(last_feat))[0]\n",
    "\n",
    "print(f\"Current regime: {REGIME_NAMES[REGIME_MAP[current_regime]]}\")\n",
    "print(f\"\\nRegime probabilities:\")\n",
    "for r in unique_regimes:\n",
    "    print(f\"  {REGIME_NAMES[REGIME_MAP[r]]}: {probs[r]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble prediction\n",
    "ensemble = np.zeros(CONFIG['pred_len'])\n",
    "for regime, model in models.items():\n",
    "    ensemble += probs[regime] * predict(model, input_x)\n",
    "\n",
    "# Add global with remaining weight\n",
    "used_weight = sum(probs[r] for r in models.keys())\n",
    "if used_weight < 1:\n",
    "    ensemble += (1 - used_weight) * predict(global_model, input_x)\n",
    "\n",
    "final_prices = to_price(ensemble, target_scaler)\n",
    "\n",
    "print(f\"\\nPrediction range: {final_prices.min():.2f} - {final_prices.max():.2f}\")\n",
    "print(f\"Change from last: {(final_prices[-1]/df['close'].iloc[-1]-1)*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "os.makedirs('submissions', exist_ok=True)\n",
    "sub = pd.DataFrame({'id': range(1, CONFIG['pred_len']+1), 'close': final_prices})\n",
    "sub_path = 'submissions/submission_hmm_multidlinear.csv'\n",
    "sub.to_csv(sub_path, index=False)\n",
    "\n",
    "print(f\"Saved: {sub_path}\")\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Historical\n",
    "hist = df.iloc[-150:]\n",
    "ax.plot(hist['time'], hist['close'], 'b-', lw=2, label='Historical')\n",
    "\n",
    "# Forecast\n",
    "future = pd.date_range(df['time'].iloc[-1] + pd.Timedelta(days=1), \n",
    "                       periods=CONFIG['pred_len'], freq='B')\n",
    "ax.plot(future, final_prices, 'r--', lw=2, label='Forecast')\n",
    "ax.axvline(df['time'].iloc[-1], color='gray', ls='--', alpha=0.5)\n",
    "\n",
    "ax.set_title('FPT Forecast: HMM + MultiDLinear Ensemble', fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price (VND)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('submissions/forecast_hmm_multidlinear.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
